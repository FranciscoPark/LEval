{"query": "How did they obtain the dataset?", "gt": "public resources where suspicious Twitter accounts were annotated, list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 20 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The dataset was built based on two lists annotated in previous works. One list contains propaganda, hoax, and clickbait accounts. The length of the suggested output is around 20 words.", "source_documents": ["top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen...", "BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha...", "Introduction\nSocial media platforms have made the spreading of fake news easier, faster as well as a..."]}
{"query": "What activation function do they use in their model?", "gt": "relu, selu, tanh", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 3 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The activation function used in their model is not specified in the given context.", "source_documents": ["top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen...", "BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources wher...", "Introduction\nSocial media platforms have made the spreading of fake news easier, faster as well as a..."]}
{"query": "What baselines do they compare to?", "gt": "Top-$k$ replies, likes, or re-tweets, FacTweet (tweet-level), LR + All Features (chunk-level), LR + All Features (tweet-level), Tweet2vec, LR + Bag-of-words", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 21 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The baselines they compare to are LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), and Top-$k$ replies, likes, or re-tweets.", "source_documents": ["top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen...", "BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources wher...", "Introduction\nSocial media platforms have made the spreading of fake news easier, faster as well as a..."]}
{"query": "How are chunks defined?", "gt": "Chunks is group of tweets from single account that  is consecutive in time - idea is that this group can show secret intention of malicious accounts.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 26 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Chunks are defined as sorted sequences of tweets from a news Twitter account. The tweets are sorted by posting date in ascending order and then split into N chunks. Each chunk consists of a sequence of tweets labeled by the label of its corresponding account. The suggested output length is not mentioned in the given context.", "source_documents": ["BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha...", "level, and especially from a textual perspective. Our contributions are:\n[leftmargin=4mm]\nWe propose...", "top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen..."]}
{"query": "What features are extracted?", "gt": "Sentiment, Morality, Style, Words embeddings", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 5 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The features extracted are:\n1. Emotion: An emotions vector is built using word occurrences of 15 emotion types from two emotional lexicons, NRC lexicon and SentiSense.\n2. Sentiment: The sentiment of the tweets is extracted using EffectWordNet, SenticNet, NRC, and subj_lexicon.\n3. Morality: Features based on the morality foundation theory are extracted, including words labeled in categories such as care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, and degradation.\n4. Style: Canonical stylistic features are used, including the count of question marks, exclamation marks, consecutive characters and letters, links, hashtags, users' mentions. The uppercase ratio and tweet length are also extracted.\n5. Words embeddings: Word embeddings for the words in the tweet are extracted using the Glove-840B-300d pretrained model. The tweet's final representation is obtained by averaging its word embeddings.\n\nNote: The provided context does not explicitly list the features, but the relevant information is mentioned in the context.", "source_documents": ["top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen...", "BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "level, and especially from a textual perspective. Our contributions are:\n[leftmargin=4mm]\nWe propose...", "on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources wher..."]}
{"query": "What features are extracted?", "gt": "15 emotion types, sentiment classes, positive and negative, care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, and degradation, count of question marks, exclamation marks, consecutive characters and letters, links, hashtags, users' mentions, uppercase ratio, tweet length, words embeddings", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 39 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The features that are extracted include emotions, sentiment, morality, style, and word embeddings. The suggested output length is around 39 words.", "source_documents": ["BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen...", "level, and especially from a textual perspective. Our contributions are:\n[leftmargin=4mm]\nWe propose...", "on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources wher..."]}
{"query": "Was the approach used in this work to detect fake news fully supervised?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Yes, the approach used in this work to detect fake news was fully supervised.", "source_documents": ["of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha...", "Introduction\nSocial media platforms have made the spreading of fake news easier, faster as well as a...", "top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen...", "on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources wher..."]}
{"query": "Based on this paper, what is the more predictive set of features to detect fake news?", "gt": "words embeddings, style, and morality features", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 6 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The more predictive set of features to detect fake news is words embeddings, style, and morality features.", "source_documents": ["Introduction\nSocial media platforms have made the spreading of fake news easier, faster as well as a...", "BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources wher...", "top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen..."]}
{"query": "How big is the dataset used in this work?", "gt": "Total dataset size: 171 account (522967 tweets)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 7 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The size of the dataset used in this work is not specified in the given context.", "source_documents": ["of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha...", "BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "Introduction\nSocial media platforms have made the spreading of fake news easier, faster as well as a...", "top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen..."]}
{"query": "How is a \"chunk of posts\" defined in this work?", "gt": "chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 16 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "A \"chunk of posts\" in this work refers to a sequence of tweets belonging to one chunk. The length of each chunk is not explicitly mentioned, but the suggested output length is around 16 words.", "source_documents": ["of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha...", "top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen...", "BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "Introduction\nSocial media platforms have made the spreading of fake news easier, faster as well as a..."]}
{"query": "What baselines were used in this work?", "gt": "LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 21 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The baselines used in this work include LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), and Top-k replies, likes, or re-tweets.", "source_documents": ["of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha...", "on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources wher...", "Introduction\nSocial media platforms have made the spreading of fake news easier, faster as well as a...", "BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word..."]}
{"query": "What models are used in the experiment?", "gt": "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 10 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The models used in the experiment are a bidirectional LSTM model (BiLSTM) and a Convolutional Neural Network (CNN) model.", "source_documents": ["(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ...", "label a post as offensive if it contains any form of non-acceptable language (profanity) or a target...", "`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "covertly aggressive, and overtly aggressive.\nBullying detection: Several studies have been published..."]}
{"query": "What are the differences between this dataset and pre-existing ones?", "gt": "no prior work has explored the target of the offensive language", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 11 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "This dataset, called OLID, is the first dataset to contain annotation of type and target of offensive language in social media. It includes annotation for the type of offense (insults, threats, or profanity) and the target of the offense (individual, group, or other). The dataset contains 14,100 tweets and is released freely to the research community. In comparison to pre-existing datasets, OLID offers a more comprehensive annotation of offensive language, including both the type and target of offenses.", "source_documents": ["(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ...", "label a post as offensive if it contains any form of non-acceptable language (profanity) or a target...", "`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "organizations, situations, events, etc. making it more challenging for systems to learn discriminati..."]}
{"query": "In what language are the tweets?", "gt": "English", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "English", "source_documents": ["label a post as offensive if it contains any form of non-acceptable language (profanity) or a target...", "(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ...", "`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "covertly aggressive, and overtly aggressive.\nBullying detection: Several studies have been published..."]}
{"query": "What is the size of the new dataset?", "gt": "14,100 tweets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "14,100 tweets", "source_documents": ["label a post as offensive if it contains any form of non-acceptable language (profanity) or a target...", "`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "organizations, situations, events, etc. making it more challenging for systems to learn discriminati...", "(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ..."]}
{"query": "What is the size of the new dataset?", "gt": "Dataset contains total of 14100 annotations.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 6 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The size of the new dataset is 14,100 tweets.", "source_documents": ["organizations, situations, events, etc. making it more challenging for systems to learn discriminati...", "label a post as offensive if it contains any form of non-acceptable language (profanity) or a target...", "`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "Introduction\nOffensive content has become pervasive in social media and a reason of concern for gove..."]}
{"query": "What kinds of offensive content are explored?", "gt": "non-targeted profanity and swearing, targeted insults such as cyberbullying, offensive content related to ethnicity, gender or sexual orientation, political affiliation, religious belief, and anything belonging to hate speech", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 28 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The kinds of offensive content explored in the study are insults, threats, profanity, cyberbullying, hate speech, and offensive language.", "source_documents": ["label a post as offensive if it contains any form of non-acceptable language (profanity) or a target...", "Introduction\nOffensive content has become pervasive in social media and a reason of concern for gove...", "`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ..."]}
{"query": "What is the best performing model?", "gt": "CNN ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "CNN", "source_documents": ["label a post as offensive if it contains any form of non-acceptable language (profanity) or a target...", "(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ...", "`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "organizations, situations, events, etc. making it more challenging for systems to learn discriminati..."]}
{"query": "How many annotators participated?", "gt": "five annotators", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "six experts", "source_documents": ["`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "label a post as offensive if it contains any form of non-acceptable language (profanity) or a target...", "(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ...", "Introduction\nOffensive content has become pervasive in social media and a reason of concern for gove..."]}
{"query": "What is the definition of offensive language?", "gt": " Most prior work focuses on a different aspect of offensive language such as abusive language BIBREF0 , BIBREF1 , (cyber-)aggression BIBREF2 , (cyber-)bullying BIBREF3 , BIBREF4 , toxic comments INLINEFORM0 , hate speech BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , and offensive language BIBREF11 . Prior work has focused on these aspects of offensive language in Twitter BIBREF3 , BIBREF7 , BIBREF8 , BIBREF11 , Wikipedia comments, and Facebook posts BIBREF2 .", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 77 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Offensive language is defined as any form of non-acceptable language that includes profanity, insults, threats, and posts containing swear words. It can be either veiled or direct, and can target individuals, groups, or others. Offensive language can also include hate speech, cyberbullying, and insults targeted at specific characteristics such as ethnicity, gender, sexual orientation, political affiliation, or religious belief. The detection and categorization of offensive language is done through a hierarchical annotation model that consists of three levels: offensive language detection, categorization of offensive language, and offensive language target identification.", "source_documents": ["label a post as offensive if it contains any form of non-acceptable language (profanity) or a target...", "(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ...", "`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "covertly aggressive, and overtly aggressive.\nBullying detection: Several studies have been published..."]}
{"query": "What are the three layers of the annotation scheme?", "gt": "Level A: Offensive language Detection\n, Level B: Categorization of Offensive Language\n, Level C: Offensive Language Target Identification\n", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 19 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The three layers of the annotation scheme are Level A: Offensive Language Detection, Level B: Categorization of Offensive Language, and Level C: Offensive Language Target Identification.", "source_documents": ["(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ...", "label a post as offensive if it contains any form of non-acceptable language (profanity) or a target...", "`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "organizations, situations, events, etc. making it more challenging for systems to learn discriminati..."]}
{"query": "How long is the dataset for each step of hierarchy?", "gt": "Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The length of the dataset for each step of hierarchy is not mentioned in the given context.", "source_documents": ["`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "organizations, situations, events, etc. making it more challenging for systems to learn discriminati...", "covertly aggressive, and overtly aggressive.\nBullying detection: Several studies have been published...", "(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ..."]}
{"query": "What is the approach of previous work?", "gt": "'shallow' naive Bayes, SVM, hierarchical stacked classifiers, bidirectional recurrent neural networks", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 11 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Previous work on language identification (LID) has used various approaches, including shallow naive Bayes classifiers, SVMs, fasttext classifiers, and hierarchical stacked classifiers with lexicons. These approaches have been applied to datasets such as the NCHLT text corpora, DSL shared task datasets, and other available resources. Deeper models like bidirectional recurrent neural networks and ensembles of recurrent neural networks have also been investigated. The effectiveness of these approaches is influenced by factors such as the length of the text, the similarity of languages, and the availability of training data.", "source_documents": ["samples and 1000 testing samples per language.\nThe recently published JW300 parallel corpus BIBREF2 ...", "Introduction\nAccurate language identification (LID) is the first step in many natural language proce...", "approaches are potentially very useful to reduce the requirement for data.\nResearchers have investig...", "from the winning language is at least one more than the number of words considered to be from the la..."]}
{"query": "Is the lexicon the same for all languages?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Based on the information provided, it is not explicitly stated whether the lexicon is the same for all languages. Therefore, it is not possible to determine if the lexicon is the same for all languages.", "source_documents": ["samples and 1000 testing samples per language.\nThe recently published JW300 parallel corpus BIBREF2 ...", "approaches are potentially very useful to reduce the requirement for data.\nResearchers have investig...", "Introduction\nAccurate language identification (LID) is the first step in many natural language proce...", "from the winning language is at least one more than the number of words considered to be from the la..."]}
{"query": "How do they obtain the lexicon?", "gt": "built over all the data and therefore includes the vocabulary from both the training and testing sets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 17 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The context does not provide any information about how the lexicon is obtained or its length. Therefore, it is not possible to answer the question accurately.", "source_documents": ["shown in Table TABREF10. Results were generated on an early 2015 13-inch Retina MacBook Pro with a 2...", "samples and 1000 testing samples per language.\nThe recently published JW300 parallel corpus BIBREF2 ...", "from the winning language is at least one more than the number of words considered to be from the la...", "approaches are potentially very useful to reduce the requirement for data.\nResearchers have investig..."]}
{"query": "What evaluation metric is used?", "gt": "average classification accuracy, execution performance", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 5 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The given context does not provide information about the specific evaluation metric used.", "source_documents": ["from the winning language is at least one more than the number of words considered to be from the la...", "samples and 1000 testing samples per language.\nThe recently published JW300 parallel corpus BIBREF2 ...", "Introduction\nAccurate language identification (LID) is the first step in many natural language proce...", "approaches are potentially very useful to reduce the requirement for data.\nResearchers have investig..."]}
{"query": "Which languages are similar to each other?", "gt": "Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 11 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The Nguni languages (zul, xho, nbl, ssw) are similar to each other.", "source_documents": ["Introduction\nAccurate language identification (LID) is the first step in many natural language proce...", "samples and 1000 testing samples per language.\nThe recently published JW300 parallel corpus BIBREF2 ...", "approaches are potentially very useful to reduce the requirement for data.\nResearchers have investig...", "shown in Table TABREF10. Results were generated on an early 2015 13-inch Retina MacBook Pro with a 2..."]}
{"query": "Which datasets are employed for South African languages LID?", "gt": "DSL 2015, DSL 2017, JW300 parallel corpus , NCHLT text corpora", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 11 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The datasets employed for South African languages LID include the NCHLT text corpora, the DSL 2015 and DSL 2017 shared task datasets, and the WiLI-2018 benchmark dataset.", "source_documents": ["samples and 1000 testing samples per language.\nThe recently published JW300 parallel corpus BIBREF2 ...", "Introduction\nAccurate language identification (LID) is the first step in many natural language proce...", "approaches are potentially very useful to reduce the requirement for data.\nResearchers have investig...", "shown in Table TABREF10. Results were generated on an early 2015 13-inch Retina MacBook Pro with a 2..."]}
{"query": "Does the paper report the performance of a baseline model on South African languages LID?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "No", "source_documents": ["samples and 1000 testing samples per language.\nThe recently published JW300 parallel corpus BIBREF2 ...", "Introduction\nAccurate language identification (LID) is the first step in many natural language proce...", "approaches are potentially very useful to reduce the requirement for data.\nResearchers have investig...", "shown in Table TABREF10. Results were generated on an early 2015 13-inch Retina MacBook Pro with a 2..."]}
{"query": "Does the algorithm improve on the state-of-the-art methods?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Yes", "source_documents": ["samples and 1000 testing samples per language.\nThe recently published JW300 parallel corpus BIBREF2 ...", "shown in Table TABREF10. Results were generated on an early 2015 13-inch Retina MacBook Pro with a 2...", "approaches are potentially very useful to reduce the requirement for data.\nResearchers have investig...", "Introduction\nAccurate language identification (LID) is the first step in many natural language proce..."]}
{"query": "Do they report results only on English data?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Based on the given context, it is not explicitly mentioned whether the results are reported only on English data or not. Therefore, the answer to the question is unknown.", "source_documents": ["about these transformer encoders' architectures are presented in BIBREF11. In the fine-tuning phase,...", "in tweets with IDs 1 and 2 in Table TABREF20, it can be understood that our BERT based classifier is...", "use the pre-trained language model BERT for hate speech classification and try to fine-tune specific...", "pre-trained WordPiece. As the implementation environment, we use Google Colaboratory tool which is a..."]}
{"query": "Do they report results only on English data?", "gt": "The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 19 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "It is not specified in the given context whether they report results only on English data or not.", "source_documents": ["about these transformer encoders' architectures are presented in BIBREF11. In the fine-tuning phase,...", "use the pre-trained language model BERT for hate speech classification and try to fine-tune specific...", "in tweets with IDs 1 and 2 in Table TABREF20, it can be understood that our BERT based classifier is...", "pre-trained WordPiece. As the implementation environment, we use Google Colaboratory tool which is a..."]}
{"query": "What evidence do the authors present that the model can capture some biases in data annotation and collection?", "gt": "Waseem-dataset, Davidson-dataset,", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "data biases", "source_documents": ["in tweets with IDs 1 and 2 in Table TABREF20, it can be understood that our BERT based classifier is...", "efficiently handle different types of abusive language such as cyberbullying, hate, sarcasm, etc. us...", "have been applied for classification purposes. Waseem et al. BIBREF5 provided a test with a list of ...", "use the pre-trained language model BERT for hate speech classification and try to fine-tune specific..."]}
{"query": "Which publicly available datasets are used?", "gt": "Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 13 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The context does not explicitly mention the names of the publicly available datasets that are used. Therefore, we do not have enough information to answer the question.", "source_documents": ["about these transformer encoders' architectures are presented in BIBREF11. In the fine-tuning phase,...", "BIBREF10, they necessitate a well-defined feature extraction approach. The trend now seems to be cha...", "pre-trained WordPiece. As the implementation environment, we use Google Colaboratory tool which is a...", "use the pre-trained language model BERT for hate speech classification and try to fine-tune specific..."]}
{"query": "What baseline is used?", "gt": "BERT based fine-tuning, Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The baselines used for comparison are Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10.", "source_documents": ["use the pre-trained language model BERT for hate speech classification and try to fine-tune specific...", "about these transformer encoders' architectures are presented in BIBREF11. In the fine-tuning phase,...", "pre-trained WordPiece. As the implementation environment, we use Google Colaboratory tool which is a...", "have been applied for classification purposes. Waseem et al. BIBREF5 provided a test with a list of ..."]}
{"query": "What new fine-tuning methods are presented?", "gt": "sampling tweets from specific keywords create systematic and substancial racial biases in datasets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 13 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The new fine-tuning methods that are presented are:\n1. BERT based fine-tuning\n2. Insert nonlinear layers\n3. Insert Bi-LSTM layer\n4. Insert CNN layer", "source_documents": ["about these transformer encoders' architectures are presented in BIBREF11. In the fine-tuning phase,...", "BIBREF10, they necessitate a well-defined feature extraction approach. The trend now seems to be cha...", "use the pre-trained language model BERT for hate speech classification and try to fine-tune specific...", "pre-trained WordPiece. As the implementation environment, we use Google Colaboratory tool which is a..."]}
{"query": "What are the existing biases?", "gt": "Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 23 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The text does not provide information about any existing biases.", "source_documents": ["about these transformer encoders' architectures are presented in BIBREF11. In the fine-tuning phase,...", "BIBREF10, they necessitate a well-defined feature extraction approach. The trend now seems to be cha...", "pre-trained WordPiece. As the implementation environment, we use Google Colaboratory tool which is a...", "use the pre-trained language model BERT for hate speech classification and try to fine-tune specific..."]}
{"query": "What biases does their model capture?", "gt": "Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 13 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The model captures biases from data collection and rules of annotation, as mentioned in the context. It also captures biases from specific language and geographic restriction in the Davidson dataset, where certain tweets with disparage words are oversampled.", "source_documents": ["have been applied for classification purposes. Waseem et al. BIBREF5 provided a test with a list of ...", "in tweets with IDs 1 and 2 in Table TABREF20, it can be understood that our BERT based classifier is...", "use the pre-trained language model BERT for hate speech classification and try to fine-tune specific...", "about these transformer encoders' architectures are presented in BIBREF11. In the fine-tuning phase,..."]}
{"query": "What does the \"sensitivity\" quantity denote?", "gt": "the number of distinct word recognition outputs that an attacker can induce", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The \"sensitivity\" quantity denotes the expected number of unique outputs that a word recognition system assigns to a set of adversarial perturbations.", "source_documents": ["vision, an important factor determining the success of an adversary is the norm constraint on the pe...", "of options.\nSecond, we evaluate first-line techniques including data augmentation and adversarial tr...", "by concatenating (i) a one hot vector of the first character ( $\\mathbf {w_{i1}}$ ); (ii) a one hot ...", "2 . Note that ATD incorrectly predicts $11.2$ words for every 100 words (in the `all' setting), wher..."]}
{"query": "What end tasks do they evaluate on?", "gt": "Sentiment analysis and paraphrase detection under adversarial attacks", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 8 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The authors evaluate their defenses on the end tasks of sentiment classification and paraphrase detection.", "source_documents": ["of options.\nSecond, we evaluate first-line techniques including data augmentation and adversarial tr...", "by concatenating (i) a one hot vector of the first character ( $\\mathbf {w_{i1}}$ ); (ii) a one hot ...", "words shorter than 4 characters.\nFor 1-character attacks, we try all possible perturbations listed a...", "2 . Note that ATD incorrectly predicts $11.2$ words for every 100 words (in the `all' setting), wher..."]}
{"query": "What is a semicharacter architecture?", "gt": "A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 22 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "A semicharacter architecture is a type of word recognition model that combines both character-level and word-level information. In this architecture, the input words are encoded using a concatenation of a one-hot vector of the first character, a one-hot representation of the last character, and a bag of characters representation of the internal characters. This representation is then fed into a BiLSTM cell to process the sequence of characters. The model is trained to predict the correct corresponding word at each sequence step using cross-entropy loss.", "source_documents": ["2 . Note that ATD incorrectly predicts $11.2$ words for every 100 words (in the `all' setting), wher...", "of options.\nSecond, we evaluate first-line techniques including data augmentation and adversarial tr...", "by concatenating (i) a one hot vector of the first character ( $\\mathbf {w_{i1}}$ ); (ii) a one hot ...", "words shorter than 4 characters.\nFor 1-character attacks, we try all possible perturbations listed a..."]}
{"query": "Do they experiment with offering multiple candidate corrections and voting on the model output, since this seems highly likely to outperform a one-best correction?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "No, there is no mention of experimenting with offering multiple candidate corrections and voting on the model output.", "source_documents": ["accuracy within $5\\%$ range for the swap attack. It is also effective under other attack classes, an...", "that adversarial examples in NLP are often ungrammatical BIBREF14 .\nIn related work on character-lev...", "words shorter than 4 characters.\nFor 1-character attacks, we try all possible perturbations listed a...", "by concatenating (i) a one hot vector of the first character ( $\\mathbf {w_{i1}}$ ); (ii) a one hot ..."]}
{"query": "Why is the adversarial setting appropriate for misspelling recognition?", "gt": "Adversarial misspellings are a real-world problem", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 6 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The adversarial setting is appropriate for misspelling recognition because it allows for the evaluation and improvement of word recognition models in handling rare and unseen words. In the adversarial setting, the focus is on how well the word recognition model can handle these cases and gracefully handle misspelled or unknown words. This is important because in real-world scenarios, misspellings and rare words are more likely to occur and could provide opportunities for attackers. By testing and improving models in an adversarial setting, researchers can work towards building more robust and accurate word-recognizers.", "source_documents": ["by concatenating (i) a one hot vector of the first character ( $\\mathbf {w_{i1}}$ ); (ii) a one hot ...", "words shorter than 4 characters.\nFor 1-character attacks, we try all possible perturbations listed a...", "include: (1) data augmentation (DA) BIBREF28 ; and (2) adversarial training (Adv) BIBREF29 . In DA, ...", "Introduction\nDespite the rapid progress of deep learning techniques on diverse supervised learning t..."]}
{"query": "How do the backoff strategies work?", "gt": "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 43 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The backoff strategies in the context refer to ways to handle rare and unseen words in the word recognition models. There are three backoff strategies mentioned:\n\n1. Pass-through: In this strategy, the word-recognizer simply passes on the (possibly misspelled) word as it is. This means that if the word recognition model predicts UNK (unknown) for a word, it will not make any changes to that word and include it in the output as it is.\n\n2. Backoff to neutral word: If the word recognition model predicts UNK for a word, instead of passing it through unchanged, this strategy suggests backing off to a neutral word like 'a'. The neutral word is chosen because it has a similar distribution across classes. So, if UNK is predicted for a word, it will be replaced with the neutral word 'a' in the output.\n\n3. Backoff to background model: This strategy involves falling back upon a more generic word recognition model trained on a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK. The background model is trained on a larger corpus and is less accurate because it needs to predict a large number of words. However, when the foreground model predicts UNK for a word, the background model is used to provide a prediction for that word instead. This strategy allows handling rare and unobserved words by utilizing the larger, more general background model.\n\nThese backoff strategies are used in the word recognition models to improve their ability to handle rare and unseen words, which can be critical for robustness against adversarial attacks.", "source_documents": ["of options.\nSecond, we evaluate first-line techniques including data augmentation and adversarial tr...", "that adversarial examples in NLP are often ungrammatical BIBREF14 .\nIn related work on character-lev...", "by concatenating (i) a one hot vector of the first character ( $\\mathbf {w_{i1}}$ ); (ii) a one hot ...", "words shorter than 4 characters.\nFor 1-character attacks, we try all possible perturbations listed a..."]}
{"query": "What baseline model is used?", "gt": "same baseline as used by lang2011unsupervised", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 6 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The baseline model used is the one that assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head.", "source_documents": ["parallel data in multilingual SRI, for example by joint modeling with SRI. We leave this exploration...", "an induced cluster corresponds to a single gold role, (ii) Collocation (CO) measures how well a gold...", "of features might get assigned different roles if they appear in different contexts.\nData\nFollowing ...", "we increase the proportion of supervised sentences. At 10% supervised sentences, the model has alrea..."]}
{"query": "Which additional latent variables are used in the model?", "gt": "CLV as a parent of the two corresponding role variables", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 10 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The additional latent variables used in the model are crosslingual latent variables (CLVs).", "source_documents": ["we increase the proportion of supervised sentences. At 10% supervised sentences, the model has alrea...", "parallel data in multilingual SRI, for example by joint modeling with SRI. We leave this exploration...", "the step 4 of this pipeline.\nMonolingual Model\nWe use the Bayesian model of garg2012unsupervised as ...", "INLINEFORM1 of them, the multilingual latent variable is connected to only those INLINEFORM2 aligned..."]}
{"query": "Which additional latent variables are used in the model?", "gt": "crosslingual latent variables", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 3 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The additional latent variables used in the model are crosslingual latent variables (CLVs).", "source_documents": ["we increase the proportion of supervised sentences. At 10% supervised sentences, the model has alrea...", "the step 4 of this pipeline.\nMonolingual Model\nWe use the Bayesian model of garg2012unsupervised as ...", "INLINEFORM1 of them, the multilingual latent variable is connected to only those INLINEFORM2 aligned...", "languages. Using the CoNLL 2009 dataset alone, titovcrosslingual report an F1 score of 80.9% (PU=86...."]}
{"query": "Which parallel corpora are used?", "gt": "English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13, EN-DE section of the Europarl corpus BIBREF14", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 19 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The parallel corpora used are the English (EN) and German (DE) sections of the CoNLL 2009 corpus and the EN-DE section of the Europarl corpus.", "source_documents": ["of features might get assigned different roles if they appear in different contexts.\nData\nFollowing ...", "parallel data in multilingual SRI, for example by joint modeling with SRI. We leave this exploration...", "we increase the proportion of supervised sentences. At 10% supervised sentences, the model has alrea...", "of multilingual SRI, we can apply the same model to a variety of training scenarios just by changing..."]}
{"query": "Overall, does having parallel data improve semantic role induction across multiple languages?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Yes", "source_documents": ["an induced cluster corresponds to a single gold role, (ii) Collocation (CO) measures how well a gold...", "parallel data in multilingual SRI, for example by joint modeling with SRI. We leave this exploration...", "Introduction\nSemantic Role Labeling (SRL) has emerged as an important task in Natural Language Proce...", "the step 4 of this pipeline.\nMonolingual Model\nWe use the Bayesian model of garg2012unsupervised as ..."]}
{"query": "Do they add one latent variable for each language pair in their Bayesian model?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Yes.", "source_documents": ["the step 4 of this pipeline.\nMonolingual Model\nWe use the Bayesian model of garg2012unsupervised as ...", "FIGREF16 illustrates this model. The generative process, as explained below, remains the same as the...", "INLINEFORM1 of them, the multilingual latent variable is connected to only those INLINEFORM2 aligned...", "an induced cluster corresponds to a single gold role, (ii) Collocation (CO) measures how well a gold..."]}
{"query": "What does an individual model consist of?", "gt": "Bayesian model of garg2012unsupervised as our base monolingual model", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 9 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "An individual model consists of individual Bayesian models for each language and an additional crosslingual latent variable to capture crosslingual semantic role patterns.", "source_documents": ["an induced cluster corresponds to a single gold role, (ii) Collocation (CO) measures how well a gold...", "the step 4 of this pipeline.\nMonolingual Model\nWe use the Bayesian model of garg2012unsupervised as ...", "Introduction\nSemantic Role Labeling (SRL) has emerged as an important task in Natural Language Proce...", "parallel data in multilingual SRI, for example by joint modeling with SRI. We leave this exploration..."]}
{"query": "Do they improve on state-of-the-art semantic role induction?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Yes, the proposed Bayesian model for semantic role induction (SRI) improves on state-of-the-art semantic role induction.", "source_documents": ["parallel data in multilingual SRI, for example by joint modeling with SRI. We leave this exploration...", "Introduction\nSemantic Role Labeling (SRL) has emerged as an important task in Natural Language Proce...", "of features might get assigned different roles if they appear in different contexts.\nData\nFollowing ...", "we increase the proportion of supervised sentences. At 10% supervised sentences, the model has alrea..."]}
{"query": "how many tags do they look at?", "gt": "48,705", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The given context does not provide information about the number of tags that are looked at. Therefore, it is not possible to determine how many tags they look at.", "source_documents": ["for the 2,896 e-books, for whom we got review keywords. To follow common practice for tag recommenda...", "however, not the case for INLINEFORM3 as the accuracy of the integrated INLINEFORM4 approach is low....", "between both rankings. This results in INLINEFORM1 with a p-value < INLINEFORM2 , which suggests a h...", "available for e-books that contain editor tags. Consequently, both data sources provide complementar..."]}
{"query": "which algorithm was the highest performer?", "gt": "A hybrid model consisting of best performing popularity-based approach with the best similarity-based approach", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 14 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The highest performer algorithm was INLINEFORM7 for INLINEFORM8.", "source_documents": ["however, not the case for INLINEFORM3 as the accuracy of the integrated INLINEFORM4 approach is low....", "(i.e., description or title) to recommend relevant tags BIBREF10 . For this, we first employ a conte...", "for the 2,896 e-books, for whom we got review keywords. To follow common practice for tag recommenda...", ", BIBREF17 as we did, for example, with our proposed semantic similarity evaluation metric. Finally,..."]}
{"query": "how is diversity measured?", "gt": " the average dissimilarity of all pairs of tags in the list of recommended tags", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 14 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The diversity is measured as the average dissimilarity of all pairs of tags in the list of recommended tags. The formula to calculate diversity is:\nDiversity = (1/n) * \u2211 distance(tag_i, tag_j)\nwhere n is the number of evaluated e-books and the distance function represents the dissimilarity between two tags. The suggested output length is not mentioned in the given context.", "source_documents": ["however, not the case for INLINEFORM3 as the accuracy of the integrated INLINEFORM4 approach is low....", "for the 2,896 e-books, for whom we got review keywords. To follow common practice for tag recommenda...", "Introduction\nWhen people shop for books online in e-book stores such as, e.g., the Amazon Kindle sto...", "to compare how semantically similar the recommended tags are to the expected review keywords. For th..."]}
{"query": "how large is the vocabulary?", "gt": "33,663", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The given information does not provide any specific details about the size of the vocabulary. Therefore, I don't know the answer to the question.", "source_documents": ["for the 2,896 e-books, for whom we got review keywords. To follow common practice for tag recommenda...", "however, not the case for INLINEFORM3 as the accuracy of the integrated INLINEFORM4 approach is low....", "Introduction\nWhen people shop for books online in e-book stores such as, e.g., the Amazon Kindle sto...", "to compare how semantically similar the recommended tags are to the expected review keywords. For th..."]}
{"query": "what dataset was used?", "gt": " E-book annotation data: editor tags, Amazon search terms, and  Amazon review keywords.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The dataset used in the study includes editor tags and Amazon search terms for e-books.", "source_documents": ["however, not the case for INLINEFORM3 as the accuracy of the integrated INLINEFORM4 approach is low....", "validate semantic matches of tag recommendations.\nApproach and findings. We exploit editor tags and ...", "Introduction\nWhen people shop for books online in e-book stores such as, e.g., the Amazon Kindle sto...", ", BIBREF17 as we did, for example, with our proposed semantic similarity evaluation metric. Finally,..."]}
{"query": "what algorithms did they use?", "gt": "popularity-based, similarity-based, hybrid", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 3 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Popularity-based, similarity-based, hybrid", "source_documents": ["however, not the case for INLINEFORM3 as the accuracy of the integrated INLINEFORM4 approach is low....", "Introduction\nWhen people shop for books online in e-book stores such as, e.g., the Amazon Kindle sto...", "available for e-books that contain editor tags. Consequently, both data sources provide complementar...", ", BIBREF17 as we did, for example, with our proposed semantic similarity evaluation metric. Finally,..."]}
{"query": "What baseline method is used?", "gt": "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 24 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The baseline method used is the word2vec algorithm.", "source_documents": ["that perform sentiment analysis in Turkish and leverages sentimental characteristics of words in gen...", "Embeddings\nIn addition to using the three approaches independently, we also combined all the matrice...", "proposed approach are discussed in Section 4. In Section 5, we conclude the paper.\nRelated Work\nIn t...", "utility of the WEKA tool. We chose the linear kernel option to classify the reviews. We trained word..."]}
{"query": "What details are given about the Twitter dataset?", "gt": "one of the Twitter datasets is about Turkish mobile network operators, there are positive, neutral and negative labels and provide the total amount plus the distribution of labels", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 28 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The Twitter dataset is formed of tweets about Turkish mobile network operators. It consists of 1,716 tweets, with 973 of them labeled as negative and 743 labeled as positive. The dataset was manually annotated by two humans, and if there was a disagreement on the polarity of a tweet, it was removed.", "source_documents": ["that perform sentiment analysis in Turkish and leverages sentimental characteristics of words in gen...", "English Twitter dataset.\nSince the TDK dictionary covers most of the domain-specific vocabulary used...", "We consider a review to be negative it the score is equal to or lower than 2.5. On the other hand, i...", "we also separately multiplied all the row scores with only +1 if the row word is a positive word, an..."]}
{"query": "What details are given about the movie domain dataset?", "gt": "there are 20,244 reviews divided into positive and negative with an average 39 words per review, each one having a star-rating score", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 22 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The movie domain dataset consists of 20,244 movie reviews. The average number of words in each review is 39. Each review has a star-rating score that indicates the sentiment. The polarity scores range from 0.5 to 5, with intervals of 0.5.", "source_documents": ["Embeddings\nIn addition to using the three approaches independently, we also combined all the matrice...", "that perform sentiment analysis in Turkish and leverages sentimental characteristics of words in gen...", "English Twitter dataset.\nSince the TDK dictionary covers most of the domain-specific vocabulary used...", "We consider a review to be negative it the score is equal to or lower than 2.5. On the other hand, i..."]}
{"query": "Which hand-crafted features are combined with word2vec?", "gt": "polarity scores, which are minimum, mean, and maximum polarity scores, from each review", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 13 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The hand-crafted features that are combined with word2vec are the minimum, mean, and maximum polarity scores of the reviews.", "source_documents": ["proposed approach are discussed in Section 4. In Section 5, we conclude the paper.\nRelated Work\nIn t...", "Embeddings\nIn addition to using the three approaches independently, we also combined all the matrice...", "utility of the WEKA tool. We chose the linear kernel option to classify the reviews. We trained word...", "English Twitter dataset.\nSince the TDK dictionary covers most of the domain-specific vocabulary used..."]}
{"query": "What word-based and dictionary-based feature are used?", "gt": "generate word embeddings specific to a domain, TDK (T\u00fcrk Dil Kurumu - \u201cTurkish Language Institution\u201d) dictionary to obtain word polarities", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 20 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The word-based features used are word vectors generated through the concatenation of corpus-based, dictionary-based, and 4-score vectors of each word. The dictionary-based feature used is the TDK (Turkish Language Institution) dictionary, which provides word polarities.", "source_documents": ["Embeddings\nIn addition to using the three approaches independently, we also combined all the matrice...", "that perform sentiment analysis in Turkish and leverages sentimental characteristics of words in gen...", "we also separately multiplied all the row scores with only +1 if the row word is a positive word, an...", "decomposition is computed. We take into account the U matrix only for each word. We have chosen the ..."]}
{"query": "How are the supervised scores of the words calculated?", "gt": "(+1 or -1), words of opposite polarities (e.g. \u201chappy\" and \u201cunhappy\") get far away from each other", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 17 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The supervised scores of the words are calculated using a formula that takes into account the sentiment polarity of the word and its occurrence in positive and negative documents. The formula is shown in (DISPLAY_FORM4). It calculates the sentiment score of a word based on the number of positive and negative documents it appears in, as well as the total number of words in the positive and negative corpora. A normalization step is also performed to prevent imbalance, and a small number is added for smoothing.", "source_documents": ["sentimental, lexical, and contextual characteristics of words. In addition to these mostly original ...", "decomposition is computed. We take into account the U matrix only for each word. We have chosen the ...", "Introduction\nSentiment analysis has recently been one of the hottest topics in natural language proc...", "we also separately multiplied all the row scores with only +1 if the row word is a positive word, an..."]}
{"query": "What topic is covered in the Chinese Facebook data? ", "gt": "anti-nuclear-power", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The topic covered in the Chinese Facebook data is anti-nuclear-power.", "source_documents": ["DISPLAYFORM0\nwhere INLINEFORM0 models the user reading preference for certain semantics, and INLINEF...", "a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show th...", "directly without additionally applying LDA.\nFor the FBFans data we report class-based f-scores as we...", "network for target-dependent Twitter sentiment analysis, where targets are topics such as windows 7 ..."]}
{"query": "How many layers does the UTCNN model have?", "gt": "eight layers", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The UTCNN model has three convolutional layers.", "source_documents": ["network for target-dependent Twitter sentiment analysis, where targets are topics such as windows 7 ...", "DISPLAYFORM0\nwhere INLINEFORM0 models the user reading preference for certain semantics, and INLINEF...", "directly without additionally applying LDA.\nFor the FBFans data we report class-based f-scores as we...", "their replies to posts. 2. When classifying social media post stances, we leverage users, including ..."]}
{"query": "What topics are included in the debate data?", "gt": "abortion, gay rights, Obama, marijuana", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 5 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The topics included in the debate data are abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR).", "source_documents": ["DISPLAYFORM0\nwhere INLINEFORM0 models the user reading preference for certain semantics, and INLINEF...", "posts can attract even more neutral likers. These results do suggest that users prefer posts of the ...", "directly without additionally applying LDA.\nFor the FBFans data we report class-based f-scores as we...", "network for target-dependent Twitter sentiment analysis, where targets are topics such as windows 7 ..."]}
{"query": "What topics are included in the debate data?", "gt": "abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 10 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The topics included in the debate data are abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR).", "source_documents": ["DISPLAYFORM0\nwhere INLINEFORM0 models the user reading preference for certain semantics, and INLINEF...", "posts can attract even more neutral likers. These results do suggest that users prefer posts of the ...", "network for target-dependent Twitter sentiment analysis, where targets are topics such as windows 7 ...", "a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show th..."]}
{"query": "What is the size of the Chinese data?", "gt": "32,595", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The size of the Chinese data is not mentioned in the given context, therefore it is unknown.", "source_documents": ["DISPLAYFORM0\nwhere INLINEFORM0 models the user reading preference for certain semantics, and INLINEF...", "a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show th...", "directly without additionally applying LDA.\nFor the FBFans data we report class-based f-scores as we...", "network for target-dependent Twitter sentiment analysis, where targets are topics such as windows 7 ..."]}
{"query": "Did they collected the two datasets?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The provided context does not mention whether the researchers collected the two datasets or not. Therefore, it is not possible to determine if they collected the datasets based on the given information.", "source_documents": ["DISPLAYFORM0\nwhere INLINEFORM0 models the user reading preference for certain semantics, and INLINEF...", "network for target-dependent Twitter sentiment analysis, where targets are topics such as windows 7 ...", "the revealed user stance: whether a user is willing to show his preference, whether a user likes to ...", "directly without additionally applying LDA.\nFor the FBFans data we report class-based f-scores as we..."]}
{"query": "What are the baselines?", "gt": "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 24 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The baselines mentioned in the given context are:\n\n1) SVM with unigram, bigram, and trigram features\n2) SVM with average word embedding\n3) SVM with average transformed word embeddings\n4) CNN (Convolutional Neural Networks)\n5) RCNN (Recurrent Convolutional Neural Networks)\n6) The above SVM and deep learning models with comment information\n7) UTCNN without user information\n8) UTCNN without the LDA model\n9) UTCNN without comments\n\nThe mentioned output length is not relevant to the question regarding the baselines.", "source_documents": ["DISPLAYFORM0\nwhere INLINEFORM0 models the user reading preference for certain semantics, and INLINEF...", "directly without additionally applying LDA.\nFor the FBFans data we report class-based f-scores as we...", "network for target-dependent Twitter sentiment analysis, where targets are topics such as windows 7 ...", "the revealed user stance: whether a user is willing to show his preference, whether a user likes to ..."]}
{"query": "why are their techniques cheaper to implement?", "gt": "They use a slightly modified copy of the target to create the pseudo-text instead of full BT to make their technique cheaper", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 22 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The text does not provide any specific information about why the mentioned techniques are cheaper to implement. Therefore, I don't know the answer to the question.", "source_documents": ["copy-marked setup are pretrained for three epochs on the in-domain data, while all remaining paramet...", "for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-Eng...", "SECREF5 ) suggest promising directions for efficient domain adaptation with cheaper techniques than ...", "is reported in Table TABREF7 , where we observe a 12 BLEU points gap between the worst and best syst..."]}
{"query": "what data simulation techniques were introduced?", "gt": "copy, copy-marked, copy-dummies", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 3 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The data simulation techniques introduced are copy, copy-marked, and copy-dummies.", "source_documents": ["copy-marked setup are pretrained for three epochs on the in-domain data, while all remaining paramet...", "dl4mt with the target side of the parallel data and the Europarl corpus (about 6M sentences for both...", "Introduction \nThe new generation of Neural Machine Translation (NMT) systems is known to be extremel...", "for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-Eng..."]}
{"query": "what is their explanation for the effectiveness of back-translation?", "gt": "when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent, automatic word alignments between artificial sources tend to be more monotonic than when using natural sources", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 40 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The provided context does not explicitly mention the explanation for the effectiveness of back-translation.", "source_documents": ["copy-marked setup are pretrained for three epochs on the in-domain data, while all remaining paramet...", "for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-Eng...", "a decrease of 0.4 to over 1.0 BLEU. We assume from these experiments that BT impacts most of all the...", "is reported in Table TABREF7 , where we observe a 12 BLEU points gap between the worst and best syst..."]}
{"query": "what dataset is used?", "gt": "Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 17 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The context does not provide information about the specific dataset that is used. Therefore, it is not possible to determine the dataset based on the given information.", "source_documents": ["copy-marked setup are pretrained for three epochs on the in-domain data, while all remaining paramet...", "for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-Eng...", "is reported in Table TABREF7 , where we observe a 12 BLEU points gap between the worst and best syst...", "recap our answers to our initial questions: the quality of BT actually matters for NMT (cf. \u00a7 SECREF..."]}
{"query": "what language pairs are explored?", "gt": "English-German, English-French", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The language pairs explored in the given context are English-French and English-German.", "source_documents": ["copy-marked setup are pretrained for three epochs on the in-domain data, while all remaining paramet...", "for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-Eng...", "is reported in Table TABREF7 , where we observe a 12 BLEU points gap between the worst and best syst...", "pairs, with the exception of a small improvement for English-French on the out-of-domain test, which..."]}
{"query": "what language is the data in?", "gt": "English , German, French", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 4 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The data is in English.", "source_documents": ["copy-marked setup are pretrained for three epochs on the in-domain data, while all remaining paramet...", "for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-Eng...", "randomly selected out-of-domain natural sentences, with the same architecture and training parameter...", "is reported in Table TABREF7 , where we observe a 12 BLEU points gap between the worst and best syst..."]}
{"query": "What metrics are used for evaluation?", "gt": "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 35 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The metrics used for evaluation are BPE perplexity, BLEU-1/4, ROUGE-L, and Distinct-1/2.", "source_documents": ["$x$ is embedded via a technique embedding layer $\\mathcal {X}$ to $\\mathbf {x}\\in \\mathbb {R}^{d_x}$...", "states as $\\lbrace \\mathbf {i}_{\\text{enc},j} \\in \\mathbb {R}^{2d_h}\\rbrace $. The caloric level is ...", "internally consistent long texts BIBREF8, BIBREF9, BIBREF10. Here, we generate procedurally structur...", "first 3-5 ingredients listed in each recipe. We decode recipe text via top-$k$ sampling BIBREF7, fin..."]}
{"query": "What natural language(s) are the recipes written in?", "gt": "English", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The given context does not provide any information about the natural language(s) in which the recipes are written. Therefore, I don't know the answer to the question.", "source_documents": ["first 3-5 ingredients listed in each recipe. We decode recipe text via top-$k$ sampling BIBREF7, fin...", "internally consistent long texts BIBREF8, BIBREF9, BIBREF10. Here, we generate procedurally structur...", "$x$ is embedded via a technique embedding layer $\\mathcal {X}$ to $\\mathbf {x}\\in \\mathbb {R}^{d_x}$...", "Introduction\nIn the kitchen, we increasingly rely on instructions from cooking websites: recipes. A ..."]}
{"query": "What were their results on the new dataset?", "gt": "average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 18 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The results of their experiments on the new dataset were not mentioned in the given context.", "source_documents": ["a [SEP] token, using the final representation of the [CLS] token. The step entailment score is compu...", "$x$ is embedded via a technique embedding layer $\\mathcal {X}$ to $\\mathbf {x}\\in \\mathbb {R}^{d_x}$...", "Introduction\nIn the kitchen, we increasingly rely on instructions from cooking websites: recipes. A ...", "internally consistent long texts BIBREF8, BIBREF9, BIBREF10. Here, we generate procedurally structur..."]}
{"query": "What are the baseline models?", "gt": "name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 10 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The baseline models mentioned in the context are the Nearest-Neighbor model (NN) and the Encoder-Decoder baseline with ingredient attention (Enc-Dec).", "source_documents": ["$x$ is embedded via a technique embedding layer $\\mathcal {X}$ to $\\mathbf {x}\\in \\mathbb {R}^{d_x}$...", "first 3-5 ingredients listed in each recipe. We decode recipe text via top-$k$ sampling BIBREF7, fin...", "internally consistent long texts BIBREF8, BIBREF9, BIBREF10. Here, we generate procedurally structur...", "Prior Name model further suggests the addition of lemon and mint, which are reasonably associated wi..."]}
{"query": "How did they obtain the interactions?", "gt": "from Food.com", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The interactions were obtained from user reviews on the Food.com website. However, the specific method of obtaining the interactions is not mentioned in the given context. Therefore, we don't know the exact details of how they obtained the interactions. As for the suggested output length, there is no mention of a specific suggested output length in the given context.", "source_documents": ["first 3-5 ingredients listed in each recipe. We decode recipe text via top-$k$ sampling BIBREF7, fin...", "$x$ is embedded via a technique embedding layer $\\mathcal {X}$ to $\\mathbf {x}\\in \\mathbb {R}^{d_x}$...", "states as $\\lbrace \\mathbf {i}_{\\text{enc},j} \\in \\mathbb {R}^{2d_h}\\rbrace $. The caloric level is ...", "internally consistent long texts BIBREF8, BIBREF9, BIBREF10. Here, we generate procedurally structur..."]}
{"query": "Where do they get the recipes from?", "gt": "from Food.com", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The recipes used in this study are collected from Food.com, a cooking website. The dataset consists of 230K+ recipe texts and 1M+ user interactions (reviews) over 18 years (2000-2018).", "source_documents": ["$x$ is embedded via a technique embedding layer $\\mathcal {X}$ to $\\mathbf {x}\\in \\mathbb {R}^{d_x}$...", "first 3-5 ingredients listed in each recipe. We decode recipe text via top-$k$ sampling BIBREF7, fin...", "Introduction\nIn the kitchen, we increasingly rely on instructions from cooking websites: recipes. A ...", "internally consistent long texts BIBREF8, BIBREF9, BIBREF10. Here, we generate procedurally structur..."]}
{"query": "what are their results on the constructed dataset?", "gt": "AutoJudge consistently and significantly outperforms all the baselines, RC models achieve better performance than most text classification models (excluding GRU+Attention), Comparing with conventional RC models, AutoJudge achieves significant improvement", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 29 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The results on the constructed dataset are not mentioned in the given context.", "source_documents": ["collect INLINEFORM0 cases from China Judgments Online, among which INLINEFORM1 cases are for trainin...", "last hidden state in the GRU, which will be introduced in the following part. INLINEFORM1 is a weigh...", "inputs, i.e., query and passage, should be modeled separately.\nDespite the introduction of the neura...", "LRC framework. As shown in Fig. FIGREF6 , AutoJudge consists of three flexible modules, including a ..."]}
{"query": "what evaluation metrics are reported?", "gt": "precision, recall, F1 and accuracy", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 5 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "precision, recall, F1, accuracy", "source_documents": ["last hidden state in the GRU, which will be introduced in the following part. INLINEFORM1 is a weigh...", "low-frequency words are replaced with the mark <UNK>. The hidden size of GRU is set to 128 for each ...", "collect INLINEFORM0 cases from China Judgments Online, among which INLINEFORM1 cases are for trainin...", "LRC framework. As shown in Fig. FIGREF6 , AutoJudge consists of three flexible modules, including a ..."]}
{"query": "what civil field is the dataset about?", "gt": "divorce", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The dataset is about divorce proceedings, a civil field of cases.", "source_documents": ["last hidden state in the GRU, which will be introduced in the following part. INLINEFORM1 is a weigh...", "collect INLINEFORM0 cases from China Judgments Online, among which INLINEFORM1 cases are for trainin...", "dates, which are anonymized.\nConclusion\nIn this paper, we explore the task of predicting judgments o...", "inputs, i.e., query and passage, should be modeled separately.\nDespite the introduction of the neura..."]}
{"query": "what are the state-of-the-art models?", "gt": "SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 10 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The state-of-the-art models mentioned in the given context are \"r-net\" and \"AoA\" models.", "source_documents": ["low-frequency words are replaced with the mark <UNK>. The hidden size of GRU is set to 128 for each ...", "last hidden state in the GRU, which will be introduced in the following part. INLINEFORM1 is a weigh...", "collect INLINEFORM0 cases from China Judgments Online, among which INLINEFORM1 cases are for trainin...", "dates, which are anonymized.\nConclusion\nIn this paper, we explore the task of predicting judgments o..."]}
{"query": "what is the size of the real-world civil case dataset?", "gt": "100 000 documents", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 3 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The size of the real-world civil case dataset is not mentioned in the given context.", "source_documents": ["collect INLINEFORM0 cases from China Judgments Online, among which INLINEFORM1 cases are for trainin...", "inputs, i.e., query and passage, should be modeled separately.\nDespite the introduction of the neura...", "the quality of the retrieved law articles would affect the final performance. As is shown in Table T...", "last hidden state in the GRU, which will be introduced in the following part. INLINEFORM1 is a weigh..."]}
{"query": "what datasets are used in the experiment?", "gt": "build a new one, collect INLINEFORM0 cases from China Judgments Online", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 11 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "In the experiment, a new dataset is built using cases from China Judgments Online. The dataset includes a total of INLINEFORM1 cases for training, INLINEFORM2 cases for validation, and INLINEFORM3 cases for testing.", "source_documents": ["last hidden state in the GRU, which will be introduced in the following part. INLINEFORM1 is a weigh...", "collect INLINEFORM0 cases from China Judgments Online, among which INLINEFORM1 cases are for trainin...", "achieves comparable performance with RC models.\n(3) Comparing with conventional RC models, AutoJudge...", "LRC framework. As shown in Fig. FIGREF6 , AutoJudge consists of three flexible modules, including a ..."]}
{"query": "What domains are covered in the corpus?", "gt": "No specific domain is covered in the corpus.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 8 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The passage does not provide information about the domains covered in the corpus.", "source_documents": ["German and 38 hours BIBREF19 for French). Moreover, CoVoST has a total of 18 hours of Dutch speeches...", "Introduction\nEnd-to-end speech-to-text translation (ST) has attracted much attention recently BIBREF...", "resource in CoVoST and it is difficult to train decent models without additional data or pre-trainin...", "manually inspected examples where the translation had a high perplexity and sent them back to transl..."]}
{"query": "What is the architecture of their model?", "gt": "follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 14 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The architecture of their model is not explicitly mentioned in the given context.", "source_documents": ["resource in CoVoST and it is difficult to train decent models without additional data or pre-trainin...", "torchaudio. The features are normalized to 0 mean and 1.0 standard deviation. We remove samples havi...", "German and 38 hours BIBREF19 for French). Moreover, CoVoST has a total of 18 hours of Dutch speeches...", "manually inspected examples where the translation had a high perplexity and sent them back to transl..."]}
{"query": "How was the dataset collected?", "gt": "Contributors record voice clips by reading from a bank of donated sentences.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The dataset was collected through crowdsourcing and donations, where contributors recorded voice clips by reading from a bank of donated sentences. The voice clips were validated by at least two other users.", "source_documents": ["train-development-test split, we also provide baseline models, including, to our knowledge, the firs...", "resource in CoVoST and it is difficult to train decent models without additional data or pre-trainin...", "German and 38 hours BIBREF19 for French). Moreover, CoVoST has a total of 18 hours of Dutch speeches...", "manually inspected examples where the translation had a high perplexity and sent them back to transl..."]}
{"query": "Which languages are part of the corpus?", "gt": "French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The languages that are part of the corpus are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian, and Chinese.", "source_documents": ["manually inspected examples where the translation had a high perplexity and sent them back to transl...", "German and 38 hours BIBREF19 for French). Moreover, CoVoST has a total of 18 hours of Dutch speeches...", "resource in CoVoST and it is difficult to train decent models without additional data or pre-trainin...", "Introduction\nEnd-to-end speech-to-text translation (ST) has attracted much attention recently BIBREF..."]}
{"query": "How is the quality of the data empirically evaluated? ", "gt": "Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 23 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The quality of the data is empirically evaluated through various measures. This includes conducting sentence-level BLEU scores to compare human translations with automatic translations, manually inspecting examples with identical source transcript and translation, measuring the perplexity of the translations using a language model, computing the ratio of English characters in the translations, calculating similarity scores between transcripts and translations, and performing sanity checks on the overlaps of train, development, and test sets.", "source_documents": ["manually inspected examples where the translation had a high perplexity and sent them back to transl...", "resource in CoVoST and it is difficult to train decent models without additional data or pre-trainin...", "German and 38 hours BIBREF19 for French). Moreover, CoVoST has a total of 18 hours of Dutch speeches...", "train-development-test split, we also provide baseline models, including, to our knowledge, the firs..."]}
{"query": "Is the data in CoVoST annotated for dialect?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "There is no information provided in the given context about whether the data in CoVoST is annotated for dialect.", "source_documents": ["resource in CoVoST and it is difficult to train decent models without additional data or pre-trainin...", "German and 38 hours BIBREF19 for French). Moreover, CoVoST has a total of 18 hours of Dutch speeches...", "manually inspected examples where the translation had a high perplexity and sent them back to transl...", "train-development-test split, we also provide baseline models, including, to our knowledge, the firs..."]}
{"query": "Is Arabic one of the 11 languages in CoVost?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "No.", "source_documents": ["resource in CoVoST and it is difficult to train decent models without additional data or pre-trainin...", "German and 38 hours BIBREF19 for French). Moreover, CoVoST has a total of 18 hours of Dutch speeches...", "train-development-test split, we also provide baseline models, including, to our knowledge, the firs...", "manually inspected examples where the translation had a high perplexity and sent them back to transl..."]}
{"query": "What kind of model do they use?", "gt": "visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 39 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The authors of the study use a joint model that combines visual features learned through Inception V3 with textual features learned through a biLSTM (bidirectional Long Short-Term Memory) model.", "source_documents": ["not improve for 20 epochs. For evaluation, we use (micro-)accuracy, following previous studies BIBRE...", "topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mec...", "subset.\nExperimental Setting\nAs discussed above, our model has two main components \u2014 biLSTM and Ince...", "document-level representation, which is combined with hand-crafted features as side information. BIB..."]}
{"query": "What kind of model do they use?", "gt": "Inception V3, biLSTM", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 3 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Joint Model", "source_documents": ["not improve for 20 epochs. For evaluation, we use (micro-)accuracy, following previous studies BIBRE...", "topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mec...", "subset.\nExperimental Setting\nAs discussed above, our model has two main components \u2014 biLSTM and Ince...", "document-level representation, which is combined with hand-crafted features as side information. BIB..."]}
{"query": "Did they release their data set of academic papers?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The passage does not provide information about whether or not they released their data set of academic papers.", "source_documents": ["subset.\nExperimental Setting\nAs discussed above, our model has two main components \u2014 biLSTM and Ince...", "not improve for 20 epochs. For evaluation, we use (micro-)accuracy, following previous studies BIBRE...", "https://en.wikipedia.org/wiki/Category:Featured_articles. This resulted in around 5K FA, 28K GA, 212...", "document-level representation, which is combined with hand-crafted features as side information. BIB..."]}
{"query": "Do the methods that work best on academic papers also work best on Wikipedia?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Yes.", "source_documents": ["not improve for 20 epochs. For evaluation, we use (micro-)accuracy, following previous studies BIBRE...", "topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mec...", "as images and visual layout, can be captured by an image classifier, at a level comparable to a text...", "the performance of Joint is statistically significant ( $p<0.05$ ). This shows that the textual and ..."]}
{"query": "What is their system's absolute accuracy?", "gt": "59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 26 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The text does not provide the absolute accuracy of their system.", "source_documents": ["not improve for 20 epochs. For evaluation, we use (micro-)accuracy, following previous studies BIBRE...", "topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mec...", "Introduction\nThe task of document quality assessment is to automatically assess a document according...", "the performance of Joint is statistically significant ( $p<0.05$ ). This shows that the textual and ..."]}
{"query": "Which is more useful, visual or textual features?", "gt": "It depends on the dataset. Experimental results over two datasets reveal that textual and visual features are complementary. ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 18 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Based on the provided context, it can be inferred that both visual and textual features are useful for document quality assessment. The Joint model, which combines visual and textual embeddings, achieves the highest accuracy in most cases, indicating that the two types of features complement each other. However, it is difficult to determine which is more useful based solely on the given information.", "source_documents": ["the image that contributes to the GA class prediction (capturing the length of the article body). Fr...", "topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mec...", "Introduction\nThe task of document quality assessment is to automatically assess a document according...", "the performance of Joint is statistically significant ( $p<0.05$ ). This shows that the textual and ..."]}
{"query": "Which languages do they use?", "gt": "English", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The context does not provide any information about the languages used in the study. Therefore, we don't know which languages they use.", "source_documents": ["not improve for 20 epochs. For evaluation, we use (micro-)accuracy, following previous studies BIBRE...", "subset.\nExperimental Setting\nAs discussed above, our model has two main components \u2014 biLSTM and Ince...", "document-level representation, which is combined with hand-crafted features as side information. BIB...", "as images and visual layout, can be captured by an image classifier, at a level comparable to a text..."]}
{"query": "How large is their data set?", "gt": "a sample of  29,794 wikipedia articles and 2,794 arXiv papers ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 10 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The size of their dataset is not mentioned in the given context.", "source_documents": ["not improve for 20 epochs. For evaluation, we use (micro-)accuracy, following previous studies BIBRE...", "subset.\nExperimental Setting\nAs discussed above, our model has two main components \u2014 biLSTM and Ince...", "Introduction\nThe task of document quality assessment is to automatically assess a document according...", "topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mec..."]}
{"query": "Where do they get their ground truth quality judgments?", "gt": "quality class labels assigned by the Wikipedia community, a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 49 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The ground truth quality judgments are obtained from the Wikipedia community. Wikipedia articles are labeled with quality classes by Wikipedia reviewers or registered users who discuss and reach consensus through the article's talk page.", "source_documents": ["not improve for 20 epochs. For evaluation, we use (micro-)accuracy, following previous studies BIBRE...", "subset.\nExperimental Setting\nAs discussed above, our model has two main components \u2014 biLSTM and Ince...", "document-level representation, which is combined with hand-crafted features as side information. BIB...", "5), the basic Inception model applies multiple convolution filters in parallel and concatenates the ..."]}
{"query": "What is the seed lexicon?", "gt": "a vocabulary of positive and negative predicates that helps determine the polarity score of an event", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 16 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The seed lexicon is a lexicon consisting of 15 positive words and 15 negative words.", "source_documents": ["into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary ...", "$x_{i2}$, respectively. $N_{\\rm AL}$ is the total number of AL pairs, and $\\lambda _{\\rm AL}$ is a h...", "and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon a...", "TABREF16.\nExperiments ::: Dataset ::: ACP (ACP Corpus)\nWe used the latest version of the ACP Corpus ..."]}
{"query": "What is the seed lexicon?", "gt": "seed lexicon consists of positive and negative predicates", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 8 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The seed lexicon consists of 15 positive words and 15 negative words.", "source_documents": ["into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary ...", "$x_{i2}$, respectively. $N_{\\rm AL}$ is the total number of AL pairs, and $\\lambda _{\\rm AL}$ is a h...", "and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon a...", "TABREF16.\nExperiments ::: Dataset ::: ACP (ACP Corpus)\nWe used the latest version of the ACP Corpus ..."]}
{"query": "What are the results?", "gt": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 94 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The results of the experiment are not mentioned in the given context. Therefore, I don't know the specific results.", "source_documents": ["into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary ...", "$x_{i2}$, respectively. $N_{\\rm AL}$ is the total number of AL pairs, and $\\lambda _{\\rm AL}$ is a h...", "TABREF16.\nExperiments ::: Dataset ::: ACP (ACP Corpus)\nWe used the latest version of the ACP Corpus ...", "and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon a..."]}
{"query": "How are relations used to propagate polarity?", "gt": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 19 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Relations are used to propagate polarity by linking events together based on discourse relations. For example, if events x1 and x2 are in a Cause relation and the seed lexicon suggests that x2 is positive, then x1 is also likely to be positive because it triggers a positive emotion. Similarly, if x1 and x2 are in a Concession relation, the polarity of x2 can be propagated to x1. The method exploits the tendency of events in the same relation to have the same polarity or reverse polarity.", "source_documents": ["Introduction\nAffective events BIBREF0 are events that typically affect people in positive or negativ...", "purely supervised baseline when labeled data were small.\nRelated Work\nLearning affective events is c...", "\u201c\u80a9\u3092\u843d\u3068\u3059\" (lit. drop one's shoulders) is an idiom that expresses a disappointed feeling. The examples ...", "into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary ..."]}
{"query": "How big is the Japanese data?", "gt": "The ACP corpus has around 700k events split into positive and negative polarity ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 13 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The size of the Japanese data is not mentioned in the given context, so it is not possible to determine how big the Japanese data is.", "source_documents": ["into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary ...", "TABREF16.\nExperiments ::: Dataset ::: ACP (ACP Corpus)\nWe used the latest version of the ACP Corpus ...", "$x_{i2}$, respectively. $N_{\\rm AL}$ is the total number of AL pairs, and $\\lambda _{\\rm AL}$ is a h...", "purely supervised baseline when labeled data were small.\nRelated Work\nLearning affective events is c..."]}
{"query": "What are labels available in dataset for supervision?", "gt": "negative, positive", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The labels available in the dataset for supervision are positive and negative.", "source_documents": ["and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon a...", "$x_{i2}$, respectively. $N_{\\rm AL}$ is the total number of AL pairs, and $\\lambda _{\\rm AL}$ is a h...", "TABREF16.\nExperiments ::: Dataset ::: ACP (ACP Corpus)\nWe used the latest version of the ACP Corpus ...", "\u201c\u80a9\u3092\u843d\u3068\u3059\" (lit. drop one's shoulders) is an idiom that expresses a disappointed feeling. The examples ..."]}
{"query": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?", "gt": "3%", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The answer to the question cannot be determined from the given context.", "source_documents": ["purely supervised baseline when labeled data were small.\nRelated Work\nLearning affective events is c...", "Introduction\nAffective events BIBREF0 are events that typically affect people in positive or negativ...", "TABREF16.\nExperiments ::: Dataset ::: ACP (ACP Corpus)\nWe used the latest version of the ACP Corpus ...", "and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon a..."]}
{"query": "How does their model learn using mostly raw data?", "gt": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 14 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Based on the given context, the model learns using mostly raw data by extracting discourse-tagged event pairs from a large raw corpus. These event pairs consist of former and latter events and are categorized into different types based on the seed lexicon and discourse relation. The model then optimizes the parameters of the polarity function using the automatically labeled event pairs, cause pairs, and concession pairs. The loss functions are constructed using mean squared error, and the model learns to predict the sentiment polarity score of an event using a neural network. The output length of the suggested model is not mentioned in the given context.", "source_documents": ["Pairs\nOur method requires a very small seed lexicon and a large raw corpus. We assume that we can au...", "purely supervised baseline when labeled data were small.\nRelated Work\nLearning affective events is c...", "TABREF16.\nExperiments ::: Dataset ::: ACP (ACP Corpus)\nWe used the latest version of the ACP Corpus ...", "into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary ..."]}
{"query": "How big is seed lexicon used for training?", "gt": "30 words", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The size of the seed lexicon used for training is not mentioned in the given context.", "source_documents": ["TABREF16.\nExperiments ::: Dataset ::: ACP (ACP Corpus)\nWe used the latest version of the ACP Corpus ...", "Pairs\nOur method requires a very small seed lexicon and a large raw corpus. We assume that we can au...", "and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon a...", "\u201c\u80a9\u3092\u843d\u3068\u3059\" (lit. drop one's shoulders) is an idiom that expresses a disappointed feeling. The examples ..."]}
{"query": "How large is raw corpus used for training?", "gt": "100 million sentences", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 3 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The size of the raw corpus used for training is not specified in the given context.", "source_documents": ["Pairs\nOur method requires a very small seed lexicon and a large raw corpus. We assume that we can au...", "TABREF16.\nExperiments ::: Dataset ::: ACP (ACP Corpus)\nWe used the latest version of the ACP Corpus ...", "and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon a...", "Introduction\nAffective events BIBREF0 are events that typically affect people in positive or negativ..."]}
{"query": "What data is the language model pretrained on?", "gt": "Chinese general corpus", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 3 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The context does not provide information on what data the language model is pretrained on.", "source_documents": ["problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved im...", "the original sentence will be tagged a label following a tag scheme. In this paper we recognize the ...", "Firstly, we investigate the performance on model without two-stage training and named entity informa...", "Introduction\nClinical text structuring (CTS) is a critical task for fetching medical research data f..."]}
{"query": "What baselines is the proposed model compared against?", "gt": "BERT-Base, QANet", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The given context does not provide information about the baselines that the proposed model is compared against.", "source_documents": ["the original sentence will be tagged a label following a tag scheme. In this paper we recognize the ...", "problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved im...", "Firstly, we investigate the performance on model without two-stage training and named entity informa...", "Introduction\nClinical text structuring (CTS) is a critical task for fetching medical research data f..."]}
{"query": "How is the clinical text structuring task defined?", "gt": "Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained., Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 78 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The clinical text structuring task is defined as extracting or generating a key-value pair from a sequence of paragraph text. The key, denoted as $Q$, is usually a query term, and the value, denoted as $V$, is the result of the query term based on the paragraph text. The task aims to find the most related text from the original paragraph text. The suggested output length is approximately 78 words.", "source_documents": ["the original sentence will be tagged a label following a tag scheme. In this paper we recognize the ...", "problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved im...", "Introduction\nClinical text structuring (CTS) is a critical task for fetching medical research data f...", "entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output..."]}
{"query": "How is the clinical text structuring task defined?", "gt": "CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 24 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The clinical text structuring task involves extracting or generating a key-value pair from a sequence of paragraph text. The key, denoted as Q, is a query term such as \"proximal resection margin,\" and the value, denoted as V, is the result of the query term Q according to the paragraph text. The task is to find the answer-related substring within the paragraph text, which is typically around 24 words in length.", "source_documents": ["the original sentence will be tagged a label following a tag scheme. In this paper we recognize the ...", "problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved im...", "Introduction\nClinical text structuring (CTS) is a critical task for fetching medical research data f...", "entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output..."]}
{"query": "What are the specific tasks being unified?", "gt": " three types of questions, namely tumor size, proximal resection margin and distal resection margin", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 14 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The specific tasks being unified are the clinical text structuring (CTS) tasks. The suggested output length is around 14 words.", "source_documents": ["problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved im...", "the original sentence will be tagged a label following a tag scheme. In this paper we recognize the ...", "Introduction\nClinical text structuring (CTS) is a critical task for fetching medical research data f...", "Firstly, we investigate the performance on model without two-stage training and named entity informa..."]}
{"query": "Is all text in this dataset a question, or are there unrelated sentences in between questions?", "gt": "the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 24 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Based on the provided information, there is no specific mention of all the text in the dataset being questions or unrelated sentences in between questions. Therefore, we cannot determine if all text in the dataset is a question or if there are unrelated sentences in between questions.", "source_documents": ["answer of the output for start word and end word respectively.\nThe Proposed Model for QA-CTS Task ::...", "problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved im...", "the original sentence will be tagged a label following a tag scheme. In this paper we recognize the ...", "Firstly, we investigate the performance on model without two-stage training and named entity informa..."]}
{"query": "How many questions are in the dataset?", "gt": "2,714 ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "There are 2,714 question-answer pairs in the dataset.", "source_documents": ["problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved im...", "proposed model achieved a 5.64% score improvement in EM-score and a 3.69% score improvement in F$_1$...", "the original sentence will be tagged a label following a tag scheme. In this paper we recognize the ...", "answer of the output for start word and end word respectively.\nThe Proposed Model for QA-CTS Task ::..."]}
{"query": "How they introduce domain-specific features into pre-trained language model?", "gt": "integrate clinical named entity information into pre-trained language model", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 9 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "In the given context, it is mentioned that they introduce domain-specific features into the pre-trained language model BERT. However, the specific details of how they introduce these features are not provided. Therefore, the exact method used to introduce domain-specific features into the pre-trained language model is unknown. Sorry for the inconvenience.", "source_documents": ["section, we present an effective model for the question answering based clinical text structuring (Q...", "problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved im...", "BIBREF4 use large amount of data to automatically model the specific task. Topaz et al. BIBREF3 cons...", "the original sentence will be tagged a label following a tag scheme. In this paper we recognize the ..."]}
{"query": "How big is QA-CTS task dataset?", "gt": "17,833 sentences, 826,987 characters and 2,714 question-answer pairs", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 8 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The exact size of the QA-CTS task dataset is not mentioned in the given context.", "source_documents": ["the original sentence will be tagged a label following a tag scheme. In this paper we recognize the ...", "problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved im...", "proposed model achieved a 5.64% score improvement in EM-score and a 3.69% score improvement in F$_1$...", "Introduction\nClinical text structuring (CTS) is a critical task for fetching medical research data f..."]}
{"query": "What are strong baseline models in specific tasks?", "gt": "state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 10 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The passage does not provide information about strong baseline models in specific tasks.", "source_documents": ["this paper, we present a question answering based clinical text structuring (QA-CTS) task, which uni...", "entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output...", "problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved im...", "Introduction\nClinical text structuring (CTS) is a critical task for fetching medical research data f..."]}
{"query": "What is the previous work's model?", "gt": "Ternary Trans-CNN", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Ternary Trans-CNN model", "source_documents": ["English. For example to express love, a code mixed spelling, specially when used social platforms mi...", "belonged to.\nBased on all the experiments we conducted below model had best performance related to m...", "Introduction\nHinglish is a linguistic blend of Hindi (very widely spoken language in India) and Engl...", "dataset are below:\nDataset and Features ::: Challenges\nThe obtained data set had many challenges and..."]}
{"query": "What dataset is used?", "gt": "HEOT , A labelled dataset for a corresponding english tweets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 10 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The dataset used is the HEOT dataset, which consists of a set of cleaned tweets obtained from Twitter for conversations happening in the Indian subcontinent.", "source_documents": ["English. For example to express love, a code mixed spelling, specially when used social platforms mi...", "Introduction\nHinglish is a linguistic blend of Hindi (very widely spoken language in India) and Engl...", "dataset are below:\nDataset and Features ::: Challenges\nThe obtained data set had many challenges and...", "belonged to.\nBased on all the experiments we conducted below model had best performance related to m..."]}
{"query": "How big is the dataset?", "gt": "3189 rows of text messages", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 5 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The size of the dataset is not mentioned in the given context.", "source_documents": ["Introduction\nHinglish is a linguistic blend of Hindi (very widely spoken language in India) and Engl...", "dataset are below:\nDataset and Features ::: Challenges\nThe obtained data set had many challenges and...", "belonged to.\nBased on all the experiments we conducted below model had best performance related to m...", "English. For example to express love, a code mixed spelling, specially when used social platforms mi..."]}
{"query": "How big is the dataset?", "gt": "Resulting dataset was 7934 messages for train and 700 messages for test.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The dataset used in the study had 3189 rows of text messages.", "source_documents": ["Introduction\nHinglish is a linguistic blend of Hindi (very widely spoken language in India) and Engl...", "English. For example to express love, a code mixed spelling, specially when used social platforms mi...", "dataset are below:\nDataset and Features ::: Challenges\nThe obtained data set had many challenges and...", "belonged to.\nBased on all the experiments we conducted below model had best performance related to m..."]}
{"query": "How is the dataset collected?", "gt": "A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al, HEOT obtained from one of the past studies done by Mathur et al", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 32 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The dataset used in the study was collected from social media platform Twitter. The specific method or process of collecting the dataset is not mentioned in the given context.", "source_documents": ["belonged to.\nBased on all the experiments we conducted below model had best performance related to m...", "Introduction\nHinglish is a linguistic blend of Hindi (very widely spoken language in India) and Engl...", "dataset are below:\nDataset and Features ::: Challenges\nThe obtained data set had many challenges and...", "English. For example to express love, a code mixed spelling, specially when used social platforms mi..."]}
{"query": "Was each text augmentation technique experimented individually?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The provided context does not mention whether each text augmentation technique was experimented individually or not.", "source_documents": ["dataset are below:\nDataset and Features ::: Challenges\nThe obtained data set had many challenges and...", "that was available for this task was very limited in number of examples and thus as noted above few ...", "English. For example to express love, a code mixed spelling, specially when used social platforms mi...", "belonged to.\nBased on all the experiments we conducted below model had best performance related to m..."]}
{"query": "What models do previous work use?", "gt": "Ternary Trans-CNN , Hybrid multi-channel CNN and LSTM", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 8 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Ternary Trans-CNN, Hybrid multi-channel CNN and LSTM", "source_documents": ["English. For example to express love, a code mixed spelling, specially when used social platforms mi...", "Introduction\nHinglish is a linguistic blend of Hindi (very widely spoken language in India) and Engl...", "belonged to.\nBased on all the experiments we conducted below model had best performance related to m...", "dataset are below:\nDataset and Features ::: Challenges\nThe obtained data set had many challenges and..."]}
{"query": "Does the dataset contain content from various social media platforms?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Based on the given context, it is not explicitly mentioned whether the dataset contains content from various social media platforms. Therefore, we cannot determine if the dataset includes content from platforms other than Twitter.", "source_documents": ["Introduction\nHinglish is a linguistic blend of Hindi (very widely spoken language in India) and Engl...", "English. For example to express love, a code mixed spelling, specially when used social platforms mi...", "detective offensive vs non offensive tweets and messages written in Hinglish in social media. The ut...", "that was available for this task was very limited in number of examples and thus as noted above few ..."]}
{"query": "What additional techniques are incorporated?", "gt": "incorporating coding syntax tree model", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 5 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The additional technique incorporated is the use of Abstract Syntax Tree (AST) for more accurate target code generation for each line.", "source_documents": ["the source texts and another for the code. Vocabulary generation is done by tokenization of words. A...", "and Japanese. In the same year, Chris Q. et al. mapped natural language with simple if-this-then-tha...", "Introduction\nRemoving computer-human language barrier is an inevitable advancement researchers are t...", "validation data from the source corpus was 74.40% (Fig. FIGREF17).\nAlthough the generated code is in..."]}
{"query": "What dataset do they use?", "gt": "A parallel corpus where the source is an English expression of code and the target is Python code.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 18 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The dataset they use is a text-code parallel corpus with 18805 aligned data in it.", "source_documents": ["the source texts and another for the code. Vocabulary generation is done by tokenization of words. A...", "Introduction\nRemoving computer-human language barrier is an inevitable advancement researchers are t...", "through XML mapping BIBREF2. They tried to convert codes from C++ to Java using XML mapping as an in...", "and Japanese. In the same year, Chris Q. et al. mapped natural language with simple if-this-then-tha..."]}
{"query": "Do they compare to other models?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "No, the given context does not mention any comparison to other models. Therefore, it is not possible to determine if they compare to other models or not.", "source_documents": ["the source texts and another for the code. Vocabulary generation is done by tokenization of words. A...", "Introduction\nRemoving computer-human language barrier is an inevitable advancement researchers are t...", "and Japanese. In the same year, Chris Q. et al. mapped natural language with simple if-this-then-tha...", "through XML mapping BIBREF2. They tried to convert codes from C++ to Java using XML mapping as an in..."]}
{"query": "What is the architecture of the system?", "gt": "seq2seq translation", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The architecture of the system is a Neural Machine Translation (NMT) model with three types of Recurrent Neural Network (RNN) layers - an encoder layer, a decoder layer, and an output layer. These layers together form a LSTM model. The model takes the tokenized source and target text as input and uses it for training. The encoder cells, part of the encoder RNN layer, process the source text tokens, while the decoder cells, part of the decoder RNN layer, process the target text tokens. Attention mechanism is applied at each target cell state to generate the prediction of the next target token.", "source_documents": ["the source texts and another for the code. Vocabulary generation is done by tokenization of words. A...", "validation data from the source corpus was 74.40% (Fig. FIGREF17).\nAlthough the generated code is in...", "Introduction\nRemoving computer-human language barrier is an inevitable advancement researchers are t...", "through XML mapping BIBREF2. They tried to convert codes from C++ to Java using XML mapping as an in..."]}
{"query": "What additional techniques could be incorporated to further improve accuracy?", "gt": "phrase-based word embedding, Abstract Syntax Tree(AST)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 6 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Some possible techniques that could be incorporated to further improve accuracy include:\n1. Incorporating coding syntax tree models to improve coherence and predict the correct code tokens.\n2. Utilizing phrase-based word embedding for improved vocabulary mapping.\n3. Implementing Abstract Syntax Tree (AST) to generate more accurate target code for each line.", "source_documents": ["the source texts and another for the code. Vocabulary generation is done by tokenization of words. A...", "and Japanese. In the same year, Chris Q. et al. mapped natural language with simple if-this-then-tha...", "validation data from the source corpus was 74.40% (Fig. FIGREF17).\nAlthough the generated code is in...", "Introduction\nRemoving computer-human language barrier is an inevitable advancement researchers are t..."]}
{"query": "What programming language is target language?", "gt": "Python", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The target programming language is Python.", "source_documents": ["the source texts and another for the code. Vocabulary generation is done by tokenization of words. A...", "Introduction\nRemoving computer-human language barrier is an inevitable advancement researchers are t...", "and Japanese. In the same year, Chris Q. et al. mapped natural language with simple if-this-then-tha...", "through XML mapping BIBREF2. They tried to convert codes from C++ to Java using XML mapping as an in..."]}
{"query": "What dataset is used to measure accuracy?", "gt": "validation data", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The dataset used to measure accuracy is the validation data from the source corpus.", "source_documents": ["the source texts and another for the code. Vocabulary generation is done by tokenization of words. A...", "through XML mapping BIBREF2. They tried to convert codes from C++ to Java using XML mapping as an in...", "and Japanese. In the same year, Chris Q. et al. mapped natural language with simple if-this-then-tha...", "validation data from the source corpus was 74.40% (Fig. FIGREF17).\nAlthough the generated code is in..."]}
{"query": "What corpus was the source of the OpenIE extractions?", "gt": "domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 18 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The corpus that was the source of the OpenIE extractions is the Sentence corpus, consisting of 80K sentences and 280 GB of plain text extracted from web pages.", "source_documents": ["graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules ...", "an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the indiv...", "phrases found in Open IE tuples.\nCompared to a curated KB, it is easy to find Open IE tuples that ma...", "Introduction\nEffective question answering (QA) systems have been a long-standing quest of AI researc..."]}
{"query": "What is the accuracy of the proposed technique?", "gt": "51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 32 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The given context does not provide information about the accuracy of the proposed technique. Therefore, I don't know the accuracy of the proposed technique.", "source_documents": ["Introduction\nEffective question answering (QA) systems have been a long-standing quest of AI researc...", "an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the indiv...", "phrases found in Open IE tuples.\nCompared to a curated KB, it is easy to find Open IE tuples that ma...", "higher grades. Errors due to lossy IE and misalignments suggest future work in incorporating context..."]}
{"query": "Is an entity linking process used?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Based on the given context, it is not mentioned whether an entity linking process is used or not.", "source_documents": ["phrases found in Open IE tuples.\nCompared to a curated KB, it is easy to find Open IE tuples that ma...", "Introduction\nEffective question answering (QA) systems have been a long-standing quest of AI researc...", "an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the indiv...", "graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules ..."]}
{"query": "Are the OpenIE extractions all triples?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "No, the OpenIE extractions are not all triples. The exact output length is not specified in the given context.", "source_documents": ["phrases found in Open IE tuples.\nCompared to a curated KB, it is easy to find Open IE tuples that ma...", "higher grades. Errors due to lossy IE and misalignments suggest future work in incorporating context...", "Introduction\nEffective question answering (QA) systems have been a long-standing quest of AI researc...", "graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules ..."]}
{"query": "What method was used to generate the OpenIE extractions?", "gt": "for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S, take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 56 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The text does not provide information about the method used to generate the OpenIE extractions.", "source_documents": ["an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the indiv...", "phrases found in Open IE tuples.\nCompared to a curated KB, it is easy to find Open IE tuples that ma...", "graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules ...", "Introduction\nEffective question answering (QA) systems have been a long-standing quest of AI researc..."]}
{"query": "Can the method answer multi-hop questions?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Yes, the system can answer multi-hop questions by combining facts from multiple tuples to reason and provide an answer.", "source_documents": ["phrases found in Open IE tuples.\nCompared to a curated KB, it is easy to find Open IE tuples that ma...", "an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the indiv...", "Introduction\nEffective question answering (QA) systems have been a long-standing quest of AI researc...", "graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules ..."]}
{"query": "What was the textual source to which OpenIE was applied?", "gt": "domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 18 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The textual source to which OpenIE was applied is sentences in the KB and additional sentences extracted on the fly from text.", "source_documents": ["an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the indiv...", "phrases found in Open IE tuples.\nCompared to a curated KB, it is easy to find Open IE tuples that ma...", "Introduction\nEffective question answering (QA) systems have been a long-standing quest of AI researc...", "N/n_x) \\\\ &\\textit {tf-idf}(t, q)=\\sum _{x \\in t\\cap q} idf(x) $\nwhere $N$ is the number of tuples i..."]}
{"query": "What OpenIE method was used to generate the extractions?", "gt": "for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S, take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 56 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The context does not provide information about the specific OpenIE method used to generate the extractions.", "source_documents": ["an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the indiv...", "phrases found in Open IE tuples.\nCompared to a curated KB, it is easy to find Open IE tuples that ma...", "graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules ...", "Introduction\nEffective question answering (QA) systems have been a long-standing quest of AI researc..."]}
{"query": "Is their method capable of multi-hop reasoning?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Yes, the method described in the text is capable of multi-hop reasoning.", "source_documents": ["phrases found in Open IE tuples.\nCompared to a curated KB, it is easy to find Open IE tuples that ma...", "an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the indiv...", "Introduction\nEffective question answering (QA) systems have been a long-standing quest of AI researc...", "graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules ..."]}
{"query": "What models do they propose?", "gt": "Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), Textual Kernels Model (TKM)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The authors propose the use of feature fusion models inspired by previous works on combining visual and textual information. They mention the work of Gao et al. and Margffoy-Tuay et al. as references for their feature fusion scheme. However, the specific models they propose for hate speech detection are not mentioned in the given context.", "source_documents": ["methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn mul...", "follows:\n[noitemsep,leftmargin=*]\nWe propose the novel task of hate speech detection in multimodal p...", "Introduction\nSocial Media platforms such as Facebook, Twitter or Reddit have empowered individuals' ...", "if a tweet is hate speech or not. But a considerable amount of the images of the selected tweets con..."]}
{"query": "Are all tweets in English?", "gt": "Unanswerable", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Yes.", "source_documents": ["methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn mul...", "in the FCM model, three fully connected layers are used to reduce the dimensionality until the class...", "used for training.\nWe also experimented using hate scores for each tweet computed given the differen...", "Introduction\nSocial Media platforms such as Facebook, Twitter or Reddit have empowered individuals' ..."]}
{"query": "How large is the dataset?", "gt": " $150,000$ tweets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "150,000 tweets", "source_documents": ["methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn mul...", "follows:\n[noitemsep,leftmargin=*]\nWe propose the novel task of hate speech detection in multimodal p...", "used for training.\nWe also experimented using hate scores for each tweet computed given the differen...", "in the FCM model, three fully connected layers are used to reduce the dimensionality until the class..."]}
{"query": "What is the results of multimodal compared to unimodal models?", "gt": "Unimodal LSTM vs Best Multimodal (FCM)\n- F score: 0.703 vs 0.704\n- AUC: 0.732 vs 0.734 \n- Mean Accuracy: 68.3 vs 68.4 ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 23 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The proposed multimodal models for hate speech detection do not outperform the unimodal textual models.", "source_documents": ["follows:\n[noitemsep,leftmargin=*]\nWe propose the novel task of hate speech detection in multimodal p...", "used for training.\nWe also experimented using hate scores for each tweet computed given the differen...", "of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to t...", "methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn mul..."]}
{"query": "What is author's opinion on why current multimodal models cannot outperform models analyzing only text?", "gt": "Noisy data, Complexity and diversity of multimodal relations, Small set of multimodal examples", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 13 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The author believes that the complexity and diversity of the relationships between visual and textual elements in hate speech multimodal publications make it difficult for current multimodal models to learn.", "source_documents": ["multimodal models to use the visual information. Eventually, though, these models end up using almos...", "methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn mul...", "of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to t...", "Introduction\nSocial Media platforms such as Facebook, Twitter or Reddit have empowered individuals' ..."]}
{"query": "What metrics are used to benchmark the results?", "gt": "F-score, Area Under the ROC Curve (AUC), mean accuracy (ACC), Precision vs Recall plot, ROC curve (which plots the True Positive Rate vs the False Positive Rate)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 27 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The Precision vs Recall plot and the ROC curve are used to benchmark the results.", "source_documents": ["methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn mul...", "of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to t...", "used for training.\nWe also experimented using hate scores for each tweet computed given the differen...", "Introduction\nSocial Media platforms such as Facebook, Twitter or Reddit have empowered individuals' ..."]}
{"query": "How is data collected, manual collection or Twitter api?", "gt": "Twitter API", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Twitter API", "source_documents": ["methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn mul...", "Introduction\nSocial Media platforms such as Facebook, Twitter or Reddit have empowered individuals' ...", "extract it and also input it to our model. To do so, we use Google Vision API Text Detection module ...", "used for training.\nWe also experimented using hate scores for each tweet computed given the differen..."]}
{"query": "How many tweats does MMHS150k contains, 150000?", "gt": "$150,000$ tweets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "Yes, the MMHS150K dataset contains 150,000 tweets.", "source_documents": ["Introduction\nSocial Media platforms such as Facebook, Twitter or Reddit have empowered individuals' ...", "methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn mul...", "datasets:\n[noitemsep,leftmargin=*]\nRM BIBREF10: Formed by $2,435$ tweets discussing Refugees and Mus...", "of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to t..."]}
{"query": "What unimodal detection models were used?", "gt": " single layer LSTM with a 150-dimensional hidden state for hate / not hate classification", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 14 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The unimodal detection models used were a CNN for image features extraction (Google Inception v3 architecture) and a single layer LSTM for tweet text classification.", "source_documents": ["used for training.\nWe also experimented using hate scores for each tweet computed given the differen...", "if a tweet is hate speech or not. But a considerable amount of the images of the selected tweets con...", "methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn mul...", "follows:\n[noitemsep,leftmargin=*]\nWe propose the novel task of hate speech detection in multimodal p..."]}
{"query": "What different models for multimodal detection were proposed?", "gt": "Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), Textual Kernels Model (TKM)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The proposed models for multimodal detection were the LSTM model for hate speech detection using only text, the FCM model for hate speech detection using only images, and the multimodal models that combine both text and images for hate speech detection.", "source_documents": ["of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to t...", "if a tweet is hate speech or not. But a considerable amount of the images of the selected tweets con...", "methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn mul...", "follows:\n[noitemsep,leftmargin=*]\nWe propose the novel task of hate speech detection in multimodal p..."]}
{"query": "What annotations are available in the dataset - tweat used hate speach or not?", "gt": "No attacks to any community,  racist, sexist, homophobic, religion based attacks, attacks to other communities", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 15 words.", "evaluation": "f1", "retrieval-bm25-turbo-0613_pred": "The annotations available in the dataset indicate whether a tweet is classified as hate speech or not.", "source_documents": ["methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn mul...", "used for training.\nWe also experimented using hate scores for each tweet computed given the differen...", "multimodal models to use the visual information. Eventually, though, these models end up using almos...", "if a tweet is hate speech or not. But a considerable amount of the images of the selected tweets con..."]}
