{"query": "How did they obtain the dataset?", "gt": "public resources where suspicious Twitter accounts were annotated, list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 20 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The dataset was obtained based on two lists of annotated Twitter accounts from previous works. The first list consisted of 180 Twitter accounts annotated with various fake news types, while the second list included 32 Twitter accounts considered trustworthy by independent third parties. The accounts were selected from public resources and were categorized based on their factuality. Duplicate, media-based, and link-only tweets were removed from the dataset. The maximum amount of tweets allowed by the Twitter API was collected for each account.", "source_documents": ["BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha...", "on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources wher...", "top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen..."]}
{"query": "What activation function do they use in their model?", "gt": "relu, selu, tanh", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 3 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The activation function used in their model is not specified in the given context.", "source_documents": ["BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen...", "of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha...", "level, and especially from a textual perspective. Our contributions are:\n[leftmargin=4mm]\nWe propose..."]}
{"query": "What baselines do they compare to?", "gt": "Top-$k$ replies, likes, or re-tweets, FacTweet (tweet-level), LR + All Features (chunk-level), LR + All Features (tweet-level), Tweet2vec, LR + Bag-of-words", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 21 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The baselines they compare to include LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), and Top-$k$ replies, likes, or re-tweets.", "source_documents": ["on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources wher...", "top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen...", "BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha..."]}
{"query": "How are chunks defined?", "gt": "Chunks is group of tweets from single account that  is consecutive in time - idea is that this group can show secret intention of malicious accounts.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 26 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Chunks are defined as sorted sequences of tweets from a Twitter account's timeline. The tweets are sorted by posting date in ascending order and then split into N chunks. Each chunk consists of a sequence of tweets. The exact number of tweets in each chunk is not specified in the given context, but based on the suggested output length of around 26 words, it can be inferred that each chunk is expected to contain approximately 26 tweets.", "source_documents": ["of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha...", "level, and especially from a textual perspective. Our contributions are:\n[leftmargin=4mm]\nWe propose...", "BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "Introduction\nSocial media platforms have made the spreading of fake news easier, faster as well as a..."]}
{"query": "What features are extracted?", "gt": "Sentiment, Morality, Style, Words embeddings", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 5 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The features that are extracted include emotions, sentiment, morality, style, and word embeddings.", "source_documents": ["BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha...", "top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen...", "level, and especially from a textual perspective. Our contributions are:\n[leftmargin=4mm]\nWe propose..."]}
{"query": "What features are extracted?", "gt": "15 emotion types, sentiment classes, positive and negative, care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, and degradation, count of question marks, exclamation marks, consecutive characters and letters, links, hashtags, users' mentions, uppercase ratio, tweet length, words embeddings", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 39 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The features extracted in this study include emotions, sentiment, morality, style, and word embeddings.", "source_documents": ["of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha...", "BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "level, and especially from a textual perspective. Our contributions are:\n[leftmargin=4mm]\nWe propose...", "top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen..."]}
{"query": "Was the approach used in this work to detect fake news fully supervised?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Yes.", "source_documents": ["Introduction\nSocial media platforms have made the spreading of fake news easier, faster as well as a...", "on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources wher...", "of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha...", "level, and especially from a textual perspective. Our contributions are:\n[leftmargin=4mm]\nWe propose..."]}
{"query": "Based on this paper, what is the more predictive set of features to detect fake news?", "gt": "words embeddings, style, and morality features", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 6 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The more predictive set of features to detect fake news is the combination of semantic and stylistic features.", "source_documents": ["Introduction\nSocial media platforms have made the spreading of fake news easier, faster as well as a...", "of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha...", "on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources wher...", "level, and especially from a textual perspective. Our contributions are:\n[leftmargin=4mm]\nWe propose..."]}
{"query": "How big is the dataset used in this work?", "gt": "Total dataset size: 171 account (522967 tweets)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 7 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The size of the dataset used in this work is not mentioned in the given context.", "source_documents": ["BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha...", "top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen...", "on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources wher..."]}
{"query": "How is a \"chunk of posts\" defined in this work?", "gt": "chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 16 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "A \"chunk of posts\" in this work is defined as a sequence of tweets from a Twitter account that are sorted by posting date and split into groups. The suggested output length for each chunk is around 16 words.", "source_documents": ["of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha...", "level, and especially from a textual perspective. Our contributions are:\n[leftmargin=4mm]\nWe propose...", "BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "Introduction\nSocial media platforms have made the spreading of fake news easier, faster as well as a..."]}
{"query": "What baselines were used in this work?", "gt": "LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 21 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The baselines used in this work include LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), and Top-k replies, likes, or re-tweets.", "source_documents": ["on a list of 180 Twitter accounts from BIBREF1. This list was created based on public resources wher...", "BIBREF0, BIBREF1, BIBREF8, BIBREF9.\n[leftmargin=4mm]\nEmotion: We build an emotions vector using word...", "top replied tweets achieves the highest result. Therefore, we consider this as a baseline.\nExperimen...", "of semantic and stylistic features \u2013 how semantic and stylistic signature evolve across time and cha..."]}
{"query": "What models are used in the experiment?", "gt": "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 10 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The models used in the experiment are a bidirectional LSTM model (RNN), a Convolutional Neural Network (CNN) model, and a linear SVM model.", "source_documents": ["(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ...", "`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "Introduction\nOffensive content has become pervasive in social media and a reason of concern for gove...", "covertly aggressive, and overtly aggressive.\nBullying detection: Several studies have been published..."]}
{"query": "What are the differences between this dataset and pre-existing ones?", "gt": "no prior work has explored the target of the offensive language", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 11 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "This dataset differs from pre-existing ones by including annotation of the type and target of offensive language in social media. It is the first dataset to contain this type of annotation.", "source_documents": ["(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ...", "`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "organizations, situations, events, etc. making it more challenging for systems to learn discriminati...", "Introduction\nOffensive content has become pervasive in social media and a reason of concern for gove..."]}
{"query": "In what language are the tweets?", "gt": "English", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "English", "source_documents": ["`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ...", "Introduction\nOffensive content has become pervasive in social media and a reason of concern for gove...", "label a post as offensive if it contains any form of non-acceptable language (profanity) or a target..."]}
{"query": "What is the size of the new dataset?", "gt": "14,100 tweets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "14,100 tweets", "source_documents": ["`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ...", "organizations, situations, events, etc. making it more challenging for systems to learn discriminati...", "Introduction\nOffensive content has become pervasive in social media and a reason of concern for gove..."]}
{"query": "What is the size of the new dataset?", "gt": "Dataset contains total of 14100 annotations.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 6 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The size of the new dataset is 14,100 tweets.", "source_documents": ["`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ...", "organizations, situations, events, etc. making it more challenging for systems to learn discriminati...", "Introduction\nOffensive content has become pervasive in social media and a reason of concern for gove..."]}
{"query": "What kinds of offensive content are explored?", "gt": "non-targeted profanity and swearing, targeted insults such as cyberbullying, offensive content related to ethnicity, gender or sexual orientation, political affiliation, religious belief, and anything belonging to hate speech", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 28 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The paper explores different aspects of offensive language, including abusive language, aggression, bullying, toxic comments, hate speech, and offensive language targeted at specific individuals or groups.", "source_documents": ["`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "Introduction\nOffensive content has become pervasive in social media and a reason of concern for gove...", "label a post as offensive if it contains any form of non-acceptable language (profanity) or a target...", "(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ..."]}
{"query": "What is the best performing model?", "gt": "CNN ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The CNN model is the best performing model with a macro-F1 score of 0.80.", "source_documents": ["(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ...", "`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "covertly aggressive, and overtly aggressive.\nBullying detection: Several studies have been published...", "Introduction\nOffensive content has become pervasive in social media and a reason of concern for gove..."]}
{"query": "How many annotators participated?", "gt": "five annotators", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "multiple annotators", "source_documents": ["`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ...", "Introduction\nOffensive content has become pervasive in social media and a reason of concern for gove...", "organizations, situations, events, etc. making it more challenging for systems to learn discriminati..."]}
{"query": "What is the definition of offensive language?", "gt": " Most prior work focuses on a different aspect of offensive language such as abusive language BIBREF0 , BIBREF1 , (cyber-)aggression BIBREF2 , (cyber-)bullying BIBREF3 , BIBREF4 , toxic comments INLINEFORM0 , hate speech BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , and offensive language BIBREF11 . Prior work has focused on these aspects of offensive language in Twitter BIBREF3 , BIBREF7 , BIBREF8 , BIBREF11 , Wikipedia comments, and Facebook posts BIBREF2 .", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 77 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Offensive language is defined as any form of language that contains non-acceptable language, such as profanity, insults, threats, and posts containing swear words. It can be either veiled or direct. Offensive language can be categorized into targeted insults/threats (TIN), which are directed towards an individual, group, or others, and untargeted insults/threats (UNT), which contain non-targeted profanity and swearing. Offensive language can also target individuals (IND), groups (GRP), or others (OTH) such as organizations, situations, events, or issues.", "source_documents": ["label a post as offensive if it contains any form of non-acceptable language (profanity) or a target...", "`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "Introduction\nOffensive content has become pervasive in social media and a reason of concern for gove...", "covertly aggressive, and overtly aggressive.\nBullying detection: Several studies have been published..."]}
{"query": "What are the three layers of the annotation scheme?", "gt": "Level A: Offensive language Detection\n, Level B: Categorization of Offensive Language\n, Level C: Offensive Language Target Identification\n", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 19 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The three layers of the annotation scheme are:\n1. Level A: Offensive language Detection\n2. Level B: Categorization of Offensive Language\n3. Level C: Offensive Language Target Identification", "source_documents": ["`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ...", "Introduction\nOffensive content has become pervasive in social media and a reason of concern for gove...", "covertly aggressive, and overtly aggressive.\nBullying detection: Several studies have been published..."]}
{"query": "How long is the dataset for each step of hierarchy?", "gt": "Level A: 14100 Tweets\nLevel B: 4640 Tweets\nLevel C: 4089 Tweets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Level A: Offensive language Detection - The dataset contains 14,100 tweets.\nLevel B: Categorization of Offensive Language - The lengths of the datasets for insults and threats (TIN) and untargeted (UNT) offenses are not provided.\nLevel C: Offensive Language Target Identification - The lengths of the datasets for groups (GRP), individuals (IND), and others (OTH) are provided. There are 395 instances in OTH, 1,075 instances in GRP, and 2,407 instances in IND.", "source_documents": ["(1) an input embedding layer, (2) a bidirectional LSTM layer, (3) an average pooling layer of input ...", "`not safe'). The vast majority of content on Twitter is not offensive so we tried different strategi...", "covertly aggressive, and overtly aggressive.\nBullying detection: Several studies have been published...", "organizations, situations, events, etc. making it more challenging for systems to learn discriminati..."]}
{"query": "What is the approach of previous work?", "gt": "'shallow' naive Bayes, SVM, hierarchical stacked classifiers, bidirectional recurrent neural networks", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 11 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The approach of previous work mentioned in the context is the use of deeper LID models like bidirectional recurrent neural networks (BIBREF23) or ensembles of recurrent neural networks (BIBREF24). These models utilize text features such as character and word n-grams, as well as informative character and word-level features learned from the training data.", "source_documents": ["shown in Table TABREF10. Results were generated on an early 2015 13-inch Retina MacBook Pro with a 2...", "approaches are potentially very useful to reduce the requirement for data.\nResearchers have investig...", "Introduction\nAccurate language identification (LID) is the first step in many natural language proce...", "from the winning language is at least one more than the number of words considered to be from the la..."]}
{"query": "Is the lexicon the same for all languages?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The context does not provide enough information to determine whether the lexicon is the same for all languages.", "source_documents": ["shown in Table TABREF10. Results were generated on an early 2015 13-inch Retina MacBook Pro with a 2...", "from the winning language is at least one more than the number of words considered to be from the la...", "approaches are potentially very useful to reduce the requirement for data.\nResearchers have investig...", "Introduction\nAccurate language identification (LID) is the first step in many natural language proce..."]}
{"query": "How do they obtain the lexicon?", "gt": "built over all the data and therefore includes the vocabulary from both the training and testing sets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 17 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The passage does not provide specific information on how the lexicon is obtained.", "source_documents": ["shown in Table TABREF10. Results were generated on an early 2015 13-inch Retina MacBook Pro with a 2...", "from the winning language is at least one more than the number of words considered to be from the la...", "approaches are potentially very useful to reduce the requirement for data.\nResearchers have investig...", "samples and 1000 testing samples per language.\nThe recently published JW300 parallel corpus BIBREF2 ..."]}
{"query": "What evaluation metric is used?", "gt": "average classification accuracy, execution performance", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 5 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The context does not provide information about the specific evaluation metric used.", "source_documents": ["shown in Table TABREF10. Results were generated on an early 2015 13-inch Retina MacBook Pro with a 2...", "from the winning language is at least one more than the number of words considered to be from the la...", "approaches are potentially very useful to reduce the requirement for data.\nResearchers have investig...", "samples and 1000 testing samples per language.\nThe recently published JW300 parallel corpus BIBREF2 ..."]}
{"query": "Which languages are similar to each other?", "gt": "Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 11 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The Nguni languages (zul, xho, nbl, ssw) are similar to each other.", "source_documents": ["Introduction\nAccurate language identification (LID) is the first step in many natural language proce...", "shown in Table TABREF10. Results were generated on an early 2015 13-inch Retina MacBook Pro with a 2...", "approaches are potentially very useful to reduce the requirement for data.\nResearchers have investig...", "samples and 1000 testing samples per language.\nThe recently published JW300 parallel corpus BIBREF2 ..."]}
{"query": "Which datasets are employed for South African languages LID?", "gt": "DSL 2015, DSL 2017, JW300 parallel corpus , NCHLT text corpora", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 11 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The datasets employed for South African languages LID include the DSL 2015 & DSL 2017 shared task datasets, the NCHLT text corpora, and the recently published JW300 parallel corpus.", "source_documents": ["Introduction\nAccurate language identification (LID) is the first step in many natural language proce...", "samples and 1000 testing samples per language.\nThe recently published JW300 parallel corpus BIBREF2 ...", "shown in Table TABREF10. Results were generated on an early 2015 13-inch Retina MacBook Pro with a 2...", "approaches are potentially very useful to reduce the requirement for data.\nResearchers have investig..."]}
{"query": "Does the paper report the performance of a baseline model on South African languages LID?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Yes.", "source_documents": ["Introduction\nAccurate language identification (LID) is the first step in many natural language proce...", "approaches are potentially very useful to reduce the requirement for data.\nResearchers have investig...", "shown in Table TABREF10. Results were generated on an early 2015 13-inch Retina MacBook Pro with a 2...", "samples and 1000 testing samples per language.\nThe recently published JW300 parallel corpus BIBREF2 ..."]}
{"query": "Does the algorithm improve on the state-of-the-art methods?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Yes", "source_documents": ["shown in Table TABREF10. Results were generated on an early 2015 13-inch Retina MacBook Pro with a 2...", "approaches are potentially very useful to reduce the requirement for data.\nResearchers have investig...", "from the winning language is at least one more than the number of words considered to be from the la...", "Introduction\nAccurate language identification (LID) is the first step in many natural language proce..."]}
{"query": "Do they report results only on English data?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Based on the given information, it is not explicitly stated whether they report results only on English data or not.", "source_documents": ["pre-trained WordPiece. As the implementation environment, we use Google Colaboratory tool which is a...", "have been applied for classification purposes. Waseem et al. BIBREF5 provided a test with a list of ...", "BIBREF10, they necessitate a well-defined feature extraction approach. The trend now seems to be cha...", "performed.\nExperiments and Results\nWe first introduce datasets used in our study and then investigat..."]}
{"query": "Do they report results only on English data?", "gt": "The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 19 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "It is not stated in the given context whether they report results only on English data.", "source_documents": ["pre-trained WordPiece. As the implementation environment, we use Google Colaboratory tool which is a...", "performed.\nExperiments and Results\nWe first introduce datasets used in our study and then investigat...", "have been applied for classification purposes. Waseem et al. BIBREF5 provided a test with a list of ...", "efficiently handle different types of abusive language such as cyberbullying, hate, sarcasm, etc. us..."]}
{"query": "What evidence do the authors present that the model can capture some biases in data annotation and collection?", "gt": "Waseem-dataset, Davidson-dataset,", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "dataset biases", "source_documents": ["efficiently handle different types of abusive language such as cyberbullying, hate, sarcasm, etc. us...", "have been applied for classification purposes. Waseem et al. BIBREF5 provided a test with a list of ...", "BIBREF10, they necessitate a well-defined feature extraction approach. The trend now seems to be cha...", "pre-trained WordPiece. As the implementation environment, we use Google Colaboratory tool which is a..."]}
{"query": "Which publicly available datasets are used?", "gt": "Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 13 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The publicly available datasets used in the study are provided by Waseem and Hovy and Davidson et al.", "source_documents": ["pre-trained WordPiece. As the implementation environment, we use Google Colaboratory tool which is a...", "efficiently handle different types of abusive language such as cyberbullying, hate, sarcasm, etc. us...", "BIBREF10, they necessitate a well-defined feature extraction approach. The trend now seems to be cha...", "performed.\nExperiments and Results\nWe first introduce datasets used in our study and then investigat..."]}
{"query": "What baseline is used?", "gt": "BERT based fine-tuning, Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The baselines used in the study are Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10.", "source_documents": ["pre-trained WordPiece. As the implementation environment, we use Google Colaboratory tool which is a...", "efficiently handle different types of abusive language such as cyberbullying, hate, sarcasm, etc. us...", "BIBREF10, they necessitate a well-defined feature extraction approach. The trend now seems to be cha...", "about these transformer encoders' architectures are presented in BIBREF11. In the fine-tuning phase,..."]}
{"query": "What new fine-tuning methods are presented?", "gt": "sampling tweets from specific keywords create systematic and substancial racial biases in datasets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 13 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The new fine-tuning methods that are presented include:\n1. BERT based fine-tuning.\n2. Insert nonlinear layers.\n3. Insert Bi-LSTM layer.\n4. Insert CNN layer.", "source_documents": ["pre-trained WordPiece. As the implementation environment, we use Google Colaboratory tool which is a...", "about these transformer encoders' architectures are presented in BIBREF11. In the fine-tuning phase,...", "BIBREF10, they necessitate a well-defined feature extraction approach. The trend now seems to be cha...", "performed.\nExperiments and Results\nWe first introduce datasets used in our study and then investigat..."]}
{"query": "What are the existing biases?", "gt": "Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 23 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The existing biases are related to racial biases in the datasets used for offensive language detection and the biases derived from hate speech training datasets.", "source_documents": ["pre-trained WordPiece. As the implementation environment, we use Google Colaboratory tool which is a...", "efficiently handle different types of abusive language such as cyberbullying, hate, sarcasm, etc. us...", "BIBREF10, they necessitate a well-defined feature extraction approach. The trend now seems to be cha...", "have been applied for classification purposes. Waseem et al. BIBREF5 provided a test with a list of ..."]}
{"query": "What biases does their model capture?", "gt": "Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 13 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The model is able to detect biases in the process of collecting or annotating datasets.", "source_documents": ["pre-trained WordPiece. As the implementation environment, we use Google Colaboratory tool which is a...", "BIBREF10, they necessitate a well-defined feature extraction approach. The trend now seems to be cha...", "have been applied for classification purposes. Waseem et al. BIBREF5 provided a test with a list of ...", "efficiently handle different types of abusive language such as cyberbullying, hate, sarcasm, etc. us..."]}
{"query": "What does the \"sensitivity\" quantity denote?", "gt": "the number of distinct word recognition outputs that an attacker can induce", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The \"sensitivity\" quantity denotes the expected number of unique outputs that a word recognition system assigns to a set of adversarial perturbations. In other words, it measures the diversity of predictions that the system generates for a given set of perturbed sentences.", "source_documents": ["vision, an important factor determining the success of an adversary is the norm constraint on the pe...", "accuracy within $5\\%$ range for the swap attack. It is also effective under other attack classes, an...", "the output string produced by the word-recognizer $W$ using $s_i$ and $\\#_{u}(\\cdot )$ counts the nu...", "an UNK, therefore reducing the number of unique outputs it predicts. Open vocabulary (i.e. char-only..."]}
{"query": "What end tasks do they evaluate on?", "gt": "Sentiment analysis and paraphrase detection under adversarial attacks", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 8 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The end tasks they evaluate on are sentiment analysis and paraphrase detection.", "source_documents": ["words shorter than 4 characters.\nFor 1-character attacks, we try all possible perturbations listed a...", "2 . Note that ATD incorrectly predicts $11.2$ words for every 100 words (in the `all' setting), wher...", "of options.\nSecond, we evaluate first-line techniques including data augmentation and adversarial tr...", "an UNK, therefore reducing the number of unique outputs it predicts. Open vocabulary (i.e. char-only..."]}
{"query": "What is a semicharacter architecture?", "gt": "A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 22 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "A semi-character architecture is a type of neural network architecture that combines both character-level and word-level information in order to process and understand text. It typically involves encoding the input words using a combination of character-level representations and word-level representations. The output length of a suggested semi-character architecture would be around 22 words.", "source_documents": ["2 . Note that ATD incorrectly predicts $11.2$ words for every 100 words (in the `all' setting), wher...", "an UNK, therefore reducing the number of unique outputs it predicts. Open vocabulary (i.e. char-only...", "words shorter than 4 characters.\nFor 1-character attacks, we try all possible perturbations listed a...", "the output string produced by the word-recognizer $W$ using $s_i$ and $\\#_{u}(\\cdot )$ counts the nu..."]}
{"query": "Do they experiment with offering multiple candidate corrections and voting on the model output, since this seems highly likely to outperform a one-best correction?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "There is no explicit mention in the given context about experimenting with offering multiple candidate corrections and voting on the model output. Therefore, it is not possible to determine if they conducted such experiments.", "source_documents": ["words shorter than 4 characters.\nFor 1-character attacks, we try all possible perturbations listed a...", "2 . Note that ATD incorrectly predicts $11.2$ words for every 100 words (in the `all' setting), wher...", "of options.\nSecond, we evaluate first-line techniques including data augmentation and adversarial tr...", "that adversarial examples in NLP are often ungrammatical BIBREF14 .\nIn related work on character-lev..."]}
{"query": "Why is the adversarial setting appropriate for misspelling recognition?", "gt": "Adversarial misspellings are a real-world problem", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 6 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The adversarial setting is appropriate for misspelling recognition because it allows for testing the robustness of a model against various types of attacks, such as dropping, adding, and swapping characters within words. By introducing adversarial examples, researchers can evaluate the performance of the model and identify its vulnerabilities and weaknesses in recognizing misspelled words. The suggested output length of around 6 words is relevant because it indicates that the experiments are conducted using sentences or phrases of sufficient length to test the model's ability to recognize and correct misspelled words in context.", "source_documents": ["words shorter than 4 characters.\nFor 1-character attacks, we try all possible perturbations listed a...", "Introduction\nDespite the rapid progress of deep learning techniques on diverse supervised learning t...", "that adversarial examples in NLP are often ungrammatical BIBREF14 .\nIn related work on character-lev...", "of options.\nSecond, we evaluate first-line techniques including data augmentation and adversarial tr..."]}
{"query": "How do the backoff strategies work?", "gt": "Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 43 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The backoff strategies involve different methods for handling the prediction of unknown words (UNK) by the word recognition models. \n\n1. Pass-through: In this strategy, the word-recognizer simply passes on the UNK-predicted word as it is, without any modification.\n\n2. Backoff to neutral word: Instead of passing the UNK-predicted word unchanged, this strategy involves backing off to a neutral word like 'a'. This neutral word has a similar distribution across classes.\n\n3. Backoff to background model: This strategy involves falling back upon a more generic word recognition model that is trained on a larger, less-specialized corpus whenever the foreground word recognition model predicts UNK. The background model is trained to predict a larger number of words, but it may be less accurate than the foreground model for the specific task. This strategy is used when the foreground model fails to recognize a rare or unseen word.\n\nThe backoff strategies are aimed at improving the accuracy and robustness of the word recognition models. They provide alternative options for handling unknown words and increase the chances of correct word recognition.", "source_documents": ["2 . Note that ATD incorrectly predicts $11.2$ words for every 100 words (in the `all' setting), wher...", "words shorter than 4 characters.\nFor 1-character attacks, we try all possible perturbations listed a...", "by concatenating (i) a one hot vector of the first character ( $\\mathbf {w_{i1}}$ ); (ii) a one hot ...", "an UNK, therefore reducing the number of unique outputs it predicts. Open vocabulary (i.e. char-only..."]}
{"query": "What baseline model is used?", "gt": "same baseline as used by lang2011unsupervised", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 6 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The baseline model used is not explicitly mentioned in the given context.", "source_documents": ["the step 4 of this pipeline.\nMonolingual Model\nWe use the Bayesian model of garg2012unsupervised as ...", "of features might get assigned different roles if they appear in different contexts.\nData\nFollowing ...", "languages. Using the CoNLL 2009 dataset alone, titovcrosslingual report an F1 score of 80.9% (PU=86....", "we increase the proportion of supervised sentences. At 10% supervised sentences, the model has alrea..."]}
{"query": "Which additional latent variables are used in the model?", "gt": "CLV as a parent of the two corresponding role variables", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 10 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The additional latent variables used in the model are crosslingual latent variables (CLVs).", "source_documents": ["we increase the proportion of supervised sentences. At 10% supervised sentences, the model has alrea...", "the step 4 of this pipeline.\nMonolingual Model\nWe use the Bayesian model of garg2012unsupervised as ...", "INLINEFORM1 of them, the multilingual latent variable is connected to only those INLINEFORM2 aligned...", "FIGREF16 illustrates this model. The generative process, as explained below, remains the same as the..."]}
{"query": "Which additional latent variables are used in the model?", "gt": "crosslingual latent variables", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 3 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The additional latent variables used in the model are crosslingual latent variables (CLVs).", "source_documents": ["the step 4 of this pipeline.\nMonolingual Model\nWe use the Bayesian model of garg2012unsupervised as ...", "we increase the proportion of supervised sentences. At 10% supervised sentences, the model has alrea...", "INLINEFORM1 of them, the multilingual latent variable is connected to only those INLINEFORM2 aligned...", "FIGREF16 illustrates this model. The generative process, as explained below, remains the same as the..."]}
{"query": "Which parallel corpora are used?", "gt": "English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13, EN-DE section of the Europarl corpus BIBREF14", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 19 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The English (EN) and German (DE) sections of the CoNLL 2009 corpus and the EN-DE section of the Europarl corpus.", "source_documents": ["of features might get assigned different roles if they appear in different contexts.\nData\nFollowing ...", "languages. Using the CoNLL 2009 dataset alone, titovcrosslingual report an F1 score of 80.9% (PU=86....", "of multilingual SRI, we can apply the same model to a variety of training scenarios just by changing...", "we increase the proportion of supervised sentences. At 10% supervised sentences, the model has alrea..."]}
{"query": "Overall, does having parallel data improve semantic role induction across multiple languages?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "No", "source_documents": ["of multilingual SRI, we can apply the same model to a variety of training scenarios just by changing...", "we increase the proportion of supervised sentences. At 10% supervised sentences, the model has alrea...", "of features might get assigned different roles if they appear in different contexts.\nData\nFollowing ...", "parallel data in multilingual SRI, for example by joint modeling with SRI. We leave this exploration..."]}
{"query": "Do they add one latent variable for each language pair in their Bayesian model?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Based on the given context, it is not clear whether they add one latent variable for each language pair in their Bayesian model.", "source_documents": ["we increase the proportion of supervised sentences. At 10% supervised sentences, the model has alrea...", "INLINEFORM1 of them, the multilingual latent variable is connected to only those INLINEFORM2 aligned...", "the step 4 of this pipeline.\nMonolingual Model\nWe use the Bayesian model of garg2012unsupervised as ...", "FIGREF16 illustrates this model. The generative process, as explained below, remains the same as the..."]}
{"query": "What does an individual model consist of?", "gt": "Bayesian model of garg2012unsupervised as our base monolingual model", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 9 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "An individual model consists of a monolingual model that uses the Bayesian model of garg2012unsupervised as its base, and includes primary roles (PRs) and secondary roles (SRs) to model role ordering and repetition preferences.", "source_documents": ["the step 4 of this pipeline.\nMonolingual Model\nWe use the Bayesian model of garg2012unsupervised as ...", "of features might get assigned different roles if they appear in different contexts.\nData\nFollowing ...", "we increase the proportion of supervised sentences. At 10% supervised sentences, the model has alrea...", "languages. Using the CoNLL 2009 dataset alone, titovcrosslingual report an F1 score of 80.9% (PU=86...."]}
{"query": "Do they improve on state-of-the-art semantic role induction?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "There is no information in the given context that directly addresses whether the proposed Bayesian model improves on state-of-the-art semantic role induction.", "source_documents": ["we increase the proportion of supervised sentences. At 10% supervised sentences, the model has alrea...", "of features might get assigned different roles if they appear in different contexts.\nData\nFollowing ...", "the step 4 of this pipeline.\nMonolingual Model\nWe use the Bayesian model of garg2012unsupervised as ...", "Introduction\nSemantic Role Labeling (SRL) has emerged as an important task in Natural Language Proce..."]}
{"query": "how many tags do they look at?", "gt": "48,705", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The given context does not provide information about how many tags they look at.", "source_documents": ["to compare how semantically similar the recommended tags are to the expected review keywords. For th...", "for the 2,896 e-books, for whom we got review keywords. To follow common practice for tag recommenda...", "Introduction\nWhen people shop for books online in e-book stores such as, e.g., the Amazon Kindle sto...", "(i.e., description or title) to recommend relevant tags BIBREF10 . For this, we first employ a conte..."]}
{"query": "which algorithm was the highest performer?", "gt": "A hybrid model consisting of best performing popularity-based approach with the best similarity-based approach", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 14 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The highest performing algorithm was the hybrid approach that combines the best performing popularity-based approach with the best performing similarity-based approach.", "source_documents": ["however, not the case for INLINEFORM3 as the accuracy of the integrated INLINEFORM4 approach is low....", "for the 2,896 e-books, for whom we got review keywords. To follow common practice for tag recommenda...", "between both rankings. This results in INLINEFORM1 with a p-value < INLINEFORM2 , which suggests a h...", "(i.e., description or title) to recommend relevant tags BIBREF10 . For this, we first employ a conte..."]}
{"query": "how is diversity measured?", "gt": " the average dissimilarity of all pairs of tags in the list of recommended tags", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 14 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Diversity is measured as the average dissimilarity of all pairs of tags in the list of recommended tags. The dissimilarity function used is cosine similarity.", "source_documents": ["for the 2,896 e-books, for whom we got review keywords. To follow common practice for tag recommenda...", "between both rankings. This results in INLINEFORM1 with a p-value < INLINEFORM2 , which suggests a h...", "to compare how semantically similar the recommended tags are to the expected review keywords. For th...", ", BIBREF17 as we did, for example, with our proposed semantic similarity evaluation metric. Finally,..."]}
{"query": "how large is the vocabulary?", "gt": "33,663", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The size of the vocabulary is not specified in the given context.", "source_documents": ["for the 2,896 e-books, for whom we got review keywords. To follow common practice for tag recommenda...", "Introduction\nWhen people shop for books online in e-book stores such as, e.g., the Amazon Kindle sto...", "between both rankings. This results in INLINEFORM1 with a p-value < INLINEFORM2 , which suggests a h...", ", BIBREF17 as we did, for example, with our proposed semantic similarity evaluation metric. Finally,..."]}
{"query": "what dataset was used?", "gt": " E-book annotation data: editor tags, Amazon search terms, and  Amazon review keywords.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The dataset used consists of e-book annotation data, including editor tags and Amazon search terms.", "source_documents": ["for the 2,896 e-books, for whom we got review keywords. To follow common practice for tag recommenda...", ", BIBREF17 as we did, for example, with our proposed semantic similarity evaluation metric. Finally,...", "between both rankings. This results in INLINEFORM1 with a p-value < INLINEFORM2 , which suggests a h...", "validate semantic matches of tag recommendations.\nApproach and findings. We exploit editor tags and ..."]}
{"query": "what algorithms did they use?", "gt": "popularity-based, similarity-based, hybrid", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 3 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "popularity-based, similarity-based, hybrid approaches", "source_documents": ["for the 2,896 e-books, for whom we got review keywords. To follow common practice for tag recommenda...", "(i.e., description or title) to recommend relevant tags BIBREF10 . For this, we first employ a conte...", "however, not the case for INLINEFORM3 as the accuracy of the integrated INLINEFORM4 approach is low....", "available for e-books that contain editor tags. Consequently, both data sources provide complementar..."]}
{"query": "What baseline method is used?", "gt": "use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 24 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The baseline method used is the word2vec approach, which is mentioned in the text.", "source_documents": ["proposed approach are discussed in Section 4. In Section 5, we conclude the paper.\nRelated Work\nIn t...", "Embeddings\nIn addition to using the three approaches independently, we also combined all the matrice...", "English Twitter dataset.\nSince the TDK dictionary covers most of the domain-specific vocabulary used...", "utility of the WEKA tool. We chose the linear kernel option to classify the reviews. We trained word..."]}
{"query": "What details are given about the Twitter dataset?", "gt": "one of the Twitter datasets is about Turkish mobile network operators, there are positive, neutral and negative labels and provide the total amount plus the distribution of labels", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 28 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The Twitter dataset is formed of tweets about Turkish mobile network operators. There are a total of 1,716 tweets, with 973 of them being negative and 743 of them being positive. The tweets were manually annotated by two humans, and if there was a disagreement on the polarity of a tweet, it was removed. The dataset is described as being noisier and shorter compared to the reviews in the movie corpus.", "source_documents": ["English Twitter dataset.\nSince the TDK dictionary covers most of the domain-specific vocabulary used...", "utility of the WEKA tool. We chose the linear kernel option to classify the reviews. We trained word...", "We consider a review to be negative it the score is equal to or lower than 2.5. On the other hand, i...", "proposed approach are discussed in Section 4. In Section 5, we conclude the paper.\nRelated Work\nIn t..."]}
{"query": "What details are given about the movie domain dataset?", "gt": "there are 20,244 reviews divided into positive and negative with an average 39 words per review, each one having a star-rating score", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 22 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The movie domain dataset consists of 20,244 reviews collected from a popular website. Each review in the dataset has a star-rating score which indicates the sentiment. The average number of words in the reviews is 39. The polarity scores in the reviews range from 0.5 to 5, with intervals of 0.5.", "source_documents": ["English Twitter dataset.\nSince the TDK dictionary covers most of the domain-specific vocabulary used...", "utility of the WEKA tool. We chose the linear kernel option to classify the reviews. We trained word...", "proposed approach are discussed in Section 4. In Section 5, we conclude the paper.\nRelated Work\nIn t...", "Embeddings\nIn addition to using the three approaches independently, we also combined all the matrice..."]}
{"query": "Which hand-crafted features are combined with word2vec?", "gt": "polarity scores, which are minimum, mean, and maximum polarity scores, from each review", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 13 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The hand-crafted features that are combined with word2vec are the minimum, mean, and maximum polarity scores of the reviews.", "source_documents": ["proposed approach are discussed in Section 4. In Section 5, we conclude the paper.\nRelated Work\nIn t...", "utility of the WEKA tool. We chose the linear kernel option to classify the reviews. We trained word...", "sentimental, lexical, and contextual characteristics of words. In addition to these mostly original ...", "English Twitter dataset.\nSince the TDK dictionary covers most of the domain-specific vocabulary used..."]}
{"query": "What word-based and dictionary-based feature are used?", "gt": "generate word embeddings specific to a domain, TDK (T\u00fcrk Dil Kurumu - \u201cTurkish Language Institution\u201d) dictionary to obtain word polarities", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 20 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The word-based feature used is word embeddings, which capture the sentimental, lexical, and contextual characteristics of words. The dictionary-based feature used is the TDK dictionary method, which performs well but lacks many words occurring in the tweets.", "source_documents": ["English Twitter dataset.\nSince the TDK dictionary covers most of the domain-specific vocabulary used...", "proposed approach are discussed in Section 4. In Section 5, we conclude the paper.\nRelated Work\nIn t...", "sentimental, lexical, and contextual characteristics of words. In addition to these mostly original ...", "utility of the WEKA tool. We chose the linear kernel option to classify the reviews. We trained word..."]}
{"query": "How are the supervised scores of the words calculated?", "gt": "(+1 or -1), words of opposite polarities (e.g. \u201chappy\" and \u201cunhappy\") get far away from each other", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 17 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The supervised scores of the words are calculated using a formula that takes into account the sentiment score of the word, the number of positive and negative documents in which the word occurs, and the total number of words in the positive and negative corpora. The formula involves normalizing the scores and adding a small number for smoothing.", "source_documents": ["we also separately multiplied all the row scores with only +1 if the row word is a positive word, an...", "Embeddings\nIn addition to using the three approaches independently, we also combined all the matrice...", "English Twitter dataset.\nSince the TDK dictionary covers most of the domain-specific vocabulary used...", "decomposition is computed. We take into account the U matrix only for each word. We have chosen the ..."]}
{"query": "What topic is covered in the Chinese Facebook data? ", "gt": "anti-nuclear-power", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "anti-nuclear-power", "source_documents": ["a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show th...", "posts can attract even more neutral likers. These results do suggest that users prefer posts of the ...", "Introduction\nThis work is licenced under a Creative Commons Attribution 4.0 International License. L...", "their replies to posts. 2. When classifying social media post stances, we leverage users, including ..."]}
{"query": "How many layers does the UTCNN model have?", "gt": "eight layers", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The number of layers in the UTCNN model is not stated in the given text. Therefore, I don't know the answer.", "source_documents": ["UTCNN model. In addition, comment information provides additional clues for stance classification. W...", "Introduction\nThis work is licenced under a Creative Commons Attribution 4.0 International License. L...", "network for target-dependent Twitter sentiment analysis, where targets are topics such as windows 7 ...", "supported by its outperforming other models though no oversampling technique is applied to the UTCNN..."]}
{"query": "What topics are included in the debate data?", "gt": "abortion, gay rights, Obama, marijuana", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 5 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "abortion, gay rights, Obama, marijuana", "source_documents": ["posts can attract even more neutral likers. These results do suggest that users prefer posts of the ...", "a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show th...", "supported by its outperforming other models though no oversampling technique is applied to the UTCNN...", "directly without additionally applying LDA.\nFor the FBFans data we report class-based f-scores as we..."]}
{"query": "What topics are included in the debate data?", "gt": "abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 10 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "abortion, gay rights, Obama, marijuana", "source_documents": ["posts can attract even more neutral likers. These results do suggest that users prefer posts of the ...", "a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show th...", "supported by its outperforming other models though no oversampling technique is applied to the UTCNN...", "directly without additionally applying LDA.\nFor the FBFans data we report class-based f-scores as we..."]}
{"query": "What is the size of the Chinese data?", "gt": "32,595", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "2,496", "source_documents": ["a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show th...", "UTCNN model. In addition, comment information provides additional clues for stance classification. W...", "posts can attract even more neutral likers. These results do suggest that users prefer posts of the ...", "supported by its outperforming other models though no oversampling technique is applied to the UTCNN..."]}
{"query": "Did they collected the two datasets?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Yes", "source_documents": ["posts can attract even more neutral likers. These results do suggest that users prefer posts of the ...", "a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show th...", "their replies to posts. 2. When classifying social media post stances, we leverage users, including ...", "supported by its outperforming other models though no oversampling technique is applied to the UTCNN..."]}
{"query": "What are the baselines?", "gt": "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 24 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The baselines are as follows: 1) SVM with unigram, bigram, and trigram features, 2) SVM with average word embedding, 3) SVM with average transformed word embeddings, 4) CNN, 5) Recurrent Convolutional Neural Networks (RCNN), 6) SVM and deep learning models with comment information, 7) UTCNN without user information, 8) UTCNN without the LDA model, and 9) UTCNN without comments.", "source_documents": ["directly without additionally applying LDA.\nFor the FBFans data we report class-based f-scores as we...", "posts can attract even more neutral likers. These results do suggest that users prefer posts of the ...", "a public, multiple-topic, English, balanced, forum dataset. Results using these two datasets show th...", "their replies to posts. 2. When classifying social media post stances, we leverage users, including ..."]}
{"query": "why are their techniques cheaper to implement?", "gt": "They use a slightly modified copy of the target to create the pseudo-text instead of full BT to make their technique cheaper", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 22 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Their techniques are cheaper to implement because they rely on generating synthetic parallel data using simpler methods such as copying the target side data, augmenting the source vocabulary with copies of the target vocabulary, or replacing each word with \"dummy\" tokens. These methods do not require the availability of a backward translation engine, unlike traditional back-translation methods. The cost of implementing these techniques is lower because they do not require extensive training with large amounts of parallel or monolingual data. The suggested output length is around 22 words.", "source_documents": ["recap our answers to our initial questions: the quality of BT actually matters for NMT (cf. \u00a7 SECREF...", "for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-Eng...", "SECREF5 ) suggest promising directions for efficient domain adaptation with cheaper techniques than ...", "randomly selected out-of-domain natural sentences, with the same architecture and training parameter..."]}
{"query": "what data simulation techniques were introduced?", "gt": "copy, copy-marked, copy-dummies", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 3 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The data simulation techniques introduced are copy, copy-marked, and copy-dummies.", "source_documents": ["for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-Eng...", "SECREF5 ) suggest promising directions for efficient domain adaptation with cheaper techniques than ...", "recap our answers to our initial questions: the quality of BT actually matters for NMT (cf. \u00a7 SECREF...", "Introduction \nThe new generation of Neural Machine Translation (NMT) systems is known to be extremel..."]}
{"query": "what is their explanation for the effectiveness of back-translation?", "gt": "when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent, automatic word alignments between artificial sources tend to be more monotonic than when using natural sources", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 40 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The explanation for the effectiveness of back-translation is that using more monotonic sentence pairs, which is facilitated by back-translation, improves the performance of NMT. Additionally, back-translation helps to increase the lexical diversity and syntactic complexity of the artificial data, which is beneficial for translation.", "source_documents": ["for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-Eng...", "recap our answers to our initial questions: the quality of BT actually matters for NMT (cf. \u00a7 SECREF...", "SECREF5 ) suggest promising directions for efficient domain adaptation with cheaper techniques than ...", "is reported in Table TABREF7 , where we observe a 12 BLEU points gap between the worst and best syst..."]}
{"query": "what dataset is used?", "gt": "Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 17 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The dataset used in this context is the Europarl corpus, specifically for the translation directions English to German and English to French.", "source_documents": ["SECREF5 ) suggest promising directions for efficient domain adaptation with cheaper techniques than ...", "for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-Eng...", "recap our answers to our initial questions: the quality of BT actually matters for NMT (cf. \u00a7 SECREF...", "randomly selected out-of-domain natural sentences, with the same architecture and training parameter..."]}
{"query": "what language pairs are explored?", "gt": "English-German, English-French", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The language pairs explored are English-German and English-French.", "source_documents": ["SECREF5 ) suggest promising directions for efficient domain adaptation with cheaper techniques than ...", "for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-Eng...", "randomly selected out-of-domain natural sentences, with the same architecture and training parameter...", "pairs, with the exception of a small improvement for English-French on the out-of-domain test, which..."]}
{"query": "what language is the data in?", "gt": "English , German, French", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 4 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The data in the given context is in English.", "source_documents": ["for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-Eng...", "SECREF5 ) suggest promising directions for efficient domain adaptation with cheaper techniques than ...", "Introduction \nThe new generation of Neural Machine Translation (NMT) systems is known to be extremel...", "recap our answers to our initial questions: the quality of BT actually matters for NMT (cf. \u00a7 SECREF..."]}
{"query": "What metrics are used for evaluation?", "gt": "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 35 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The metrics used for evaluation are user matching accuracy (UMA), Mean Reciprocal Rank (MRR), recipe-level coherence scores, recipe step entailment scores, and human evaluation.", "source_documents": ["Prior Name model further suggests the addition of lemon and mint, which are reasonably associated wi...", "recipes labeled `A' and `B'. One recipe is generated from our baseline encoder-decoder model and one...", "a [SEP] token, using the final representation of the [CLS] token. The step entailment score is compu...", "first 3-5 ingredients listed in each recipe. We decode recipe text via top-$k$ sampling BIBREF7, fin..."]}
{"query": "What natural language(s) are the recipes written in?", "gt": "English", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The recipes are written in natural language, specifically in English.", "source_documents": ["internally consistent long texts BIBREF8, BIBREF9, BIBREF10. Here, we generate procedurally structur...", "recipes labeled `A' and `B'. One recipe is generated from our baseline encoder-decoder model and one...", "Introduction\nIn the kitchen, we increasingly rely on instructions from cooking websites: recipes. A ...", "Prior Name model further suggests the addition of lemon and mint, which are reasonably associated wi..."]}
{"query": "What were their results on the new dataset?", "gt": "average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 18 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The results on the new dataset show that the personalized generative models can generate plausible, personalized, and coherent recipes that are preferred by human evaluators for consumption. On average, human evaluators preferred personalized model outputs to the baseline 63% of the time. Additionally, in a small-scale human coherence survey, 60% of users found recipes generated by personalized models to be more coherent and preferable to those generated by baseline models.", "source_documents": ["a [SEP] token, using the final representation of the [CLS] token. The step entailment score is compu...", "first 3-5 ingredients listed in each recipe. We decode recipe text via top-$k$ sampling BIBREF7, fin...", "Prior Name model further suggests the addition of lemon and mint, which are reasonably associated wi...", "recipes labeled `A' and `B'. One recipe is generated from our baseline encoder-decoder model and one..."]}
{"query": "What are the baseline models?", "gt": "name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 10 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The baseline models mentioned in the context are the name-based Nearest-Neighbor model (NN) and the Encoder-Decoder baseline with ingredient attention (Enc-Dec).", "source_documents": ["Prior Name model further suggests the addition of lemon and mint, which are reasonably associated wi...", "first 3-5 ingredients listed in each recipe. We decode recipe text via top-$k$ sampling BIBREF7, fin...", "recipes labeled `A' and `B'. One recipe is generated from our baseline encoder-decoder model and one...", "internally consistent long texts BIBREF8, BIBREF9, BIBREF10. Here, we generate procedurally structur..."]}
{"query": "How did they obtain the interactions?", "gt": "from Food.com", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The interactions were obtained by scraping 1.4M user-recipe interactions (reviews) from Food.com.", "source_documents": ["internally consistent long texts BIBREF8, BIBREF9, BIBREF10. Here, we generate procedurally structur...", "recipes labeled `A' and `B'. One recipe is generated from our baseline encoder-decoder model and one...", "Prior Name model further suggests the addition of lemon and mint, which are reasonably associated wi...", "a [SEP] token, using the final representation of the [CLS] token. The step entailment score is compu..."]}
{"query": "Where do they get the recipes from?", "gt": "from Food.com", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "We don't have enough information to answer the question.", "source_documents": ["internally consistent long texts BIBREF8, BIBREF9, BIBREF10. Here, we generate procedurally structur...", "recipes labeled `A' and `B'. One recipe is generated from our baseline encoder-decoder model and one...", "Introduction\nIn the kitchen, we increasingly rely on instructions from cooking websites: recipes. A ...", "Prior Name model further suggests the addition of lemon and mint, which are reasonably associated wi..."]}
{"query": "what are their results on the constructed dataset?", "gt": "AutoJudge consistently and significantly outperforms all the baselines, RC models achieve better performance than most text classification models (excluding GRU+Attention), Comparing with conventional RC models, AutoJudge achieves significant improvement", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 29 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The results on the constructed dataset are not explicitly mentioned in the given text.", "source_documents": ["low-frequency words are replaced with the mark <UNK>. The hidden size of GRU is set to 128 for each ...", "dates, which are anonymized.\nConclusion\nIn this paper, we explore the task of predicting judgments o...", "the quality of the retrieved law articles would affect the final performance. As is shown in Table T...", "inputs, i.e., query and passage, should be modeled separately.\nDespite the introduction of the neura..."]}
{"query": "what evaluation metrics are reported?", "gt": "precision, recall, F1 and accuracy", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 5 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "precision, recall, F1, accuracy", "source_documents": ["low-frequency words are replaced with the mark <UNK>. The hidden size of GRU is set to 128 for each ...", "dates, which are anonymized.\nConclusion\nIn this paper, we explore the task of predicting judgments o...", "the quality of the retrieved law articles would affect the final performance. As is shown in Table T...", "inputs, i.e., query and passage, should be modeled separately.\nDespite the introduction of the neura..."]}
{"query": "what civil field is the dataset about?", "gt": "divorce", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "divorce", "source_documents": ["dates, which are anonymized.\nConclusion\nIn this paper, we explore the task of predicting judgments o...", "the quality of the retrieved law articles would affect the final performance. As is shown in Table T...", "Introduction\nAutomatic judgment prediction is to train a machine judge to determine whether a certai...", "low-frequency words are replaced with the mark <UNK>. The hidden size of GRU is set to 128 for each ..."]}
{"query": "what are the state-of-the-art models?", "gt": "SVM , CNN , GRU , CNN/GRU+law, r-net , AoA ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 10 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The state-of-the-art models mentioned in the given context are R-Net and AoA.", "source_documents": ["low-frequency words are replaced with the mark <UNK>. The hidden size of GRU is set to 128 for each ...", "dates, which are anonymized.\nConclusion\nIn this paper, we explore the task of predicting judgments o...", "inputs, i.e., query and passage, should be modeled separately.\nDespite the introduction of the neura...", "experiments, improvements achieved by LRC prove to be robust.\nJudgment Prediction\nAutomatic judgment..."]}
{"query": "what is the size of the real-world civil case dataset?", "gt": "100 000 documents", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 3 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "don't know", "source_documents": ["dates, which are anonymized.\nConclusion\nIn this paper, we explore the task of predicting judgments o...", "the quality of the retrieved law articles would affect the final performance. As is shown in Table T...", "Introduction\nAutomatic judgment prediction is to train a machine judge to determine whether a certai...", "low-frequency words are replaced with the mark <UNK>. The hidden size of GRU is set to 128 for each ..."]}
{"query": "what datasets are used in the experiment?", "gt": "build a new one, collect INLINEFORM0 cases from China Judgments Online", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 11 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The experiment uses a new dataset that was randomly created and consists of INLINEFORM0 case documents that were made publicly available by the Supreme People's Court of People's Republic of China.", "source_documents": ["low-frequency words are replaced with the mark <UNK>. The hidden size of GRU is set to 128 for each ...", "dates, which are anonymized.\nConclusion\nIn this paper, we explore the task of predicting judgments o...", "last hidden state in the GRU, which will be introduced in the following part. INLINEFORM1 is a weigh...", "inputs, i.e., query and passage, should be modeled separately.\nDespite the introduction of the neura..."]}
{"query": "What domains are covered in the corpus?", "gt": "No specific domain is covered in the corpus.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 8 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The corpus covers multiple domains, including the tourism domain.", "source_documents": ["Introduction\nEnd-to-end speech-to-text translation (ST) has attracted much attention recently BIBREF...", "German and 38 hours BIBREF19 for French). Moreover, CoVoST has a total of 18 hours of Dutch speeches...", "rich speaker diversity in the test set and relatively small train set (see also Figure FIGREF6 and T...", "manually inspected examples where the translation had a high perplexity and sent them back to transl..."]}
{"query": "What is the architecture of their model?", "gt": "follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 14 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The architecture of their model follows the architecture in berard2018end, but with 3 decoder layers like that in pino2019harnessing. However, the specific details of the architecture are not provided in the given context.", "source_documents": ["torchaudio. The features are normalized to 0 mean and 1.0 standard deviation. We remove samples havi...", "rich speaker diversity in the test set and relatively small train set (see also Figure FIGREF6 and T...", "German and 38 hours BIBREF19 for French). Moreover, CoVoST has a total of 18 hours of Dutch speeches...", "Introduction\nEnd-to-end speech-to-text translation (ST) has attracted much attention recently BIBREF..."]}
{"query": "How was the dataset collected?", "gt": "Contributors record voice clips by reading from a bank of donated sentences.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The dataset was collected from Common Voice and Tatoeba. The Common Voice dataset was used for 11 languages and the Tatoeba dataset was used for French, German, Dutch, Russian, and Spanish. The dataset includes speeches from over 11,000 speakers with multiple accents. Both datasets were collected at the sentence level and did not require additional alignments or segmentation.", "source_documents": ["German and 38 hours BIBREF19 for French). Moreover, CoVoST has a total of 18 hours of Dutch speeches...", "manually inspected examples where the translation had a high perplexity and sent them back to transl...", "Introduction\nEnd-to-end speech-to-text translation (ST) has attracted much attention recently BIBREF...", "rich speaker diversity in the test set and relatively small train set (see also Figure FIGREF6 and T..."]}
{"query": "Which languages are part of the corpus?", "gt": "French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The languages that are part of the corpus are French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian, and Chinese.", "source_documents": ["Introduction\nEnd-to-end speech-to-text translation (ST) has attracted much attention recently BIBREF...", "rich speaker diversity in the test set and relatively small train set (see also Figure FIGREF6 and T...", "German and 38 hours BIBREF19 for French). Moreover, CoVoST has a total of 18 hours of Dutch speeches...", "manually inspected examples where the translation had a high perplexity and sent them back to transl..."]}
{"query": "How is the quality of the data empirically evaluated? ", "gt": "Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 23 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The passage does not explicitly mention how the quality of the data is empirically evaluated. Therefore, we do not have enough information to answer the question.", "source_documents": ["German and 38 hours BIBREF19 for French). Moreover, CoVoST has a total of 18 hours of Dutch speeches...", "manually inspected examples where the translation had a high perplexity and sent them back to transl...", "Introduction\nEnd-to-end speech-to-text translation (ST) has attracted much attention recently BIBREF...", "rich speaker diversity in the test set and relatively small train set (see also Figure FIGREF6 and T..."]}
{"query": "Is the data in CoVoST annotated for dialect?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "No, the passage does not mention whether the data in CoVoST is annotated for dialect.", "source_documents": ["German and 38 hours BIBREF19 for French). Moreover, CoVoST has a total of 18 hours of Dutch speeches...", "rich speaker diversity in the test set and relatively small train set (see also Figure FIGREF6 and T...", "train-development-test split, we also provide baseline models, including, to our knowledge, the firs...", "manually inspected examples where the translation had a high perplexity and sent them back to transl..."]}
{"query": "Is Arabic one of the 11 languages in CoVost?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "No.", "source_documents": ["rich speaker diversity in the test set and relatively small train set (see also Figure FIGREF6 and T...", "German and 38 hours BIBREF19 for French). Moreover, CoVoST has a total of 18 hours of Dutch speeches...", "Introduction\nEnd-to-end speech-to-text translation (ST) has attracted much attention recently BIBREF...", "train-development-test split, we also provide baseline models, including, to our knowledge, the firs..."]}
{"query": "What kind of model do they use?", "gt": "visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 39 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "They use a joint model that integrates visual features learned through Inception V3 with textual features learned through a biLSTM.", "source_documents": ["subset.\nExperimental Setting\nAs discussed above, our model has two main components \u2014 biLSTM and Ince...", "topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mec...", "5), the basic Inception model applies multiple convolution filters in parallel and concatenates the ...", "separation between Start and Stub achieved by Joint being the clearest among the three models. Incep..."]}
{"query": "What kind of model do they use?", "gt": "Inception V3, biLSTM", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 3 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The authors use a joint model.", "source_documents": ["subset.\nExperimental Setting\nAs discussed above, our model has two main components \u2014 biLSTM and Ince...", "topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mec...", "5), the basic Inception model applies multiple convolution filters in parallel and concatenates the ...", "separation between Start and Stub achieved by Joint being the clearest among the three models. Incep..."]}
{"query": "Did they release their data set of academic papers?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "No, it is not mentioned in the given context whether they released their dataset of academic papers.", "source_documents": ["https://en.wikipedia.org/wiki/Category:Featured_articles. This resulted in around 5K FA, 28K GA, 212...", "not improve for 20 epochs. For evaluation, we use (micro-)accuracy, following previous studies BIBRE...", "subset.\nExperimental Setting\nAs discussed above, our model has two main components \u2014 biLSTM and Ince...", "topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mec..."]}
{"query": "Do the methods that work best on academic papers also work best on Wikipedia?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Based on the provided information, it is not possible to determine if the methods that work best on academic papers also work best on Wikipedia.", "source_documents": ["https://en.wikipedia.org/wiki/Category:Featured_articles. This resulted in around 5K FA, 28K GA, 212...", "not improve for 20 epochs. For evaluation, we use (micro-)accuracy, following previous studies BIBRE...", "5), the basic Inception model applies multiple convolution filters in parallel and concatenates the ...", "topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mec..."]}
{"query": "What is their system's absolute accuracy?", "gt": "59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 26 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The passage does not provide the absolute accuracy of their system.", "source_documents": ["subset.\nExperimental Setting\nAs discussed above, our model has two main components \u2014 biLSTM and Ince...", "not improve for 20 epochs. For evaluation, we use (micro-)accuracy, following previous studies BIBRE...", "topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mec...", "separation between Start and Stub achieved by Joint being the clearest among the three models. Incep..."]}
{"query": "Which is more useful, visual or textual features?", "gt": "It depends on the dataset. Experimental results over two datasets reveal that textual and visual features are complementary. ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 18 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The joint model, which combines both visual and textual features, outperformed the visual-only and text-only models in all cases. Thus, both visual and textual features are useful, and their combination yields the best results.", "source_documents": ["separation between Start and Stub achieved by Joint being the clearest among the three models. Incep...", "topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mec...", "Introduction\nThe task of document quality assessment is to automatically assess a document according...", "the performance of Joint is statistically significant ( $p<0.05$ ). This shows that the textual and ..."]}
{"query": "Which languages do they use?", "gt": "English", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The context does not mention anything about the languages used in the model. Therefore, it is not possible to determine which languages they use.", "source_documents": ["subset.\nExperimental Setting\nAs discussed above, our model has two main components \u2014 biLSTM and Ince...", "5), the basic Inception model applies multiple convolution filters in parallel and concatenates the ...", "topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mec...", "not improve for 20 epochs. For evaluation, we use (micro-)accuracy, following previous studies BIBRE..."]}
{"query": "How large is their data set?", "gt": "a sample of  29,794 wikipedia articles and 2,794 arXiv papers ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 10 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Their dataset consists of 29,794 articles.", "source_documents": ["subset.\nExperimental Setting\nAs discussed above, our model has two main components \u2014 biLSTM and Ince...", "not improve for 20 epochs. For evaluation, we use (micro-)accuracy, following previous studies BIBRE...", "https://en.wikipedia.org/wiki/Category:Featured_articles. This resulted in around 5K FA, 28K GA, 212...", "5), the basic Inception model applies multiple convolution filters in parallel and concatenates the ..."]}
{"query": "Where do they get their ground truth quality judgments?", "gt": "quality class labels assigned by the Wikipedia community, a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 49 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The ground truth quality judgments for the documents in the experiments are obtained from the Wikipedia community. The quality class labels for the Wikipedia dataset are assigned by Wikipedia reviewers or registered users based on the grading scheme of Wikipedia. The dataset includes articles from different quality classes such as Featured Article (\"FA\"), Good Article (\"GA\"), B-class Article (\"B\"), C-class Article (\"C\"), Start Article (\"Start\"), and Stub Article (\"Stub\").", "source_documents": ["subset.\nExperimental Setting\nAs discussed above, our model has two main components \u2014 biLSTM and Ince...", "topic relevance, organization and coherence, word usage and sentence complexity, and grammar and mec...", "separation between Start and Stub achieved by Joint being the clearest among the three models. Incep...", "5), the basic Inception model applies multiple convolution filters in parallel and concatenates the ..."]}
{"query": "What is the seed lexicon?", "gt": "a vocabulary of positive and negative predicates that helps determine the polarity score of an event", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 16 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The seed lexicon is a collection of positive and negative words or predicates used in the study. The positive words include \"\u559c\u3076\" (rejoice), \"\u5b09\u3057\u3044\" (be glad), \"\u697d\u3057\u3044\" (be pleasant), and others, while the negative words include \"\u6012\u308b\" (get angry), \"\u60b2\u3057\u3044\" (be sad), \"\u5bc2\u3057\u3044\" (be lonely), and others. The seed lexicon is used to assign polarity scores to the extracted event pairs based on whether the predicates match the events.", "source_documents": ["into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary ...", "\u201c\u80a9\u3092\u843d\u3068\u3059\" (lit. drop one's shoulders) is an idiom that expresses a disappointed feeling. The examples ...", "and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon a...", "Pairs\nOur method requires a very small seed lexicon and a large raw corpus. We assume that we can au..."]}
{"query": "What is the seed lexicon?", "gt": "seed lexicon consists of positive and negative predicates", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 8 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The seed lexicon is a collection of positive and negative words or predicates that are used to assign polarity scores to extracted events. It consists of words such as \"rejoice,\" \"be glad,\" \"get angry,\" \"be sad,\" etc. The seed lexicon is used as a reference to assign polarity scores to events based on whether their predicate matches with the words in the seed lexicon or not. The polarity scores can be either positive (+1) or negative (-1) depending on the word match.", "source_documents": ["into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary ...", "and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon a...", "\u201c\u80a9\u3092\u843d\u3068\u3059\" (lit. drop one's shoulders) is an idiom that expresses a disappointed feeling. The examples ...", "Pairs\nOur method requires a very small seed lexicon and a large raw corpus. We assume that we can au..."]}
{"query": "What are the results?", "gt": "Using all data to train: AL -- BiGRU achieved 0.843 accuracy, AL -- BERT achieved 0.863 accuracy, AL+CA+CO -- BiGRU achieved 0.866 accuracy, AL+CA+CO -- BERT achieved 0.835, accuracy, ACP -- BiGRU achieved 0.919 accuracy, ACP -- BERT achived 0.933, accuracy, ACP+AL+CA+CO -- BiGRU achieved 0.917 accuracy, ACP+AL+CA+CO -- BERT achieved 0.913 accuracy. \nUsing a subset to train: BERT achieved 0.876 accuracy using ACP (6K), BERT achieved 0.886 accuracy using ACP (6K) + AL, BiGRU achieved 0.830 accuracy using ACP (6K), BiGRU achieved 0.879 accuracy using ACP (6K) + AL + CA + CO.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 94 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The provided context does not contain any information about the results of the experiments. Therefore, I don't know the specific results.", "source_documents": ["into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary ...", "and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon a...", "$x_{i2}$, respectively. $N_{\\rm AL}$ is the total number of AL pairs, and $\\lambda _{\\rm AL}$ is a h...", "TABREF16.\nExperiments ::: Dataset ::: ACP (ACP Corpus)\nWe used the latest version of the ACP Corpus ..."]}
{"query": "How are relations used to propagate polarity?", "gt": "cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 19 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Relations are used to propagate polarity by assigning polarity scores to event pairs based on their discourse relation type. Specifically, in the proposed method, if the discourse relation type is Cause, the former event is given the same score as the latter event. If the discourse relation type is Concession, the former event is given the opposite score of the latter event. This allows for the propagation of polarity information from one event to another through their discourse relation.", "source_documents": ["purely supervised baseline when labeled data were small.\nRelated Work\nLearning affective events is c...", "into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary ...", "Pairs\nOur method requires a very small seed lexicon and a large raw corpus. We assume that we can au...", "and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon a..."]}
{"query": "How big is the Japanese data?", "gt": "The ACP corpus has around 700k events split into positive and negative polarity ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 13 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The context does not provide information about the size of the Japanese data. Therefore, it is not possible to determine how big it is.", "source_documents": ["into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary ...", "\u201c\u80a9\u3092\u843d\u3068\u3059\" (lit. drop one's shoulders) is an idiom that expresses a disappointed feeling. The examples ...", "$x_{i2}$, respectively. $N_{\\rm AL}$ is the total number of AL pairs, and $\\lambda _{\\rm AL}$ is a h...", "TABREF16.\nExperiments ::: Dataset ::: ACP (ACP Corpus)\nWe used the latest version of the ACP Corpus ..."]}
{"query": "What are labels available in dataset for supervision?", "gt": "negative, positive", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The labels available in the dataset for supervision are positive and negative.", "source_documents": ["into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary ...", "and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon a...", "$x_{i2}$, respectively. $N_{\\rm AL}$ is the total number of AL pairs, and $\\lambda _{\\rm AL}$ is a h...", "Pairs\nOur method requires a very small seed lexicon and a large raw corpus. We assume that we can au..."]}
{"query": "How big are improvements of supervszed learning results trained on smalled labeled data enhanced with proposed approach copared to basic approach?", "gt": "3%", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The given context does not provide specific information about the size of the improvements in supervised learning results when trained on smaller labeled data enhanced with the proposed approach compared to the basic approach. Therefore, it is not possible to provide an answer to the question.", "source_documents": ["into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary ...", "and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon a...", "TABREF16.\nExperiments ::: Dataset ::: ACP (ACP Corpus)\nWe used the latest version of the ACP Corpus ...", "purely supervised baseline when labeled data were small.\nRelated Work\nLearning affective events is c..."]}
{"query": "How does their model learn using mostly raw data?", "gt": "by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 14 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Their model learns using mostly raw data by extracting discourse-tagged event pairs from the raw corpus. These event pairs consist of former and latter events, which are then classified into three types: AL (Automatically Labeled Pairs), CA (Cause Pairs), and CO (Concession Pairs). The seed lexicon is used to assign polarity scores to the event pairs based on their discourse relation type. The model then optimizes the parameters of the polarity function using mean squared error loss functions for each type of event pair. By propagating the labels through these event pairs, the model is able to learn complex phenomena and make predictions on new data. The suggested output length is around 14 words.", "source_documents": ["into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary ...", "and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon a...", "TABREF16.\nExperiments ::: Dataset ::: ACP (ACP Corpus)\nWe used the latest version of the ACP Corpus ...", "Pairs\nOur method requires a very small seed lexicon and a large raw corpus. We assume that we can au..."]}
{"query": "How big is seed lexicon used for training?", "gt": "30 words", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The size of the seed lexicon used for training is not specified in the given context.", "source_documents": ["into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary ...", "and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon a...", "Pairs\nOur method requires a very small seed lexicon and a large raw corpus. We assume that we can au...", "\u201c\u80a9\u3092\u843d\u3068\u3059\" (lit. drop one's shoulders) is an idiom that expresses a disappointed feeling. The examples ..."]}
{"query": "How large is raw corpus used for training?", "gt": "100 million sentences", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 3 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The size of the raw corpus used for training is about 100 million sentences.", "source_documents": ["into words by Juman++, and words were broken into subwords by applying BPE BIBREF20. The vocabulary ...", "TABREF16.\nExperiments ::: Dataset ::: ACP (ACP Corpus)\nWe used the latest version of the ACP Corpus ...", "and negative labels were distributed evenly. The Random+Seed baseline made use of the seed lexicon a...", "$x_{i2}$, respectively. $N_{\\rm AL}$ is the total number of AL pairs, and $\\lambda _{\\rm AL}$ is a h..."]}
{"query": "What data is the language model pretrained on?", "gt": "Chinese general corpus", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 3 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The language model is pretrained on a Chinese general corpus.", "source_documents": ["answer of the output for start word and end word respectively.\nThe Proposed Model for QA-CTS Task ::...", "BIBREF4 use large amount of data to automatically model the specific task. Topaz et al. BIBREF3 cons...", "3 or 4 due to the lack of graphical memory. We select BERT-base as the pre-trained language model in...", "the original sentence will be tagged a label following a tag scheme. In this paper we recognize the ..."]}
{"query": "What baselines is the proposed model compared against?", "gt": "BERT-Base, QANet", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The proposed model is compared against the QANet model and the BERT-Base model.", "source_documents": ["answer of the output for start word and end word respectively.\nThe Proposed Model for QA-CTS Task ::...", "3 or 4 due to the lack of graphical memory. We select BERT-base as the pre-trained language model in...", "BIBREF4 use large amount of data to automatically model the specific task. Topaz et al. BIBREF3 cons...", "proposed model achieved a 5.64% score improvement in EM-score and a 3.69% score improvement in F$_1$..."]}
{"query": "How is the clinical text structuring task defined?", "gt": "Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained., Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 78 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The clinical text structuring task is defined as the extraction or generation of a key-value pair from a sequence of paragraph text. The key, denoted as Q, is a query term, and the value, denoted as V, is the result of the query term according to the paragraph text. The task involves picking out the answer-related text and performing additional steps such as entity names conversion and negative words recognition to generate the final answer. The objective is to find the answer-related substring from the paragraph text. The suggested output length is around 78 words.", "source_documents": ["Introduction\nClinical text structuring (CTS) is a critical task for fetching medical research data f...", "entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output...", "problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved im...", "this paper, we present a question answering based clinical text structuring (QA-CTS) task, which uni..."]}
{"query": "How is the clinical text structuring task defined?", "gt": "CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 24 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The clinical text structuring task is defined as extracting or generating a key-value pair from a sequence of paragraph text. The key, represented by Q, is a query term, and the value, represented by V, is the result of the query term according to the paragraph text. The task involves finding the answer-related substring from the paragraph text, which unifies the output format of different clinical text structuring tasks. The suggested output length is around 24 words.", "source_documents": ["Introduction\nClinical text structuring (CTS) is a critical task for fetching medical research data f...", "entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output...", "problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved im...", "this paper, we present a question answering based clinical text structuring (QA-CTS) task, which uni..."]}
{"query": "What are the specific tasks being unified?", "gt": " three types of questions, namely tumor size, proximal resection margin and distal resection margin", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 14 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The specific tasks being unified in the QA-CTS task are different clinical text structuring tasks.", "source_documents": ["this paper, we present a question answering based clinical text structuring (QA-CTS) task, which uni...", "BIBREF4 use large amount of data to automatically model the specific task. Topaz et al. BIBREF3 cons...", "entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output...", "answer of the output for start word and end word respectively.\nThe Proposed Model for QA-CTS Task ::..."]}
{"query": "Is all text in this dataset a question, or are there unrelated sentences in between questions?", "gt": "the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 24 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Based on the provided context, there is no information about whether all text in the dataset is a question or if there are unrelated sentences in between questions. Therefore, I don't know the answer to this question.", "source_documents": ["problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved im...", "answer of the output for start word and end word respectively.\nThe Proposed Model for QA-CTS Task ::...", "this paper, we present a question answering based clinical text structuring (QA-CTS) task, which uni...", "section, we present an effective model for the question answering based clinical text structuring (Q..."]}
{"query": "How many questions are in the dataset?", "gt": "2,714 ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "2,714", "source_documents": ["answer of the output for start word and end word respectively.\nThe Proposed Model for QA-CTS Task ::...", "the original sentence will be tagged a label following a tag scheme. In this paper we recognize the ...", "3 or 4 due to the lack of graphical memory. We select BERT-base as the pre-trained language model in...", "problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved im..."]}
{"query": "How they introduce domain-specific features into pre-trained language model?", "gt": "integrate clinical named entity information into pre-trained language model", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 9 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "They introduce domain-specific features into the pre-trained language model by attaching named entity information layers and retraining the model. The final prediction is made using the integrated representation of the named entity information and the contextualized representation generated by the pre-trained language model. The output length is not specified in the given context.", "source_documents": ["answer of the output for start word and end word respectively.\nThe Proposed Model for QA-CTS Task ::...", "BIBREF4 use large amount of data to automatically model the specific task. Topaz et al. BIBREF3 cons...", "3 or 4 due to the lack of graphical memory. We select BERT-base as the pre-trained language model in...", "the original sentence will be tagged a label following a tag scheme. In this paper we recognize the ..."]}
{"query": "How big is QA-CTS task dataset?", "gt": "17,833 sentences, 826,987 characters and 2,714 question-answer pairs", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 8 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The QA-CTS task dataset contains 17,833 sentences, 826,987 characters, and 2,714 question-answer pairs.", "source_documents": ["this paper, we present a question answering based clinical text structuring (QA-CTS) task, which uni...", "answer of the output for start word and end word respectively.\nThe Proposed Model for QA-CTS Task ::...", "entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output...", "problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved im..."]}
{"query": "What are strong baseline models in specific tasks?", "gt": "state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 10 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The strong baseline models in specific tasks are not explicitly mentioned in the given context. Therefore, I don't know the answer.", "source_documents": ["BIBREF4 use large amount of data to automatically model the specific task. Topaz et al. BIBREF3 cons...", "answer of the output for start word and end word respectively.\nThe Proposed Model for QA-CTS Task ::...", "3 or 4 due to the lack of graphical memory. We select BERT-base as the pre-trained language model in...", "this paper, we present a question answering based clinical text structuring (QA-CTS) task, which uni..."]}
{"query": "What is the previous work's model?", "gt": "Ternary Trans-CNN", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Trans-CNN", "source_documents": ["belonged to.\nBased on all the experiments we conducted below model had best performance related to m...", "English. For example to express love, a code mixed spelling, specially when used social platforms mi...", "that was available for this task was very limited in number of examples and thus as noted above few ...", "dataset are below:\nDataset and Features ::: Challenges\nThe obtained data set had many challenges and..."]}
{"query": "What dataset is used?", "gt": "HEOT , A labelled dataset for a corresponding english tweets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 10 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The dataset used is the HEOT dataset, which consists of cleaned tweets obtained from Twitter conversations in the Indian subcontinent, as well as a labeled dataset of corresponding English tweets obtained from a study conducted by Davidson et al.", "source_documents": ["English. For example to express love, a code mixed spelling, specially when used social platforms mi...", "dataset are below:\nDataset and Features ::: Challenges\nThe obtained data set had many challenges and...", "belonged to.\nBased on all the experiments we conducted below model had best performance related to m...", "that was available for this task was very limited in number of examples and thus as noted above few ..."]}
{"query": "How big is the dataset?", "gt": "3189 rows of text messages", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 5 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The dataset consists of 3189 rows.", "source_documents": ["English. For example to express love, a code mixed spelling, specially when used social platforms mi...", "dataset are below:\nDataset and Features ::: Challenges\nThe obtained data set had many challenges and...", "belonged to.\nBased on all the experiments we conducted below model had best performance related to m...", "that was available for this task was very limited in number of examples and thus as noted above few ..."]}
{"query": "How big is the dataset?", "gt": "Resulting dataset was 7934 messages for train and 700 messages for test.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The dataset used in the study contained 3189 text messages with an average length of 116 words.", "source_documents": ["English. For example to express love, a code mixed spelling, specially when used social platforms mi...", "dataset are below:\nDataset and Features ::: Challenges\nThe obtained data set had many challenges and...", "belonged to.\nBased on all the experiments we conducted below model had best performance related to m...", "that was available for this task was very limited in number of examples and thus as noted above few ..."]}
{"query": "How is the dataset collected?", "gt": "A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al, HEOT obtained from one of the past studies done by Mathur et al", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 32 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The dataset was collected from past studies conducted by Mathur et al. and Davidson et al. The dataset consists of cleaned tweets obtained from Twitter for conversations happening in the Indian subcontinent. A labeled dataset for corresponding English tweets was also obtained from Davidson et al. The dataset was cleaned and prepared for deep learning by removing messy text messages, eliminating stop words, and translating/transliterating Hinglish tweets to English. The dataset was small, so data augmentation techniques were employed to boost the learning of the deep network. The final distribution of messages for training and testing is not provided.", "source_documents": ["English. For example to express love, a code mixed spelling, specially when used social platforms mi...", "dataset are below:\nDataset and Features ::: Challenges\nThe obtained data set had many challenges and...", "belonged to.\nBased on all the experiments we conducted below model had best performance related to m...", "that was available for this task was very limited in number of examples and thus as noted above few ..."]}
{"query": "Was each text augmentation technique experimented individually?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "No, the text augmentation techniques were not experimented individually. They were used together in the training phase.", "source_documents": ["that was available for this task was very limited in number of examples and thus as noted above few ...", "English. For example to express love, a code mixed spelling, specially when used social platforms mi...", "belonged to.\nBased on all the experiments we conducted below model had best performance related to m...", "dataset are below:\nDataset and Features ::: Challenges\nThe obtained data set had many challenges and..."]}
{"query": "What models do previous work use?", "gt": "Ternary Trans-CNN , Hybrid multi-channel CNN and LSTM", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 8 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Previous work uses Fully Connected dense networks, Convolution based architectures, andSequence models such as SimpleRNN, LSTM, GRU, and Bidirectional LSTM model architectures.", "source_documents": ["belonged to.\nBased on all the experiments we conducted below model had best performance related to m...", "English. For example to express love, a code mixed spelling, specially when used social platforms mi...", "that was available for this task was very limited in number of examples and thus as noted above few ...", "detective offensive vs non offensive tweets and messages written in Hinglish in social media. The ut..."]}
{"query": "Does the dataset contain content from various social media platforms?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Based on the given context, it is not mentioned whether the dataset contains content from various social media platforms.", "source_documents": ["English. For example to express love, a code mixed spelling, specially when used social platforms mi...", "dataset are below:\nDataset and Features ::: Challenges\nThe obtained data set had many challenges and...", "belonged to.\nBased on all the experiments we conducted below model had best performance related to m...", "that was available for this task was very limited in number of examples and thus as noted above few ..."]}
{"query": "What additional techniques are incorporated?", "gt": "incorporating coding syntax tree model", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 5 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The additional techniques incorporated are LSTM (Long Short-Term Memory) for seq2seq translation, attention mechanism, and sigmoidal optimization for prediction optimization.", "source_documents": ["the source texts and another for the code. Vocabulary generation is done by tokenization of words. A...", "and Japanese. In the same year, Chris Q. et al. mapped natural language with simple if-this-then-tha...", "validation data from the source corpus was 74.40% (Fig. FIGREF17).\nAlthough the generated code is in...", "through XML mapping BIBREF2. They tried to convert codes from C++ to Java using XML mapping as an in..."]}
{"query": "What dataset do they use?", "gt": "A parallel corpus where the source is an English expression of code and the target is Python code.", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 18 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The dataset used is a parallel corpus consisting of 18805 aligned data. The source data is written in English language and the target data is written in Python programming language. The suggested output length is not mentioned in the given context.", "source_documents": ["the source texts and another for the code. Vocabulary generation is done by tokenization of words. A...", "validation data from the source corpus was 74.40% (Fig. FIGREF17).\nAlthough the generated code is in...", "and Japanese. In the same year, Chris Q. et al. mapped natural language with simple if-this-then-tha...", "through XML mapping BIBREF2. They tried to convert codes from C++ to Java using XML mapping as an in..."]}
{"query": "Do they compare to other models?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The given text does not provide any information about comparing the proposed model to other models. Therefore, it is not possible to answer the question.", "source_documents": ["the source texts and another for the code. Vocabulary generation is done by tokenization of words. A...", "and Japanese. In the same year, Chris Q. et al. mapped natural language with simple if-this-then-tha...", "validation data from the source corpus was 74.40% (Fig. FIGREF17).\nAlthough the generated code is in...", "through XML mapping BIBREF2. They tried to convert codes from C++ to Java using XML mapping as an in..."]}
{"query": "What is the architecture of the system?", "gt": "seq2seq translation", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "LSTM model", "source_documents": ["the source texts and another for the code. Vocabulary generation is done by tokenization of words. A...", "and Japanese. In the same year, Chris Q. et al. mapped natural language with simple if-this-then-tha...", "through XML mapping BIBREF2. They tried to convert codes from C++ to Java using XML mapping as an in...", "validation data from the source corpus was 74.40% (Fig. FIGREF17).\nAlthough the generated code is in..."]}
{"query": "What additional techniques could be incorporated to further improve accuracy?", "gt": "phrase-based word embedding, Abstract Syntax Tree(AST)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 6 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Abstract Syntax Tree (AST) and phrase-based word embedding can be incorporated to further improve accuracy.", "source_documents": ["the source texts and another for the code. Vocabulary generation is done by tokenization of words. A...", "validation data from the source corpus was 74.40% (Fig. FIGREF17).\nAlthough the generated code is in...", "and Japanese. In the same year, Chris Q. et al. mapped natural language with simple if-this-then-tha...", "through XML mapping BIBREF2. They tried to convert codes from C++ to Java using XML mapping as an in..."]}
{"query": "What programming language is target language?", "gt": "Python", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Python", "source_documents": ["validation data from the source corpus was 74.40% (Fig. FIGREF17).\nAlthough the generated code is in...", "and Japanese. In the same year, Chris Q. et al. mapped natural language with simple if-this-then-tha...", "Introduction\nRemoving computer-human language barrier is an inevitable advancement researchers are t...", "through XML mapping BIBREF2. They tried to convert codes from C++ to Java using XML mapping as an in..."]}
{"query": "What dataset is used to measure accuracy?", "gt": "validation data", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The dataset used to measure accuracy is the validation data from the source corpus.", "source_documents": ["the source texts and another for the code. Vocabulary generation is done by tokenization of words. A...", "validation data from the source corpus was 74.40% (Fig. FIGREF17).\nAlthough the generated code is in...", "and Japanese. In the same year, Chris Q. et al. mapped natural language with simple if-this-then-tha...", "through XML mapping BIBREF2. They tried to convert codes from C++ to Java using XML mapping as an in..."]}
{"query": "What corpus was the source of the OpenIE extractions?", "gt": "domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 18 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The source of the OpenIE extractions was the Sentence corpus, which consists of domain-targeted approximately 80K sentences and 280 GB of plain text extracted from web pages.", "source_documents": ["higher grades. Errors due to lossy IE and misalignments suggest future work in incorporating context...", "graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules ...", "phrases found in Open IE tuples.\nCompared to a curated KB, it is easy to find Open IE tuples that ma...", "an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the indiv..."]}
{"query": "What is the accuracy of the proposed technique?", "gt": "51.7 and 51.6 on 4th and 8th grade question sets with no curated knowledge. 47.5 and 48.0 on 4th and 8th grade question sets when both solvers are given the same knowledge", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 32 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The given context does not provide information about the accuracy of the proposed technique.", "source_documents": ["higher grades. Errors due to lossy IE and misalignments suggest future work in incorporating context...", "graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules ...", "an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the indiv...", "Introduction\nEffective question answering (QA) systems have been a long-standing quest of AI researc..."]}
{"query": "Is an entity linking process used?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "No", "source_documents": ["higher grades. Errors due to lossy IE and misalignments suggest future work in incorporating context...", "Introduction\nEffective question answering (QA) systems have been a long-standing quest of AI researc...", "N/n_x) \\\\ &\\textit {tf-idf}(t, q)=\\sum _{x \\in t\\cap q} idf(x) $\nwhere $N$ is the number of tuples i...", "graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules ..."]}
{"query": "Are the OpenIE extractions all triples?", "gt": "No", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Based on the given context, we can infer that OpenIE extractions are not limited to triples. The text mentions that Open IE tuples do not come with schema and join rules and that TupleInf uses a substantially simpler model compared to TableILP. This suggests that OpenIE extractions can have different structures and lengths, not necessarily limited to triples.", "source_documents": ["phrases found in Open IE tuples.\nCompared to a curated KB, it is easy to find Open IE tuples that ma...", "higher grades. Errors due to lossy IE and misalignments suggest future work in incorporating context...", "graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules ...", "an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the indiv..."]}
{"query": "What method was used to generate the OpenIE extractions?", "gt": "for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S, take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 56 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The method used to generate the OpenIE extractions is not specified in the given text.", "source_documents": ["higher grades. Errors due to lossy IE and misalignments suggest future work in incorporating context...", "phrases found in Open IE tuples.\nCompared to a curated KB, it is easy to find Open IE tuples that ma...", "graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules ...", "an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the indiv..."]}
{"query": "Can the method answer multi-hop questions?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Based on the given context, there is no explicit information about whether the method can answer multi-hop questions.", "source_documents": ["Introduction\nEffective question answering (QA) systems have been a long-standing quest of AI researc...", "higher grades. Errors due to lossy IE and misalignments suggest future work in incorporating context...", "an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the indiv...", "N/n_x) \\\\ &\\textit {tf-idf}(t, q)=\\sum _{x \\in t\\cap q} idf(x) $\nwhere $N$ is the number of tuples i..."]}
{"query": "What was the textual source to which OpenIE was applied?", "gt": "domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 18 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The textual source to which OpenIE was applied is the Sentence corpus consisting of domain-targeted 80K sentences and 280 GB of plain text extracted from web pages.", "source_documents": ["higher grades. Errors due to lossy IE and misalignments suggest future work in incorporating context...", "phrases found in Open IE tuples.\nCompared to a curated KB, it is easy to find Open IE tuples that ma...", "graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules ...", "Introduction\nEffective question answering (QA) systems have been a long-standing quest of AI researc..."]}
{"query": "What OpenIE method was used to generate the extractions?", "gt": "for each multiple-choice question $(q,A) \\in Q_\\mathit {tr}$ and each choice $a \\in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S, take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \\in A$ and over all questions in $Q_\\mathit {tr}$", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 56 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The passage does not provide information about the specific OpenIE method that was used to generate the extractions.", "source_documents": ["higher grades. Errors due to lossy IE and misalignments suggest future work in incorporating context...", "phrases found in Open IE tuples.\nCompared to a curated KB, it is easy to find Open IE tuples that ma...", "graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules ...", "an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the indiv..."]}
{"query": "Is their method capable of multi-hop reasoning?", "gt": "Yes", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Yes, the method described in the given context is capable of multi-hop reasoning.", "source_documents": ["Introduction\nEffective question answering (QA) systems have been a long-standing quest of AI researc...", "an ensemble system BIBREF6 which, as shown in the table, provides a substantial boost over the indiv...", "higher grades. Errors due to lossy IE and misalignments suggest future work in incorporating context...", "graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules ..."]}
{"query": "What models do they propose?", "gt": "Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), Textual Kernels Model (TKM)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The models proposed are the LSTM, the Davison method trained with MMHS150K, and random scores. They also propose the Feature Concatenation Model (FCM), the Spatial Concatenation Model (SCM), and the Textual Kernels Model (TKM).", "source_documents": ["of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to t...", "in the FCM model, three fully connected layers are used to reduce the dimensionality until the class...", "extract it and also input it to our model. To do so, we use Google Vision API Text Detection module ...", "multimodal models to use the visual information. Eventually, though, these models end up using almos..."]}
{"query": "Are all tweets in English?", "gt": "Unanswerable", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 1 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Yes.", "source_documents": ["if a tweet is hate speech or not. But a considerable amount of the images of the selected tweets con...", "used for training.\nWe also experimented using hate scores for each tweet computed given the differen...", "Introduction\nSocial Media platforms such as Facebook, Twitter or Reddit have empowered individuals' ...", "methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn mul..."]}
{"query": "How large is the dataset?", "gt": " $150,000$ tweets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "150,000 tweets.", "source_documents": ["used for training.\nWe also experimented using hate scores for each tweet computed given the differen...", "methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn mul...", "if a tweet is hate speech or not. But a considerable amount of the images of the selected tweets con...", "multimodal models to use the visual information. Eventually, though, these models end up using almos..."]}
{"query": "What is the results of multimodal compared to unimodal models?", "gt": "Unimodal LSTM vs Best Multimodal (FCM)\n- F score: 0.703 vs 0.704\n- AUC: 0.732 vs 0.734 \n- Mean Accuracy: 68.3 vs 68.4 ", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 23 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The results of the multimodal models are not able to improve the detection compared to the textual (unimodal) models.", "source_documents": ["multimodal models to use the visual information. Eventually, though, these models end up using almos...", "extract it and also input it to our model. To do so, we use Google Vision API Text Detection module ...", "in the FCM model, three fully connected layers are used to reduce the dimensionality until the class...", "of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to t..."]}
{"query": "What is author's opinion on why current multimodal models cannot outperform models analyzing only text?", "gt": "Noisy data, Complexity and diversity of multimodal relations, Small set of multimodal examples", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 13 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The author suggests that the complexity and diversity of multimodal relations in hate speech publications, the small set of multimodal examples, and the challenge of noisy data contribute to the current multimodal models not outperforming models analyzing only text.", "source_documents": ["multimodal models to use the visual information. Eventually, though, these models end up using almos...", "of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to t...", "extract it and also input it to our model. To do so, we use Google Vision API Text Detection module ...", "in the FCM model, three fully connected layers are used to reduce the dimensionality until the class..."]}
{"query": "What metrics are used to benchmark the results?", "gt": "F-score, Area Under the ROC Curve (AUC), mean accuracy (ACC), Precision vs Recall plot, ROC curve (which plots the True Positive Rate vs the False Positive Rate)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 27 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The metrics used to benchmark the results are F-score, Area Under the ROC Curve (AUC), and mean accuracy (ACC).", "source_documents": ["of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to t...", "in the FCM model, three fully connected layers are used to reduce the dimensionality until the class...", "used for training.\nWe also experimented using hate scores for each tweet computed given the differen...", "extract it and also input it to our model. To do so, we use Google Vision API Text Detection module ..."]}
{"query": "How is data collected, manual collection or Twitter api?", "gt": "Twitter API", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Twitter API", "source_documents": ["if a tweet is hate speech or not. But a considerable amount of the images of the selected tweets con...", "used for training.\nWe also experimented using hate scores for each tweet computed given the differen...", "methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn mul...", "datasets:\n[noitemsep,leftmargin=*]\nRM BIBREF10: Formed by $2,435$ tweets discussing Refugees and Mus..."]}
{"query": "How many tweats does MMHS150k contains, 150000?", "gt": "$150,000$ tweets", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 2 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "Yes, the MMHS150k dataset contains 150,000 tweets.", "source_documents": ["if a tweet is hate speech or not. But a considerable amount of the images of the selected tweets con...", "methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn mul...", "of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to t...", "multimodal models to use the visual information. Eventually, though, these models end up using almos..."]}
{"query": "What unimodal detection models were used?", "gt": " single layer LSTM with a 150-dimensional hidden state for hate / not hate classification", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 14 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The unimodal detection models used in the study were the LSTM and the Davison method proposed in BIBREF7 trained with MMHS150K.", "source_documents": ["of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to t...", "extract it and also input it to our model. To do so, we use Google Vision API Text Detection module ...", "multimodal models to use the visual information. Eventually, though, these models end up using almos...", "follows:\n[noitemsep,leftmargin=*]\nWe propose the novel task of hate speech detection in multimodal p..."]}
{"query": "What different models for multimodal detection were proposed?", "gt": "Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), Textual Kernels Model (TKM)", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 12 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The different models for multimodal detection that were proposed are the Feature Concatenation Model (FCM), the Spatial Concatenation Model (SCM), and the Textual Kernels Model (TKM).", "source_documents": ["multimodal models to use the visual information. Eventually, though, these models end up using almos...", "extract it and also input it to our model. To do so, we use Google Vision API Text Detection module ...", "of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to t...", "in the FCM model, three fully connected layers are used to reduce the dimensionality until the class..."]}
{"query": "What annotations are available in the dataset - tweat used hate speach or not?", "gt": "No attacks to any community,  racist, sexist, homophobic, religion based attacks, attacks to other communities", "prompt": "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. \n\\{context\\} \nQuestion: \\{question\\} The suggested output length is around 15 words.", "evaluation": "f1", "retrieval-turbo-0613_pred": "The dataset includes annotations for each tweet to determine if it contains hate speech or not. The tweets are labeled in one of six categories: No attacks to any community, racist, sexist, homophobic, religion based attacks, or attacks to other communities. Each tweet is labeled by three different workers, and the majority voting between the annotations is used to determine the tweet's category.", "source_documents": ["if a tweet is hate speech or not. But a considerable amount of the images of the selected tweets con...", "used for training.\nWe also experimented using hate scores for each tweet computed given the differen...", "methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn mul...", "follows:\n[noitemsep,leftmargin=*]\nWe propose the novel task of hate speech detection in multimodal p..."]}
