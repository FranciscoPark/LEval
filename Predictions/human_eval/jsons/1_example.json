{"input": "Professor B: I think for two years we were two months , uh , away from being done .\nPhD A: And what was that , Morgan ? What project ?\nProfessor B: Uh , the , uh , TORRENT chip .\nPhD A: Oh .\nProfessor B: Yeah . We were two {disfmarker} we were {disfmarker}\nPhD C: Yeah .\nProfessor B: Uh , uh , we went through it {disfmarker} Jim and I went through old emails at one point and {disfmarker} and for two years there was this thing saying , yeah , we 're {disfmarker} we 're two months away from being done . It was very {disfmarker} very believable schedules , too . I mean , we went through and {disfmarker} with the schedules {disfmarker} and we {disfmarker}\nPhD A: It was true for two years .\nProfessor B: Yeah . Oh , yeah . It was very true .\nPhD A: So , should we just do the same kind of deal where we {pause} go around and do , uh , status report {pause} kind of things ? OK . And I guess when Sunil gets here he can do his last or something . So .\nProfessor B: Yeah . So we {pause} probably should wait for him to come before we do his .\nPhD C: Mm - hmm .\nPhD A: OK . That 's a good idea .\nProfessor B: Yeah .\nGrad F: OK .\nProfessor B: Yeah .\nPhD A: Any objection ? Do y OK , M\nProfessor B: All in favor\nPhD A: Do you want to start , Morgan ? Do you have anything , or {disfmarker} ?\nProfessor B: Uh , I don't do anything . I {disfmarker} No , I mean , I {disfmarker} I 'm involved in discussions with {disfmarker} with people about what they 're doing , but I think they 're {disfmarker} since they 're here , they can talk about it themselves .\nGrad F: OK . So should I go so that , uh ,\nPhD A: Yeah . Why don't you go ahead , Barry ?\nGrad F: you 're gonna talk about Aurora stuff , per se ?\nPhD A: OK .\nGrad F: OK . Um . Well , this past week I 've just been , uh , getting down and dirty into writing my {disfmarker} my proposal . So , um {disfmarker} Mmm . I just finished a section on , uh {disfmarker} on talking about these intermediate categories that I want to classify , um , as a {disfmarker} as a middle step . And , um , I hope to {disfmarker} hope to get this , um {disfmarker} a full rough draft done by , uh , Monday so I can give it to Morgan .\nPhD A: When is your , uh , meeting ?\nGrad F: Um , my meeting\nPhD A: Yeah .\nGrad F: with , uh {disfmarker} ? Oh , oh , you mean the {disfmarker} the quals .\nPhD A: The quals . Yeah .\nGrad F: Uh , the quals are happening in July twenty - fifth .\nPhD A: Oh . Soon .\nGrad F: Yeah .\nPhD A: Uh - huh .\nGrad F: D - Day .\nPhD A: Yeah .\nGrad F: Uh - huh .\nPhD A: So , is the idea you 're going to do this paper and then you pass it out to everybody ahead of time and {disfmarker} ?\nGrad F: Right , right . So , y you write up a proposal , and give it to people ahead of time , and you have a short presentation . And , um , and then , um {disfmarker} then everybody asks you questions .\nPhD A: Hmm .\nGrad F: Yeah .\nPhD A: I remember now .\nGrad F: Yep . So , um .\nPhD A: Have you d ? I was just gonna ask , do you want to say any {disfmarker} a little bit about it ,\nGrad F: Y s\nPhD A: or {disfmarker} ? Mmm .\nGrad F: Oh . Uh , a little bit about {disfmarker} ?\nPhD A: Wh - what you 're {disfmarker} what you 're gonna {disfmarker} You said {disfmarker} you were talking about the , uh , particular features that you were looking at ,\nGrad F: Oh , the {disfmarker} the {disfmarker}\nPhD A: or {disfmarker}\nGrad F: Right . Well , I was , um , I think one of the perplexing problems is , um , for a while I was thinking that I had to come up with a complete set of intermediate features {disfmarker} in intermediate categories to {disfmarker} to classify right away . But what I 'm thinking now is , I would start with {disfmarker} with a reasonable set . Something {disfmarker} something like , um , um {disfmarker} like , uh , re regular phonetic features , just to {disfmarker} just to start off that way . And do some phone recognition . Um , build a system that , uh , classifies these , um {disfmarker} these feat uh , these intermediate categories using , uh , multi - band techniques . Combine them and do phon phoneme recognition . Look at {disfmarker} then I would look at the errors produced in the phoneme recognition and say , OK , well , I could probably reduce the errors if I included this extra feature or this extra intermediate category . That would {disfmarker} that would reduce certain confusions over other confusions . And then {disfmarker} and then {vocalsound} reiterate . Um , build the intermediate classifiers . Uh , do phoneme recognition . Look at the errors . And then postulate new {disfmarker} or remove , um , intermediate categories . And then do it again .\nPhD A: So you 're gonna use TIMIT ?\nGrad F: Um , for that {disfmarker} for that part of the {disfmarker} the process , yeah , I would use TIMIT .\nPhD A: Mm - hmm .\nGrad F: And , um , then {disfmarker} after {disfmarker} after , uh , um , doing TIMIT . Right ?\nPhD A: Mm - hmm .\nGrad F: Um , that 's {disfmarker} {vocalsound} that 's , um {disfmarker} that 's just the ph the phone recognition task .\nPhD A: Yeah .\nGrad F: Uh , I wanted to take a look at , um , things that I could model within word . So , I would mov I would then shift the focus to , um , something like Schw - Switchboard , uh , where I 'd {disfmarker} I would be able to , um {disfmarker} to model , um , intermediate categories that span across phonemes ,\nPhD A: Mm - hmm .\nGrad F: not just within the phonemes , themselves , um , and then do the same process there , um , on {disfmarker} on a large vocabulary task like Switchboard . Uh , and for that {disfmarker} for that part I would {disfmarker} I 'd use the SRI recognizer since it 's already set up for {disfmarker} for Switchboard . And I 'd run some {disfmarker} some sort of tandem - style processing with , uh , my intermediate classifiers .\nPhD A: Oh . So that 's why you were interested in getting your own features into the SRI files .\nGrad F: Yeah . That 's why I {disfmarker} I was asking about that .\nPhD A: Yeah . Yeah .\nGrad F: Yeah . Um , and I guess that 's {disfmarker} that 's it . Any {disfmarker} any questions ?\nPhD A: Sounds good . So you just have a few more weeks , huh ?\nGrad F: Um , yeah . A few more .\nPhD A: It 's about a month from now ?\nGrad F: It 's a {disfmarker} it 's a month and {disfmarker} and a week .\nPhD A: Yeah .\nGrad F: Yeah .\nPhD A: So , uh , you want to go next , Dave ? And we 'll do {disfmarker}\nGrad E: Oh . OK , sure . So , um , last week I finally got results from the SRI system about this mean subtraction approach . And , um , we {disfmarker} we got an improvement , uh , in word error rate , training on the TI - digits data set and testing on Meeting Recorder digits of , um , {vocalsound} six percent to four point five percent , um , on the n on the far - mike data using PZM F , but , um , the near - mike performance worsened , um , from one point two percent to two point four percent . And , um , wh why would that be , um , {vocalsound} considering that we actually got an improvement in near - mike performance using HTK ? And so , uh , with some input from , uh , Andreas , I have a theory in two parts . Um , first of all HTK {disfmarker} sorry , SR - the SRI system is doing channel adaptation , and so HTK wasn't . Um , so this , um {disfmarker} This mean subtraction approach will do a kind of channel {pause} normalization and so that might have given the HTK use of it a boost that wouldn't have been applied in the SRI case . And also , um , the {disfmarker} Andreas pointed out the SRI system is using more parameters . It 's got finer - grained acoustic models . So those finer - grained acoustic models could be more sensitive to the artifacts {pause} in the re - synthesized audio . Um . And me and Barry were listening to the re - synthesized audio and sometimes it seems like you get of a bit of an echo of speech in the background . And so that seems like it could be difficult for training , cuz you could have {pause} different phones {pause} lined up with a different foreground phone , {vocalsound} um , {vocalsound} depending on {pause} the timing of the echo . So , um , I 'm gonna try training on a larger data set , and then , eh , the system will have seen more examples o of these artifacts and hopefully will be more robust to them . So I 'm planning to use the Macrophone set of , um , read speech , and , um {disfmarker} Hmm .\nProfessor B: I had another thought just now , which is , uh , remember we were talking before about {disfmarker} we were talking in our meeting about , uh , this stuff that {disfmarker} some of the other stuff that Avendano did , where they were , um , getting rid of low - energy {pause} sections ? Um , uh , if you {disfmarker} if you did a high - pass filtering , as Hirsch did in {pause} late eighties to reduce some of the effects of reverberation , uh , uh , Avendano and Hermansky were arguing that , uh , perhaps one of the reasons for that working was ma may not have even been the filtering so much but the fact that when you filter a {disfmarker} an all - positive power spectrum you get some negative values , and you gotta figure out what to do with them if you 're gonna continue treating this as a power spectrum . So , what {disfmarker} what Hirsch did was , uh , set them to zero {disfmarker} set the negative values to zero . So if you imagine a {disfmarker} a waveform that 's all positive , which is the time trajectory of energy , um , and , uh , shifting it downwards , and then getting rid of the negative parts , that 's essentially throwing away the low - energy things . And it 's the low - energy parts of the speech where the reverberation is most audible . You know , you have the reverberation from higher - energy things showing up in {disfmarker} So in this case you have some artificially imposed {pause} reverberation - like thing . I mean , you 're getting rid of some of the other effects of reverberation , but because you have these non - causal windows , you 're getting these funny things coming in , uh , at n And , um , what if you did {disfmarker} ? I mean , there 's nothing to say that the {disfmarker} the processing for this re - synthesis has to be restricted to trying to get it back to the original , according to some equation . I mean , you also could , uh , just try to make it nicer .\nGrad E: Uh - huh .\nProfessor B: And one of the things you could do is , you could do some sort of VAD - like thing\nGrad E: Mm - hmm .\nProfessor B: and you actually could take very low - energy sections and set them to some {disfmarker} some , uh , very low or {disfmarker} or near zero {pause} value . I mean , uh , I 'm just saying if in fact it turns out that {disfmarker} that these echoes that you 're hearing are , uh {disfmarker}\nGrad E: Uh - huh .\nProfessor B: or pre - echoes , whichever they are {disfmarker} are {disfmarker} are , uh , part of what 's causing the problem , you actually could get rid of them .\nGrad E: Uh - huh .\nProfessor B: Be pretty simple . I mean , you do it in a pretty conservative way\nGrad E: OK .\nProfessor B: so that if you made a mistake you were more likely to {pause} keep in an echo than to throw out speech .\nGrad E: Hmm .\nPhD G: Um , what is the reverberation time {pause} like {pause} there ?\nGrad E: In thi in this room ? Uh {disfmarker}\nPhD G: On , uh , the {disfmarker} the one what {disfmarker} the s in the speech that you are {disfmarker} you are using like ?\nGrad E: Y Yeah . I {disfmarker} I {disfmarker} I {disfmarker} I don't know .\nProfessor B: So , it 's this room .\nPhD G: It 's , uh {disfmarker}\nProfessor B: It 's {disfmarker} it 's this room .\nPhD G: Oh , this room ?\nProfessor B: So {disfmarker}\nPhD G: OK .\nProfessor B: so it 's {disfmarker} these are just microphone {disfmarker} this micro close microphone and a distant microphone , he 's doing these different tests on .\nGrad F: Oh .\nProfessor B: Uh , we should do a measurement in here . I g think we never have . I think it 's {disfmarker} I would guess , uh , point seven , point eight seconds f uh , R T\nGrad F: Hmm !\nProfessor B: something like that ? But it 's {disfmarker} you know , it 's this room .\nPhD G: Mm - hmm .\nProfessor B: So .\nPhD G: OK . Mm - hmm .\nProfessor B: Uh . But the other thing is , he 's putting in {disfmarker} w I was using the word \" reverberation \" in two ways . He 's also putting in , uh , a {disfmarker} he 's taking out some reverberation , but he 's putting in something , because he has {pause} averages over multiple windows stretching out to twelve seconds , which are then being subtracted from the speech . And since , you know , what you subtract , sometimes you 'll be {disfmarker} you 'll be subtracting from some larger number and sometimes you won't . And {disfmarker}\nPhD G: Mm - hmm . Mm - hmm .\nProfessor B: So you can end up with some components in it that are affected by things that are seconds away . Uh , and if it 's a low {pause} energy compo portion , you might actually hear some {pause} funny things .\nPhD G: Yeah .\nGrad E: O o one thing , um , I noticed is that , um , the mean subtraction seems to make the PZM signals louder after they 've been re - synthesized . So I was wondering , is it possible that one reason it helped with the Aurora baseline system is {pause} just as a kind of gain control ? Cuz some of the PZM signals sound pretty quiet if you don't amplify them .\nPhD C: Mm - hmm . I don't see why {disfmarker} why your signal is louder after processing , because yo\nGrad E: Yeah . I don't know why - y , uh , either .\nPhD C: Yeah .\nProfessor B: I don't think just multiplying the signal by two would have any effect .\nPhD C: Mm - hmm .\nGrad E: Oh , OK .\nProfessor B: Yeah . I mean , I think if you really have louder signals , what you mean is that you have {pause} better signal - to - noise ratio .\nPhD C: Well , well {disfmarker}\nProfessor B: So if what you 're doing is improving the signal - to - noise ratio , then it would be better .\nPhD C: Mm - hmm .\nProfessor B: But just it being bigger if {disfmarker} with the same signal - to - noise ratio {disfmarker}\nGrad E: It w i i it wouldn't affect things .\nProfessor B: No .\nPhD C: Yeah .\nGrad E: OK .\nPhD C: Well , the system is {disfmarker} use {pause} the absolute energy , so it 's a little bit dependent on {disfmarker} on the {pause} signal level . But , not so much , I guess .\nProfessor B: Well , yeah . But it 's trained and tested on the same thing .\nPhD C: Mmm .\nProfessor B: So if the {disfmarker} if the {disfmarker} if you change {vocalsound} in both training and test , the absolute level by a factor of two , it will n have no effect .\nPhD C: Mm - hmm . Yeah .\nPhD A: Did you add {pause} this data to the training set , for the Aurora ? Or you just tested on this ?\nGrad E: Uh {disfmarker} Um . Did I w what ?\nPhD A: Well , Morgan was just saying that , uh , as long as you do it in both training and testing , it shouldn't have any effect .\nGrad E: Sorry ? Yeah .\nPhD A: But I {disfmarker} I was {pause} sort of under the impression that you just tested with this data .\nGrad E: I {disfmarker} I b\nPhD A: You didn't {pause} train it also .\nGrad E: I {disfmarker} Right . I trained on clean TI - digits . I {disfmarker} I did the mean subtraction on clean TI - digits . But I didn't {disfmarker} I 'm not sure if it made the clean ti TI - digits any louder .\nProfessor B: Oh , I see .\nGrad E: I only remember noticing it made the , um , PZM signal louder .\nProfessor B: OK . Well , I don't understand then . Yeah .\nGrad E: Huh . I don't know . If it 's {disfmarker} if it 's {disfmarker} like , if it 's trying to find a {disfmarker} a reverberation filter , it could be that this reverberation filter is making things quieter . And then if you take it out {disfmarker} that taking it out makes things louder . I mean .\nProfessor B: Uh , no . I mean , {vocalsound} uh , there 's {disfmarker} there 's nothing inherent about removing {disfmarker} if you 're really removing ,\nGrad E: Nuh - huh .\nProfessor B: uh , r uh , then I don't {pause} see how that would make it louder .\nGrad E: The mean . OK . Yeah , I see .\nProfessor B: So it might be just some {disfmarker}\nGrad E: Yeah . OK . So I should maybe listen to that stuff again .\nProfessor B: Yeah . It might just be some artifact of the processing that {disfmarker} that , uh , if you 're {disfmarker} Uh , yeah . I don't know .\nGrad E: Oh . OK .\nPhD A: I wonder if there could be something like , uh {disfmarker} for s for the PZM data ,\nPhD C: Eh\nPhD A: uh , you know , if occasionally , uh , somebody hits the table or something , you could get a spike . Uh . I 'm just wondering if there 's something about the , um {disfmarker} you know , doing the mean normalization where , uh , it {disfmarker} it could cause {pause} you to have better signal - to - noise ratio . Um .\nProfessor B: Well , you know , there is this . Wait a minute . It {disfmarker} it {disfmarker} i maybe {disfmarker} i If , um {disfmarker} Subtracting the {disfmarker} the mean log spectrum is {disfmarker} is {disfmarker} is like dividing by the spectrum . So , depending what you divide by , if your {disfmarker} if s your estimate is off and sometimes you 're {disfmarker} you 're {disfmarker} you 're getting a small number , you could make it bigger .\nPhD A: Mm - hmm .\nGrad E: Mm - hmm .\nProfessor B: So , it 's {disfmarker} it 's just a {disfmarker} a question of {disfmarker} there 's {disfmarker} It {disfmarker} it could be that there 's some normalization that 's missing , or something to make it {disfmarker}\nGrad E: Mm - hmm .\nProfessor B: Uh , y you 'd think it shouldn't be larger , but maybe in practice it is . That 's something to think about .\nGrad E: Hmm .\nProfessor B: I don't know .\nPhD C: I had a question about the system {disfmarker} the SRI system . So , {vocalsound} you trained it on TI - digits ? But except this , it 's exactly the same system as the one that was tested before and that was trained on {pause} Macrophone . Right ? So on TI - digits it gives you one point two percent error rate and on Macrophone it 's still O point eight . Uh , but is it {pause} exactly the same system ?\nGrad E: Uh . I think so .\nPhD C: Hmm .\nGrad E: If you 're talking about the Macrophone results that Andreas had about , um , a week and a half ago , I think it 's the same system .\nPhD C: Mm - hmm . So you use VTL - uh , vocal tract length normalization and , um , like MLLR transformations also ,\nGrad E: Mm - hmm .\nPhD C: and {disfmarker}\nProfessor B: I 'm sorry , was his point eight percent , er , a {disfmarker} a result on testing on Macrophone or {disfmarker} or training ?\nPhD C: all that stuff .\nGrad E: That 's {disfmarker}\nPhD C: It was {pause} training on Macrophone and testing {disfmarker} yeah , on {disfmarker} on meeting digits .\nProfessor B: Oh . So that was done already . So we were {disfmarker} Uh , and it 's point eight ? OK .\nPhD C: Mm - hmm .\nProfessor B: OK .\nPhD C: Yeah . I {disfmarker} I 've just been text {comment} testing the new {pause} Aurora front - end with {disfmarker} well , Aurora system actually {disfmarker} so front - end and HTK , um , acoustic models on the meeting digits and it 's a little bit better than the previous system . We have {disfmarker} I have two point seven percent error rate . And before with the system that was proposed , it 's what ? It was three point nine . So .\nProfessor B: Oh , that 's a lot better .\nPhD C: We are getting better .\nProfessor B: So , what {disfmarker} w ?\nPhD C: And {disfmarker}\nPhD G: With the {disfmarker} with the HTK back - end ? What we have for Aurora ?\nPhD C: Yeah . Two point seven .\nPhD G: I know in the meeting , like {disfmarker}\nPhD C: On the meeting we have two point seven .\nPhD G: Right . Oh .\nGrad F: That 's with the new IIR filters ?\nPhD C: Uh . Yeah , yeah . So , yeah ,\nGrad F: OK .\nPhD C: we have {pause} the new LDA filters , and {disfmarker} I think , maybe {disfmarker} I didn't look , but one thing that makes a difference is this DC offset compensation . Uh , eh {disfmarker} Do y did you have a look at {disfmarker} at the meet uh , meeting digits , if they have a DC component , or {disfmarker} ?\nGrad E: I {disfmarker} I didn't . No .\nPhD C: Oh .\nProfessor B: Hmm .\nPhD G: No . The DC component could be negligible . I mean , if you are {pause} recording it through a mike . I mean , any {disfmarker} all of the mikes have the DC removal {disfmarker} some capacitor sitting right in {pause} that bias it .\nProfessor B: Yeah . But this {disfmarker} uh , uh , uh , no . Because , uh , there 's a sample and hold in the A - toD. And these period these typically do have a DC offset .\nPhD G: Oh , OK .\nProfessor B: And {disfmarker} and they can be surprisingly large . It depends on the electronics .\nPhD G: Oh , so it is the digital {disfmarker} OK . It 's the A - toD that introduces the DC in .\nProfessor B: Yeah . The microphone isn't gonna pass any DC .\nPhD G: Yeah . Yeah . Yeah .\nProfessor B: But {disfmarker} but ,\nPhD G: OK .\nProfessor B: typi you know , unless {disfmarker} Actually , there are {pause} instrumentation mikes that {disfmarker} that do pass {disfmarker} go down to DC . But {disfmarker} but ,\nPhD G: Mm - hmm .\nProfessor B: uh , no , it 's the electronics . And they {disfmarker} and {disfmarker}\nPhD G: Mm - hmm .\nProfessor B: then there 's amplification afterwards . And you can get , I think it was {disfmarker} I think it was in the {pause} Wall Street Journal data that {disfmarker} that {disfmarker} I can't remember , one of the DARPA things . There was this big DC - DC offset\nPhD A: Mm - hmm .\nProfessor B: we didn't {disfmarker} we didn't know about for a while , while we were {pause} messing with it . And we were getting these terrible results . And then we were talking to somebody and they said , \" Oh , yeah . Didn't you know ? Everybody knows that . There 's all this DC offset in th \" So , yes . You can have DC offset in the data .\nPhD G: Oh , OK .\nProfessor B: Yeah .\nPhD G: OK .\nPhD A: So was that {disfmarker} was that everything , Dave ?\nGrad E: Oh . And I also , um , did some experiments {pause} about normalizing the phase . Um . So I c I came up with a web page that people can take a look at . And , um , the interesting thing that I tried was , um , Adam and Morgan had this idea , um , since my original attempts to , um , take the mean of the phase spectra over time and normalize using that , by subtracting that off , didn't work . Um , so , well , that we thought that might be due to , um , problems with , um , the arithmetic of phases . They {disfmarker} they add in this modulo two pi way and , um , there 's reason to believe that that approach of taking the mean of the phase spectrum wasn't really {pause} mathematically correct . So , {vocalsound} what I did instead is I {vocalsound} took the mean of the FFT spectrum without taking the log or anything , and then I took the phase of that , and I subtracted that phase {pause} off to normalize . But that , um , didn't work either .\nProfessor B: See , we have a different interpretation of this . He says it doesn't work . I said , I think it works magnificently , but just not for the task we intended . Uh , it gets rid of the speech .\nPhD A: What does it leave ?\nGrad F: Uh , gets rid of the speech .\nProfessor B: Uh , it leaves {disfmarker} you know , it leaves the junk . I mean , I {disfmarker} I think it 's {disfmarker} it 's tremendous .\nGrad F: Oh , wow .\nProfessor B: You see , all he has to do is go back and reverse what he did before , and he 's really got something .\nPhD A: Well , could you take what was left over and then subtract that ?\nProfessor B: Ex - exactly . Yeah , you got it .\nGrad F: Yeah .\nPhD G: Yeah .\nProfessor B: So , it 's {disfmarker} it 's a general rule .\nPhD G: Oh , it 's {disfmarker}\nProfessor B: Just listen very carefully to what I say and do the opposite . Including what I just said .\nGrad E: And , yeah , that 's everything .\nPhD A: All set ? Do you want to go , Stephane ?\nPhD C: Um . Yeah . Maybe , concerning these d still , these meeting digits . I 'm more interested in trying to figure out what 's still the difference between the SRI system and the Aurora system . And {disfmarker} Um . Yeah . So , I think I will maybe train , like , gender - dependent models , because {pause} this is also one big difference between {pause} the two systems . Um , the other differences were {pause} the fact that maybe the acoustic models of the SRI are more {disfmarker} SRI system are more complex . But , uh , Chuck , you did some experiments with this and\nPhD A: It didn't seem to help in the HTK system .\nPhD C: it was hard t to {disfmarker} to have some exper some improvement with this . Um .\nProfessor B: Well , it sounds like they also have {disfmarker} he {disfmarker} he 's saying they have all these , uh , uh , different kinds of adaptation .\nPhD C: Mm - hmm .\nProfessor B: You know , they have channel adaptation . They have speaker adaptation .\nPhD C: Yeah . Right .\nPhD A: Well , there 's also the normalization .\nProfessor B: Yeah . Yeah .\nPhD C: Yeah .\nGrad F: Yeah .\nPhD A: Like they do , um {disfmarker} I 'm not sure how they would do it when they 're working with the digits ,\nPhD C: The vocal tr\nPhD A: but , like , in the Switchboard data , there 's , um {disfmarker} conversation - side normalization for the {pause} non - C - zero components ,\nPhD C: Yeah . Yeah . This is another difference . Their normalization works like on {disfmarker} on the utterance levels .\nPhD A: Mm - hmm .\nPhD C: But we have to do it {disfmarker} We have a system that does it on - line .\nPhD A: Right .\nPhD C: So , it might be {disfmarker} it might be better with {disfmarker} it might be worse if the {pause} channel is constant ,\nPhD A: Yeah .\nPhD C: or {disfmarker} Nnn .\nPhD G: And the acoustic models are like - k triphone models or {disfmarker} or is it the whole word ?\nPhD C: SRI {disfmarker} it 's {disfmarker} it 's tr\nGrad F: SRI .\nPhD G: Yeah .\nPhD C: Yeah . I guess it 's triphones .\nPhD G: It 's triphone .\nProfessor B: I think it 's probably more than that .\nPhD C: Huh .\nProfessor B: I mean , so they {disfmarker} they have {disfmarker} I {disfmarker} I thin think they use these , uh , uh , genone things . So there 's {disfmarker} there 's these kind of , uh , uh , pooled models and {disfmarker} and they can go out to all sorts of dependencies .\nPhD G: Oh . It 's like the tied state .\nProfessor B: So .\nPhD A: Mm - hmm .\nProfessor B: They have tied states and I think {disfmarker} I {disfmarker} I {disfmarker} I don't real I 'm talk I 'm just guessing here . But I think {disfmarker} I think they {disfmarker} they don't just have triphones .\nPhD G: OK .\nProfessor B: I think they have a range of {disfmarker} of , uh , dependencies .\nPhD C: Mm - hmm .\nPhD G: Mm - hmm .\nPhD C: Mm - hmm .\nGrad F: Hmm .\nPhD C: And {disfmarker} Yeah . Well . Um . Well , the first thing I {disfmarker} that I want to do is just maybe these gender things . Uh . And maybe see with {pause} Andreas if {disfmarker} Well , I {disfmarker} I don't know {pause} how much it helps , what 's the model .\nPhD A: So {disfmarker} so the n stuff on the numbers you got , the two point seven , is that using the same training data that the SRI system used and got one point two ?\nPhD C: That 's right . So it 's the clean {pause} TI - digits training set .\nPhD A: So exact same training data ?\nPhD C: Right .\nPhD A: OK .\nPhD C: Mm - hmm . I guess you used the clean training set .\nGrad E: Right .\nPhD C: Mm - hmm .\nGrad E: For {disfmarker} with the SRI system {disfmarker}\nPhD C: Well .\nGrad E: You know , the {disfmarker} the Aurora baseline is set up with these , um {disfmarker} {vocalsound} this version of the clean training set that 's been filtered with this G - seven - one - two filter , and , um , to train the SRI system on digits S - Andreas used the original TI - digits , um , under U doctor - speech data TI - digits , which don't have this filter . But I don't think there 's any other difference .\nPhD C: Mm - hmm . Mm - hmm . Yeah .\nProfessor B: So is that {disfmarker} ? Uh , are {disfmarker} are these results comparable ? So you {disfmarker} you were getting with the , uh , Aurora baseline something like two point four percent {pause} on clean TI - digits , when , uh , training the SRI system with clean TR digits {disfmarker} {comment} TI - digits . Right ? And {disfmarker}\nGrad E: Um . Uh - huh .\nProfessor B: Yeah . And , so , is your two point seven comparable , where you 're , uh , uh , using , uh , the submitted system ?\nPhD C: Yeah . I think so .\nProfessor B: OK .\nPhD C: Yeah .\nProfessor B: So it 's {pause} about the same ,\nPhD C: Mm - hmm .\nProfessor B: maybe a little worse .\nGrad E: W w it was one {disfmarker} one point two\nPhD C: Ye\nGrad E: with the SRI system ,\nProfessor B: I 'm sorry .\nPhD C: Yeah .\nGrad E: I {disfmarker}\nPhD C: The complete SRI system is one point two .\nProfessor B: You {disfmarker} you were HTK .\nPhD C: Yeah .\nProfessor B: Right ? OK . That 's right . So {disfmarker}\nPhD C: Mm - hmm .\nProfessor B: OK , so {pause} the comparable number then , uh {pause} for what you were talking about then , since it was HTK , would be the {pause} um , two point f\nPhD C: It was four point something . Right ? The HTK system with , uh , b\nGrad E: D d\nProfessor B: Oh , right , right , right , right .\nPhD C: MFCC features {disfmarker}\nGrad E: Do you mean the b ? The baseline Aurora - two system , trained on TI - digits , tested on Meeting Recorder near , I think we saw in it today , and it was about six point six percent .\nProfessor B: Right . Right , right , right .\nPhD C: Oh .\nProfessor B: OK . Alright . So {disfmarker} He 's doing some {pause} different things .\nPhD C: So {disfmarker} Yeah . The only difference is the features , right now , between this and {disfmarker}\nProfessor B: Yes . OK , good . So they are helping .\nPhD C: Mm - hmm .\nProfessor B: That 's good to hear . Yeah .\nPhD C: They are helping . Yeah . Um . Yeah . And another thing I {disfmarker} I maybe would like to do is to {pause} just test the SRI system that 's trained on Macrophone {disfmarker} test it on , uh , the noisy TI - digits ,\nProfessor B: Yeah .\nPhD C: cuz I 'm still wondering {pause} where this {pause} improvement comes from . When you train on Macrophone , it seems better on meeting digits . But I wonder if it 's just because maybe {pause} Macrophone is acoustically closer to the meeting digits than {disfmarker} than TI - digit is , which is {disfmarker} TI - digits are very {pause} clean recorded digits\nProfessor B: Mm - hmm .\nPhD C: and {disfmarker}\nPhD A: You know , it would also be interesting to see , uh {disfmarker} to do the regular Aurora test ,\nPhD C: Uh , f s\nPhD A: um , but use the SRI system instead of HTK .\nPhD C: That 's {disfmarker} Yeah . That 's what {pause} I wanted , just , uh {disfmarker} Yeah . So , just using the SRI system , test it on {disfmarker} and test it on {pause} Aurora TI - digits . Right .\nPhD A: Why not the full Aurora , uh , test ?\nPhD C: Um . Yeah . There is this problem of multilinguality yet .\nPhD A: Mm - hmm .\nPhD C: So we don't {disfmarker}\nProfessor B: You 'd have to train the SRI system with {disfmarker} with all the different languages .\nPhD C: i i\nPhD A: Right .\nPhD C: We would have to train on {disfmarker}\nPhD A: Yeah . That 's what I mean .\nPhD C: Yeah .\nPhD A: So , like , comple\nProfessor B: It 'd be a {pause} lot of work . That 's the only thing .\nPhD C: Yeah .\nPhD A: Mmm .\nPhD C: It 's {disfmarker}\nPhD A: Well , I mean ,\nPhD C: Mmm .\nPhD A: uh , uh , I guess the work would be into getting the {disfmarker} the files in the right formats , or something . Right ? I mean {disfmarker}\nPhD C: Mm - hmm .\nPhD A: Because when you train up the Aurora system , you 're , uh {disfmarker} you 're also training on all the data .\nPhD C: That 's right .\nPhD A: I mean , it 's {disfmarker}\nPhD C: Yeah . Yeah . I see . Oh , so , OK . Right . I see what you mean .\nProfessor B: That 's true , but I think that also when we 've had these meetings week after week , oftentimes people have not done the full arrange of things\nPhD A: Mm - hmm .\nProfessor B: because {disfmarker} on {disfmarker} on whatever it is they 're trying , because it 's a lot of work , even just with the HTK .\nPhD A: Mm - hmm .\nProfessor B: So , it 's {disfmarker} it 's a good idea , but it seems like {pause} it makes sense to do some pruning\nPhD A: Mm - hmm .\nProfessor B: first with a {disfmarker} a test or two that makes sense for you ,\nPhD A: Yeah .\nProfessor B: and then {pause} take the likely candidates and go further .\nPhD A: Yeah .\nPhD C: Mm - hmm . Yeah . But , just testing on TI - digits would already give us some information {pause} about what 's going on . And {disfmarker} mm - hmm . Uh , yeah . OK . Uh , the next thing is this {disfmarker} this VAD problem that , um , um {disfmarker} So , I 'm just talking about the {disfmarker} the curves that I {disfmarker} I sent {disfmarker} {vocalsound} I sent you {disfmarker} so , whi that shows that {vocalsound} when the SNR decrease , {vocalsound} uh , the current {pause} VAD approach doesn't drop much frames {pause} for some particular noises , uh , which might be then noises that are closer to speech , uh , acoustically .\nProfessor B: I i Just to clarify something for me . I They were supp Supposedly , in the next evaluation , they 're going to be supplying us with boundaries .\nPhD C: Mm - hmm .\nProfessor B: So does any of this matter ? I mean , other than our interest in it . Uh {disfmarker}\nPhD C: Uh {disfmarker} Well . First of all , the boundaries might be , uh {disfmarker} like we would have t two hundred milliseconds or {disfmarker} before and after speech . Uh . So removing more than that might still make {pause} a difference {pause} in the results .\nProfessor B: Do we {disfmarker} ? I mean , is there some reason that we think that 's the case ?\nPhD C: And {disfmarker} No . Because we don't {disfmarker} didn't looked {pause} that much at that .\nProfessor B: Yeah .\nPhD C: But , {vocalsound} still , I think it 's an interesting problem .\nProfessor B: Oh , yeah .\nPhD C: And {disfmarker} Um . Yeah .\nProfessor B: But maybe we 'll get some insight on that when {disfmarker} when , uh , the gang gets back from Crete . Because {pause} there 's lots of interesting problems , of course .\nPhD C: Mm - hmm .\nProfessor B: And then the thing is if {disfmarker} if they really are going to have some means of giving us {pause} fairly tight , uh , boundaries , then that won't be so much the issue .\nPhD C: Yeah , yeah . Mm - hmm . Mm - hmm .\nProfessor B: Um But {vocalsound} I don't know .\nPhD G: Because w we were wondering whether that {pause} VAD is going to be , like , a realistic one or is it going to be some manual segmentation . And then , like , if {disfmarker} if that VAD is going to be a realistic one , then we can actually use their markers to shift the point around , I mean , the way we want\nProfessor B: Mm - hmm .\nPhD G: to find a {disfmarker} I mean , rather than keeping the twenty frames , we can actually move the marker to a point which we find more {pause} suitable for us .\nProfessor B: Right .\nPhD G: But if that is going to be something like a manual , uh , segmenter , then we can't {pause} use that information anymore ,\nPhD C: Mm - hmm .\nPhD G: because that 's not going to be the one that is used in the final evaluation .\nProfessor B: Right .\nPhD G: So . We don't know what is the type of {pause} {vocalsound} {pause} VAD which they 're going to provide .\nProfessor B: Yeah .\nPhD C: Yeah . And actually there 's {disfmarker} Yeah . There 's an {disfmarker} uh , I think it 's still for {disfmarker} even for the evaluation , uh , it might still be interesting to {vocalsound} work on this because {pause} the boundaries apparently that they would provide is just , {vocalsound} um , starting of speech and end of speech {pause} uh , at the utterance level . And {disfmarker} Um .\nPhD G: With some {disfmarker} some gap .\nPhD C: So {disfmarker}\nPhD G: I mean , with some pauses in the center , provided they meet that {disfmarker} whatever the hang - over time which they are talking .\nPhD C: Yeah . But when you have like , uh , five or six frames , both {disfmarker}\nPhD G: Yeah . Then the they will just fill {disfmarker} fill it up .\nPhD C: it {disfmarker} it {disfmarker} with {disfmarker}\nPhD G: I mean , th {disfmarker} Yeah .\nPhD C: Yeah .\nProfessor B: So if you could get at some of that , uh {disfmarker}\nPhD C: So {disfmarker}\nProfessor B: although that 'd be hard .\nPhD C: Yeah . It might be useful for , like , noise estimation , and a lot of other {pause} things that we want to work on .\nProfessor B: But {disfmarker} but {disfmarker} Yeah .\nPhD G: Yeah .\nProfessor B: Right . OK .\nPhD C: But {disfmarker} Mmm . Yeah . So I did {disfmarker} I just {pause} started to test {pause} putting together two VAD which was {disfmarker} was not much work actually . Um , I im re - implemented a VAD that 's very close to the , {vocalsound} um , energy - based VAD {vocalsound} that , uh , the other Aurora guys use . Um . So , which is just putting a threshold on {pause} the noise energy ,\nProfessor B: Mm - hmm .\nPhD C: and , detect detecting the first {pause} group of four frames {pause} that have a energy that 's above this threshold , and , uh , from this point , uh , tagging the frames there as speech . So it removes {vocalsound} the first silent portion {disfmarker} portion of each utterance . And it really removes it , um , still o on the noises where {pause} our MLP VAD doesn't {pause} work a lot .\nProfessor B: Mmm .\nPhD C: Uh ,\nProfessor B: Cuz I would have thought that having some kind of spectral {pause} information ,\nPhD C: and {disfmarker}\nProfessor B: uh {disfmarker} uh , you know , in the old days people would use energy and zero crossings , for instance {disfmarker} uh , would give you some {pause} better performance . Right ? Cuz you might have low - energy fricatives or {disfmarker} or , uh {pause} stop consonants , or something like that .\nPhD C: Mm - hmm .\nProfessor B: Uh .\nPhD C: Yeah . So , your point is {disfmarker} will be to u use whatever {disfmarker}\nProfessor B: Oh , that if you d if you use purely energy and don't look at anything spectral , then you don't have a good way of distinguishing between low - energy speech components and {pause} nonspeech . And , um ,\nPhD C: Mm - hmm .\nProfessor B: just as a gross generalization , most nonsp many nonspeech noises have a low - pass kind of characteristic , some sort of slope . And {disfmarker} and most , um , low - energy speech components that are unvoiced have a {disfmarker} a high - pass kind of characteristic {disfmarker}\nPhD C: Mm - hmm .\nProfessor B: an upward slope . So having some kind of a {disfmarker}\nPhD C: Yeah .\nProfessor B: uh , you know , at the beginning of a {disfmarker} of a {disfmarker} of an S sound for instance , just starting in , it might be pretty low - energy ,\nPhD C: Mm - hmm .\nProfessor B: but it will tend to have this high - frequency component . Whereas , {vocalsound} a {disfmarker} a lot of rumble , and background noises , and so forth will be predominantly low - frequency . Uh , you know , by itself it 's not enough to tell you , but it plus energy is sort of {disfmarker}\nPhD C: Yeah .\nProfessor B: it plus energy plus timing information is sort of {disfmarker}\nPhD C: Mm - hmm .\nProfessor B: I mean , if you look up in Rabiner and Schafer from like twenty - five years ago or something , that 's sort of {pause} what they were using then .\nPhD C: Mm - hmm .\nProfessor B: So it 's {disfmarker} it 's not a {disfmarker}\nPhD C: Mm - hmm .\nGrad F: Hmm .\nPhD C: So , yeah . It {disfmarker} it might be that what I did is {disfmarker} so , removes like {vocalsound} low , um , {vocalsound} uh {disfmarker} low - energy , uh , speech frames . Because {pause} the way I do it is I just {disfmarker} I just combine the two decisions {disfmarker} so , the one from the MLP and the one from the energy - based {disfmarker} with the {disfmarker} with the and {pause} operator . So , I only {pause} keep the frames where the two agree {pause} that it 's speech . So if the energy - based dropped {disfmarker} dropped low - energy speech , mmm , they {disfmarker} they are {disfmarker} they are lost . Mmm .\nProfessor B: Mm - hmm .\nPhD C: But s still , the way it 's done right now it {disfmarker} it helps on {disfmarker} on the noises where {disfmarker} it seems to help on the noises where {vocalsound} our VAD was not very {pause} good .\nProfessor B: Well , I guess {disfmarker} I mean , one could imagine combining them in different ways . But {disfmarker} but , I guess what you 're saying is that the {disfmarker} the MLP - based one has the spectral information . So .\nPhD C: Yeah . But {disfmarker} Yeah . But the way it 's combined wi is maybe done {disfmarker} Well , yeah .\nProfessor B: Well , you can imagine {disfmarker}\nPhD C: The way I use a an a \" AND \" operator is {disfmarker} So , it {disfmarker} I , uh {disfmarker}\nProfessor B: Is {disfmarker} ?\nPhD C: The frames that are dropped by the energy - based system are {disfmarker} are , uh , dropped , even if the , um , MLP decides to keep them .\nProfessor B: Right . Right . And that might not be optimal ,\nPhD C: But , yeah .\nProfessor B: but {disfmarker}\nPhD C: Mm - hmm .\nPhD A: No\nProfessor B: but {disfmarker} I mean , I guess in principle what you 'd want to do is have a {disfmarker} {vocalsound} uh , a probability estimated by each one and {disfmarker} and put them together .\nPhD C: Yeah . Mmm . M Yeah .\nPhD A: Something that {disfmarker} that I 've used in the past is , um {disfmarker} when just looking at the energy , is to look at the derivative . And you {pause} make your decision when the derivative is increasing for {pause} so many frames . Then you say that 's beginning of speech .\nPhD C: Uh - huh .\nPhD A: But , I 'm {disfmarker} I 'm trying to remember if that requires that you keep some amount of speech in a buffer . I guess it depends on how you do it . But {pause} I mean , that 's {disfmarker} that 's been a useful thing .\nProfessor B: Yeah .\nPhD C: Mm - hmm .\nGrad F: Mm - hmm .\nPhD G: Yeah . Well , every everywhere has a delay associated with it . I mean , you still have to k always keep a buffer ,\nPhD A: Mm - hmm .\nPhD G: then only make a decision because {pause} you still need to smooth the {pause} decision further .\nPhD A: Right . Right .\nPhD G: So that 's always there .\nPhD A: Yeah . OK .\nPhD C: Well , actually if I don't {disfmarker} maybe don't want to work too much of {disfmarker} on it right now . I just wanted to {disfmarker} to see if it 's {disfmarker} {vocalsound} what I observed was the re was caused by this {disfmarker} this VAD problem .\nProfessor B: Mm - hmm .\nPhD C: And it seems to be the case . Um . Uh , the second thing is the {disfmarker} this spectral subtraction . Um . Um , which I 've just started yesterday to launch a bunch of , uh , {nonvocalsound} twenty - five experiments , uh , with different , uh , values for the parameters that are used . So , it 's the Makhoul - type spectral subtraction which use {pause} an over - estimation factor . So , we substr I subtract more , {vocalsound} {vocalsound} um , {nonvocalsound} {vocalsound} noise than the noise spectra that {pause} is estimated {pause} on the noise portion of the s uh , the utterances . So I tried several , uh , over - estimation factors . And after subtraction , I also add {pause} a constant noise , and I also try different , uh , {vocalsound} noise , uh , values and we 'll see what happen .\nProfessor B: Hmm . OK .\nPhD C: Mm - hmm . Mm - hmm . But st still when we look at the , um {disfmarker} Well , it depends on the parameters that you use , but for moderate over - estimation factors and moderate noise level that you add , you st have a lot of musical noise . Um . On the other hand , when you {pause} subtract more and when you add more noise , you get rid of this musical noise but {pause} maybe you distort a lot of speech . So . Well . Mmm . Well , it {disfmarker} until now , it doesn't seem to help . But We 'll see . So the next thing , maybe I {disfmarker} what I will {pause} try to {disfmarker} to do is just {pause} to try to smooth mmm , {vocalsound} the , um {disfmarker} to smooth the d the result of the subtraction , to get rid of the musical noise , using some kind of filter , or {disfmarker}\nPhD G: Can smooth the SNR estimate , also .\nPhD C: Yeah . Right . Mmm .\nPhD G: Your filter is a function of SNR . Hmm ?\nPhD C: Yeah . So , to get something that 's {disfmarker} would be closer to {pause} what you tried to do with Wiener filtering .\nPhD G: Yeah .\nPhD C: And {disfmarker} Mm - hmm . Yeah .\nPhD G: Actually , it 's , uh {disfmarker} Uh . I don't know , it 's {disfmarker} go ahead .\nPhD C: It {disfmarker}\nPhD G: And it 's {disfmarker}\nPhD C: Maybe you can {disfmarker}\nPhD G: go ahead .\nPhD C: I think it 's {disfmarker} That 's it for me .\nPhD G: OK . So , uh {disfmarker} u th I 've been playing with this Wiener filter , like . And there are {disfmarker} there were some bugs in the program , so I was p initially trying to clear them up . Because one of the bug was {disfmarker} I was assuming that always the VAD {disfmarker} uh , the initial frames were silence . It always started in the silence state , but it wasn't for some utterances . So the {disfmarker} it wasn't estimating the noise initially , and then it never estimated , because I assumed that it was always silence .\nPhD C: Mm - hmm . So this is on SpeechDat - Car Italian ?\nPhD G: Yeah .\nPhD C: So , in some cases s there are also {disfmarker}\nPhD G: SpeechDat - Car Italian . Yeah . There 're a few cases , actually , which I found later , that there are .\nPhD C: o Uh - huh .\nPhD G: So that was one of the {pause} bugs that was there in estimating the noise . And , uh , so once it was cleared , uh , I ran a few experiments with {pause} different ways of smoothing the estimated clean speech and how t estimated the noise and , eh , smoothing the SNR also . And so the {disfmarker} the trend seems to be like , {vocalsound} uh , smoothing the {pause} current estimate of the clean speech for deriving the SNR , which is like {pause} deriving the Wiener filter , seems to be helping . Then updating it quite fast using a very small time constant . So we 'll have , like , a few results where the {disfmarker} estimating the {disfmarker} the {disfmarker} More smoothing is helping . But still it 's like {disfmarker} it 's still comparable to the baseline . I haven't got anything beyond the baseline . But that 's , like , not using any Wiener filter . And , uh , so I 'm {disfmarker} I 'm trying a few more experiments with different time constants for smoothing the noise spectrum , and smoothing the clean speech , and smoothing SNR . So there are three time constants that I have . So , I 'm just playing around . So , one is fixed in the line , like {pause} Smoothing the clean speech is {disfmarker} is helping , so I 'm not going to change it that much . But , the way I 'm estimating the noise and the way I 'm estimating the SNR , I 'm just trying {disfmarker} trying a little bit . So , that h And the other thing is , like , putting a floor on the , uh , SNR , because that {disfmarker} if some {disfmarker} In some cases the clean speech is , like {disfmarker} when it 's estimated , it goes to very low values , so the SNR is , like , very low . And so that actually creates a lot of variance in the low - energy region of the speech . So , I 'm thinking of , like , putting a floor also for the SNR so that it doesn't {pause} vary a lot in the low - energy regions . And , uh . So . The results are , like {disfmarker} So far I 've been testing only with the {pause} baseline , which is {disfmarker} which doesn't have any LDA filtering and on - line normalization . I just want to separate the {disfmarker} the contributions out . So it 's just VAD , plus the Wiener filter , plus the baseline system , which is , uh , just the spectral {disfmarker} I mean , the mel sp mel , uh , frequency coefficients . Um . And the other thing that I tried was {disfmarker} but I just {vocalsound} took of those , uh , {pause} {vocalsound} Carlos filters , which Hynek had , to see whether it really h helps or not . I mean , it was just a {disfmarker} a run to see whether it really degrades or it helps . And it 's {disfmarker} it seems to be like it 's not {vocalsound} hurting a lot by just blindly picking up one filter which is nothing but a {pause} four hertz {disfmarker} a band - pass m m filter on the cubic root of the power spectrum . So , that was the filter that Hy - uh , Carlos had . And so {disfmarker} Yeah . Just {disfmarker} just to see whether it really {disfmarker} it 's {disfmarker} it 's {disfmarker} is it worth trying or not . So , it doesn't seems to be degrading a lot on that . So there must be something that I can {disfmarker} that can be done with that type of noise compensation also , which {disfmarker} {vocalsound} I guess I would ask Carlos about that . I mean , how {disfmarker} how he derived those filters and {disfmarker} and where d if he has any filters which are derived on OGI stories , added with some type of noise which {disfmarker} what we are using currently , or something like that . So maybe I 'll {disfmarker}\nProfessor B: This is cubic root of power spectra ?\nPhD G: Yeah . Cubic root of power spectrum .\nProfessor B: So , if you have this band - pass filter , you probably get n you get negative values . Right ?\nPhD G: Yeah . And I 'm , like , floating it to z zeros right now .\nProfessor B: OK .\nPhD G: So it has , like {disfmarker} the spectrogram has , like {disfmarker} Uh , it actually , uh , enhances the onset and offset of {disfmarker} I mean , the {disfmarker} the begin and the end of the speech . So it 's {disfmarker} there seems to be , like , deep valleys in the begin and the end of , like , high - energy regions ,\nProfessor B: Mm - hmm .\nPhD G: because the filter has , like , a sort of Mexican - hat type structure .\nProfessor B: Mm - hmm .\nPhD G: So , those are the regions where there are , like {disfmarker} when I look at the spectrogram , there are those deep valleys on the begin and the end of the speech . But the rest of it seems to be , like , pretty nice .\nProfessor B: Mm - hmm .\nPhD G: So . That 's {pause} something I observe using that filter . And {disfmarker} Yeah . There are a few {disfmarker} very {disfmarker} not a lot of {disfmarker} because the filter doesn't have a {disfmarker} really a deep negative portion , so that it 's not really creating a lot of negative values in the cubic root . So , I 'll {disfmarker} I 'll s may continue with that for some w I 'll {disfmarker} I 'll {disfmarker} Maybe I 'll ask Carlos a little more about how to play with those filters , and {disfmarker} but while {pause} making this Wiener filter better . So . Yeah . That {disfmarker} that 's it , Morgan .\nProfessor B: Uh , last week you were also talking about building up the subspace {pause} stuff ?\nPhD G: Yeah . I {disfmarker} I {disfmarker} I would actually m m didn't get enough time to work on the subspace last week . It was mostly about {pause} finding those bugs and\nProfessor B: OK .\nPhD G: th you know , things , and I didn't work much on that .\nPhD A: How about you , Carmen ?\nPhD D: Well , I am still working with , eh , VTS . And , one of the things that last week , eh , say here is that maybe the problem was with the diff because the signal have different level of energy .\nProfessor B: Hmm ?\nPhD D: And , maybe , talking with Stephane and with Sunil , we decide that maybe it was interesting to {disfmarker} to apply on - line normalization before applying VTS . But then {vocalsound} we decided that that 's {disfmarker} it doesn't work absolutely , because we modified also the noise . And {disfmarker} Well , thinking about that , we {disfmarker} we then {disfmarker} we decide that maybe is a good idea . We don't know . I don't hav I don't {disfmarker} this is {disfmarker} I didn't {pause} do the experiment yet {disfmarker} to apply VTS in cepstral domain .\nProfessor B: The other thing {pause} is {disfmarker} So {disfmarker} so , in {disfmarker} i i and {disfmarker} Not {disfmarker} and C - zero would be a different {disfmarker} So you could do a different normalization for C - zero than for other things anyway . I mean , the other thing I was gonna suggest is that you could have {pause} two kinds of normalization with {disfmarker} with , uh , different time constants . So , uh , you could do some normalization {vocalsound} s uh , before the VTS , and then do some other normalization after . I don't know . But {disfmarker} but C - zero certainly acts differently than the others do ,\nPhD D: Uh .\nProfessor B: so that 's {disfmarker}\nPhD C: Mm - hmm .\nPhD D: Well , we s decide to m to {disfmarker} to obtain the new expression if we work in the cepstral domain . And {disfmarker} Well . I am working in that now ,\nProfessor B: Uh - huh .\nPhD D: but {vocalsound} I 'm not sure if that will be usefu useful . I don't know . It 's k it 's k It 's quite a lot {disfmarker} It 's a lot of work .\nProfessor B: Uh - huh .\nPhD D: Well , it 's not too much , but this {disfmarker} it 's work .\nProfessor B: Yeah .\nPhD D: And I want to know if {disfmarker} if we have some {pause} feeling that {pause} the result {disfmarker} I {disfmarker} I would like to know if {disfmarker} I don't have any feeling if this will work better than apply VTS aft in cepstral domain will work better than apply in m mel {disfmarker} in filter bank domain . I r I 'm not sure . I don't {disfmarker} I don't know absolutely nothing .\nPhD C: Mm - hmm .\nProfessor B: Yeah . Well , you 're {disfmarker} I think you 're the first one here to work with VTS , so , uh , maybe we could call someone else up who has , ask them their opinion . Uh ,\nPhD C: Mm - hmm .\nProfessor B: I don't {disfmarker} I don't have a good feeling for it . Um .\nPhD G: Pratibha .\nPhD C: Actually , the VTS that you tested before was in the log domain and so {pause} the codebook is e e kind of dependent on the {pause} level of the speech signal .\nPhD D: Yeah ?\nPhD C: And {disfmarker} So I expect it {disfmarker} If {disfmarker} if you have something that 's independent of this , I expect it to {disfmarker} it {disfmarker} to , uh , be a better model of speech .\nPhD D: To have better {disfmarker}\nPhD C: And . Well .\nProfessor B: You {disfmarker} you wouldn't even need to switch to cepstra . Right ? I mean , you can just sort of normalize the {disfmarker}\nPhD C: No . We could normali norm I mean , remove the median .\nProfessor B: Yeah . Yeah . And then you have {pause} one number which is very dependent on the level cuz it is the level ,\nPhD D: Mm - hmm .\nProfessor B: and the other which isn't .\nPhD C: Mm - hmm . Yeah . But here also we would have to be careful about removing the mean {pause} of speech and not of noise .\nPhD D: Ye\nPhD C: Because it 's like {pause} first doing general normalization\nPhD D: Yea\nPhD C: and then noise removal , which is {disfmarker}\nPhD D: Yeah . We {disfmarker} I was thinking to {disfmarker} to {disfmarker} to estimate the noise {pause} with the first frames and then apply the VAD ,\nProfessor B: Mm - hmm .\nPhD C: Mm - hmm .\nPhD D: before the on - line normalization .\nPhD C: Mm - hmm .\nPhD D: We {disfmarker} we see {disfmarker} Well , I am thinking {vocalsound} about that and working about that ,\nProfessor B: Yeah .\nPhD D: but I don't have result this week .\nProfessor B: Sure . I mean , one of the things we 've talked about {disfmarker} maybe it might be star time to start thinking about pretty soon , is as we look at the pros and cons of these different methods , how do they fit in with one another ? Because {pause} we 've talked about potentially doing some combination of a couple of them . Maybe {disfmarker} maybe pretty soon we 'll have some sense of what their {pause} characteristics are ,\nPhD D: Mm - hmm .\nProfessor B: so we can see what should be combined .\nPhD C: Mm - hmm .\nPhD A: Is that it ? OK ?\nProfessor B: OK . Why don't we read some digits ?\nPhD A: Yep . Want to go ahead , Morgan ?\nProfessor B: Sure .\nPhD A: Transcript L dash two one five .\nProfessor B: O K .", "source": "meeting_summ", "evaluation": "human", "instructions #0": "Summarize the discussion on TORRENT schedule and intermediate categorization", "instructions #1": "What did Grad F say about his proposal?", "instructions #2": "What did Grad F think about intermediate categories?", "instructions #3": "Summarize the discussion on mean subtraction in SRI", "instructions #4": "What did the professor think about echoes and reverberation?", "instructions #5": "What did PhD C think about the signal to noise ratio?", "instructions #6": "Summarize the meeting", "outputs #0": "The professor told the team that the TORRENT chip schedule kept getting pushed. Then, Grad F talked about his proposal, in which he was done with the section on intermediate categories. Including features from intermediate categories was a potential way of reducing error.", "outputs #1": "Grad F explained that he was focusing on writing his proposal for his qualification exams, which was on the 25th of July. He had to write a paper and pass it around before that date.", "outputs #2": "Grad F informed the team that he was building a system that classified intermediate categories with multi-band techniques. Then, to reduce error for phoneme recognition, the intermediate categories could be added to improve performance. The method could be replicated for large vocabulary tasks like switchboard.", "outputs #3": "The team got an improvement on the SRI system for TI- digits and Meeting Recorder digits but near mic performance worsened. The team explored the reasons for this difference. The professor suggested getting rid of low energy sections. The team also discussed how more nuanced normalization approaches could improve task performance.", "outputs #4": "The professor thought it was possible to reduce the effects of reverberation by removing the low-energy segments. He thought a VAD-like approach would work. This would make it so that the model was more likely to keep an echo than throw out speech.", "outputs #5": "PhD C was skeptical of why the signal was louder after processing. PhD C suggested that the system is not too dependent on the signal level, agreeing with the professor that improvement in the model was more likely dependent on the ratio.", "outputs #6": "The meeting began with a discussion on the TORRENT project completion being pushed for two years. Grad F then introduced intermediate categorization, which was his topic for his qualification exams. The team then discussed mean subtraction from SRI. Using it had led to an improvement in Meeting Recorder digits though near mic performance worsened. The professor points to pre-echoes as the culprit. The team continued to study differences between SRI and Aurora. The team thought it would be interesting to do the Aurora tests with the SRI system instead of the HTK. The team was also exploring the Wiener filter and VTS. The professor did not seem too excited about the VTS.", "gpt4_outputs #0": "The discussion on TORRENT schedule mainly revolved around how the project was consistently two months away from completion for two years. The participants also discussed intermediate categorization in the context of Grad F's proposal, where he mentioned classifying intermediate categories as a middle step.\n\n ---------------------- Your Score (1~5) ----------------------\n", "gpt4_outputs #1": "Grad F mentioned that he had been working on writing his proposal and had just finished a section about intermediate categories. He hoped to complete a full rough draft by Monday and submit it to Morgan. The proposal is for his quals, which are scheduled to take place on July 25th.\n\n ---------------------- Your Score (1~5) ----------------------\n", "gpt4_outputs #2": "Grad F thought that instead of coming up with a complete set of intermediate features right away, he would start with a reasonable set, such as regular phonetic features, and build a system that classifies these using multi-band techniques. He would then analyze the errors produced and modify the intermediate categories accordingly in an iterative process.\n\n ---------------------- Your Score (1~5) ----------------------\n", "gpt4_outputs #3": "The discussion on mean subtraction in SRI focused on the improvement in word error rate when using the TI-digits dataset and testing on Meeting Recorder digits. They observed an improvement in far-mike data but a worsening in near-mike performance. They also discussed the possible reasons for this, such as channel adaptation, finer-grained acoustic models, and the presence of artifacts in the re-synthesized audio.\n\n ---------------------- Your Score (1~5) ----------------------\n", "gpt4_outputs #4": "The professor thought that echoes and reverberation could cause problems in speech recognition, particularly in low-energy parts of the speech where reverberation is most audible.\n\n ---------------------- Your Score (1~5) ----------------------\n", "gpt4_outputs #5": "PhD C believed that the signal-to-noise ratio is important in speech recognition, and that simply making the signal louder without improving the signal-to-noise ratio wouldn't be beneficial.\n\n ---------------------- Your Score (1~5) ----------------------\n", "gpt4_outputs #6": "The meeting involved discussing various research projects and experiments related to speech recognition, specifically focusing on techniques such as TORRENT chip, Aurora, VAD, VTS, and Wiener filters. They discussed potential improvements to these techniques, as well as the challenges faced in obtaining better results. The participants also discussed the importance of signal-to-noise ratio, echoes, and reverberation in speech recognition.\n\n ---------------------- Your Score (1~5) ----------------------\n", "claude_outputs #0": "Prof B mentioned that for two years during the TORRENT chip project, they were two months away from being done. However, this was only \"believable schedules\" and not actually achieved.\n ---------------------- Your Score (1~5) ----------------------\n", "claude_outputs #1": "Grad F said that he has been working on writing his proposal. He finished a section discussing the intermediate categories he wants to classify. He hopes to complete a full rough draft by Monday to give to Morgan. His quals are on July 25. The proposal will be distributed to evaluators beforehand and he will give a short presentation and then answer questions.\n ---------------------- Your Score (1~5) ----------------------\n", "claude_outputs #2": "Grad F initially thought he had to come up with a complete set of intermediate features and categories from the beginning. But now he thinks he will start with a reasonable set of regular phonetic features and build a system to classify the intermediate categories. He will then look at the errors produced and add or remove intermediate categories to improve performance.\n ---------------------- Your Score (1~5) ----------------------\n", "claude_outputs #3": "There was a discussion about why Grad E's signal was louder after mean subtraction processing. There were various theories but no definite conclusion. Prof B pointed out that only changing the signal level shouldn't actually affect things. They discussed the possibility of mean subtraction improving the signal-to-noise ratio instead of just the raw level.\n ---------------------- Your Score (1~5) ----------------------\n", "claude_outputs #4": "The professor thought that removing echoes and low energy portions of the speech could help solve the problem. He suggested that PhD E could implement a VAD-like system to remove low energy sections that may contain echoes, as the echoes seem to be causing some of the issues.\n ---------------------- Your Score (1~5) ----------------------\n", "claude_outputs #5": "PhD C was initially confused as to why the mean subtraction processing made the signal louder after being re-synthesized. The professor explained that it likely meant the processing improved the signal to noise ratio, not just the absolute signal level. PhD C agreed with this explanation.\n ---------------------- Your Score (1~5) ----------------------\n", "claude_outputs #6": "The main topics discussed in the meeting were:\n\nPhD E's experiments with mean subtraction and why it made the signal louder\nPhD C's experiments with combining energy-based VAD with MLP-based VAD and potential issues with the approach\nPhD G's work on Wiener filtering and noise compensation techniques\nPhD D's work on applying VTS in the cepstral domain versus the filter bank domain\nThe potential for combining different noise reduction and compensation methods once they have a better understanding of each technique's characteristics\nThe professors and PhD students discussed the experiments, potential issues, and ways to improve the various approaches. They provided feedback and suggestions to help each other refine and improve the noise compensation and other signal processing techniques.\n ---------------------- Your Score (1~5) ----------------------\n", "llama2-13b-chat-4k_pred #0": "  The discussion centered on the TORRENT schedule and intermediate categorization for speech recognition. Grad F presented their proposal on using a combination of multi-band techniques and phone recognition to classify intermediate categories. The group discussed the use of TIMIT and Switchboard datasets, and the potential for echoes in the re-synthesized audio. Professor B mentioned the possibility of using high-pass filtering to reduce the effects of reverberation.\n ---------------------- Your Score (1~5) ----------------------\n", "llama2-13b-chat-4k_pred #1": "  Grad F said he is working on a proposal and will give a presentation on it soon. He mentioned that he is focusing on intermediate categories and will use TIMIT for the phone recognition task.\n ---------------------- Your Score (1~5) ----------------------\n", "llama2-13b-chat-4k_pred #2": "  Grad F thought that intermediate categories were a good idea and wanted to use them in his research. He believed that these categories would help improve the accuracy of speech recognition systems.\n ---------------------- Your Score (1~5) ----------------------\n", "llama2-13b-chat-4k_pred #3": "  The discussion on mean subtraction in SRI focused on improving word error rate in speech recognition. The SRI system uses channel adaptation and finer-grained acoustic models, which can be sensitive to artifacts in the re-synthesized audio. Gradient F suggested using a larger data set to train the system and proposed a two-part theory to explain the worsened near-mike performance. Professor B mentioned the possibility of getting rid of low-energy parts of speech to reduce reverberation and improve the system's robustness.\n ---------------------- Your Score (1~5) ----------------------\n", "llama2-13b-chat-4k_pred #4": "  According to the professor, echoes and reverberation can be problematic for speech recognition systems, and the professor suggested using a high-pass filtering to reduce the effects of reverberation. The professor also mentioned that the echoes and reverberation in the room could be affecting the system's performance.\n ---------------------- Your Score (1~5) ----------------------\n", "llama2-13b-chat-4k_pred #5": "  Based on the conversation, PhD C thought that the signal-to-noise ratio was not ideal for the meeting, as there was a lot of background noise and echoes in the room.\n ---------------------- Your Score (1~5) ----------------------\n", "llama2-13b-chat-4k_pred #6": "  The meeting discussed the progress of a research project on speech recognition, specifically the use of a TORRENT chip for the task. The team reviewed the project's timeline and discussed the challenges they have faced, including the need to wait for Sunil to arrive before proceeding with the project. The team also discussed the use of intermediate categories for speech recognition and the potential benefits of using a multi-band approach. Additionally, the team reviewed the results of an experiment using a mean subtraction approach and discussed the theory behind it. Finally, the team discussed the issue of reverberation in the speech recordings and the potential solutions to address it.\n ---------------------- Your Score (1~5) ----------------------\n", "vicuna-7b-2k_pred #0": "\n\nThe group discusses the TORRENT chip project and how they were two months away from being done. They go through old emails and find that for two years they were saying they were two months away from being done. They talk about schedules and how they went through them and for a while, they thought they had to come up with a complete set of intermediate features in intermediate categories to classify right away. However, they are now thinking of starting with a reasonable set and building a system that classifies these intermediate categories using multi-band techniques and phoneme recognition. They will then look at the errors produced in the phoneme recognition and postulate new intermediate categories or remove them if necessary.\n ---------------------- Your Score (1~5) ----------------------\n", "vicuna-7b-2k_pred #1": "Grad F said that he has been working on his proposal and has just finished a section on intermediate categories that he wants to classify as a middle step. He hopes to have a rough draft done by Monday so he can give it to Morgan. He plans to give it to people ahead of time and have a short presentation, followed by questions. He is thinking of starting with a reasonable set of intermediate features and doing some phone recognition before building a system that classifies these features using multi-band techniques and phoneme recognition. He will then look at the errors produced in the phoneme recognition and add or remove intermediate categories as needed.\n ---------------------- Your Score (1~5) ----------------------\n", "vicuna-7b-2k_pred #2": "\n\nGrad F thinks about intermediate categories as a middle step in the classification process. They have just finished a section on these intermediate categories in their proposal and hope to have a rough draft done by Monday to give to Morgan. They plan to give the proposal to people ahead of time and have a short presentation, followed by questions. Grad F is considering starting with a reasonable set of intermediate features and building a system that classifies these using multi-band techniques, combining them for phoneme recognition. They will then look at the errors produced in the phoneme recognition and add or remove intermediate categories as needed.\n ---------------------- Your Score (1~5) ----------------------\n", "vicuna-7b-2k_pred #3": "\n\nThe document is a discussion about the TORRENT chip project and the progress made on it. Professor B mentions that they were two months away from being done with the project, but it took two years to complete. The document also discusses the need for status reports and the possibility of starting a meeting to discuss the Aurora project. Grad F talks about writing a proposal for their project and hopes to have a rough draft done by Monday so they can give it to Morgan. The document ends with a discussion about the features Grad F is looking at in their project and the idea of starting with a reasonable set of intermediate features to classify and then building the system to include more features as needed.\n ---------------------- Your Score (1~5) ----------------------\n", "vicuna-7b-2k_pred #4": "I'm sorry, but I don't have enough context to answer your question. Could you please provide more information or clarify your question?\n ---------------------- Your Score (1~5) ----------------------\n", "vicuna-7b-2k_pred #5": "I'm sorry, but I don't have enough context to answer your question. Could you please provide more information or clarify your question?\n ---------------------- Your Score (1~5) ----------------------\n", "vicuna-7b-2k_pred #6": "\n\nThe meeting discussed the TORRENT chip project, which has been ongoing for two years. Professor B mentioned that they were two months away from being done with the project. However, they went through old emails and realized that they had been saying they were two months away for two years. They discussed schedules and Jim and Morgan went through them. Professor B suggested that they do status reports and wait for Sunil to arrive before discussing his work. Grad F discussed his proposal and said he hopes to have a rough draft done by Monday so he can give it to Morgan. He plans to classify intermediate categories as a middle step and hopes to finish a full draft by July 25th for his quals.\n ---------------------- Your Score (1~5) ----------------------\n", "turbo-16k-0613_pred #0": "The discussion revolved around the TORRENT chip project and the challenges faced in completing it. The team mentioned that they were consistently two months away from completion for two years. They also discussed the use of different VAD (Voice Activity Detection) techniques and the potential benefits of applying VTS (Vocal Tract Length Normalization) in the cepstral domain. The team was unsure if applying VTS in the cepstral domain would be more effective than in the filter bank domain.\n ---------------------- Your Score (1~5) ----------------------\n", "turbo-16k-0613_pred #1": "Grad F mentioned that he has been working on writing his proposal. He mentioned that he has been focusing on classifying intermediate categories and hopes to have a rough draft of his proposal done by Monday. He also mentioned that his proposal will involve using TIMIT and Switchboard datasets to build a system that can classify these intermediate categories and improve phoneme recognition. He also mentioned that his qualifying exams are scheduled for July 25th.\n ---------------------- Your Score (1~5) ----------------------\n", "turbo-16k-0613_pred #2": "Grad F mentioned that he is working on classifying intermediate categories as a middle step in his research. He explained that he is starting with a reasonable set of features and then plans to build a system that can classify these intermediate categories using multi-band techniques. Grad F also mentioned that he hopes to reduce errors in phoneme recognition by adding or removing intermediate categories based on the errors produced. Overall, he seems to be focused on improving the accuracy and efficiency of the classification process for intermediate categories.\n ---------------------- Your Score (1~5) ----------------------\n", "turbo-16k-0613_pred #3": "The discussion on mean subtraction in SRI focused on the potential benefits and challenges of using this technique. The researchers mentioned that mean subtraction could improve the signal-to-noise ratio and help with reverberation effects. However, they also noted that it could lead to louder signals and the removal of low-energy speech components. The researchers discussed different approaches to mean subtraction, including using different filters and normalization techniques. They also mentioned the need for further experimentation to determine the effectiveness of mean subtraction in improving speech recognition accuracy.\n ---------------------- Your Score (1~5) ----------------------\n", "turbo-16k-0613_pred #4": "The professor mentioned that echoes and reverberation can cause problems in speech recognition systems. He discussed the effects of reverberation on speech signals and how it can impact the accuracy of the system. He also mentioned the possibility of using high-pass filtering and VAD-like techniques to reduce the effects of reverberation and echoes.\n ---------------------- Your Score (1~5) ----------------------\n", "turbo-16k-0613_pred #5": "PhD C mentioned that the signal to noise ratio (SNR) is an important factor in their experiments. They discussed the need for smoothing the SNR estimate and mentioned that more smoothing seems to be helping. They also mentioned the possibility of putting a floor on the SNR to reduce variance in low-energy regions. Overall, they are still working on finding the best approach for dealing with the SNR in their experiments.\n ---------------------- Your Score (1~5) ----------------------\n", "turbo-16k-0613_pred #6": "During the meeting, the team discussed various topics related to their research. They talked about the TORRENT chip project and how they were two months away from completing it. They also discussed the progress of their individual projects, such as writing proposals and conducting experiments. Some team members were working on VAD (Voice Activity Detection) and Wiener filtering, while others were exploring the use of VTS (Vocal Tract Length Normalization) in the cepstral domain. They also discussed the possibility of using different types of normalization and filters to improve their results. Overall, the team was focused on finding ways to enhance their models and improve the accuracy of their systems.\n ---------------------- Your Score (1~5) ----------------------\n", "longchat-13b-16k-flash_pred #0": "\nThe team discussed the TORRENT chip and its development progress. They also talked about the intermediate categorization process and its potential impact on the project. The team agreed to wait for Sunil's input before proceeding with the proposal. They also discussed the possibility of using TIMIT for the phone recognition task and the SRI system for the quals. The team also discussed the potential benefits of using VTL and MLLR transformations for the Aurora system.\n ---------------------- Your Score (1~5) ----------------------\n", "longchat-13b-16k-flash_pred #1": "35 words:\n\nGrad F discussed his proposal for improving the TI-Digits dataset by using a combination of VAD and spectral subtraction techniques. He mentioned that he has been running 25 experiments with different parameter values and noise levels to evaluate the effectiveness of these methods. Grad F also mentioned that he plans to try smoothing the estimated clean speech and noise spectra using a Wiener filter to further improve the results.\n ---------------------- Your Score (1~5) ----------------------\n", "longchat-13b-16k-flash_pred #2": "\nGrad F thought that intermediate categories might be a good idea, but they also mentioned that it would be a lot of work to implement. They suggested starting with a smaller set of categories and then expanding if necessary. They also mentioned that they would need to see some examples of the proposed categories before they could give a more detailed opinion.\n ---------------------- Your Score (1~5) ----------------------\n", "longchat-13b-16k-flash_pred #3": "\nThe discussion revolves around the mean subtraction approach used in SRI for speech recognition. The group is considering using a high-pass filter to remove low-energy speech components and improve the system's performance. They are also experimenting with different VAD (voice activity detection) methods to better segment speech from noise. Additionally, they are exploring the use of a Wiener filter to smooth the estimated clean speech and noise spectra, as well as considering the use of a floor on the SNR to reduce variance in the low-energy regions. Finally, they are discussing the potential benefits of using a band-pass filter derived from the cubic root of the power spectrum to enhance the onset and offset of speech.\n ---------------------- Your Score (1~5) ----------------------\n", "longchat-13b-16k-flash_pred #4": "1. The professor mentioned that echoes and reverberation can affect the performance of speech recognition systems.\n2. He suggested using a high-pass filter to reduce the effects of reverberation.\n3. The professor also discussed the SRI system and its use of multi-band techniques for classifying intermediate categories.\n4. He mentioned that the Aurora system uses a different approach to classify intermediate categories.\n5. The professor mentioned that the SRI system was two months away from being completed, but the project was abandoned.\n6. He suggested that the group should wait for Sunil to arrive before proceeding with the project.\n7. The professor mentioned that the quals were happening in July 25th.\n8. He suggested that the group should start writing their proposals and give a rough draft to Morgan.\n9. The professor mentioned that the TIMIT dataset was used for the project.\n10. He suggested that the group should use the TI-digits training set for the Aurora system.\n11. The professor mentioned that the SRI system was trained on clean TI-digits and had a point eight percent error rate.\n12. He suggested that the group should train the Aurora system on the same training set as the SRI system.\n13. The professor mentioned that the SRI system was using finer-grained acoustic models.\n14. He suggested that the group should try to make the Aurora system more robust to channel adaptation.\n15. The professor mentioned that the SRI system was using more parameters.\n16. He suggested that the group should try to reduce the number of parameters in the Aurora system.\n17. The professor mentioned that the SRI system was using a mean subtraction approach.\n18. He suggested that the group should try to classify intermediate categories using a similar approach.\n19. The professor mentioned that the SRI system was using a tandem-style processing with the SRI system.\n20. He suggested that the group should try to use a similar approach with the Aurora system.\n21. The professor mentioned that the SRI system was using a VTL transformation.\n22. He suggested that the group should try to use a similar transformation with the Aurora system.\n23. The professor mentioned that the SRI system was using MLLR transformations.\n2\n ---------------------- Your Score (1~5) ----------------------\n", "longchat-13b-16k-flash_pred #5": "42 words:\n\nPhD C thought that the signal-to-noise ratio was important in speech recognition. He mentioned that combining energy and timing information was useful for distinguishing between low-energy speech components and non-speech noises. He also discussed the use of spectral subtraction and the Makhoul-type spectral subtraction, which uses an over-estimation factor to subtract more noise than the noise spectra that is estimated on the noise portion of the utterances. PhD C also mentioned that smoothing the result of the subtraction using a filter, such as a Wiener filter, could help get rid of musical noise.\n ---------------------- Your Score (1~5) ----------------------\n", "longchat-13b-16k-flash_pred #6": "1. The group discussed the progress of the TORRENT chip project, which is about 2 months away from being completed.\n2. They also talked about the Aurora system and its development. The team is working on improving the system and testing it on different datasets.\n3. They mentioned the SRI system and its complexity compared to the Aurora system. The team is trying to understand the differences between the two systems.\n4. The group discussed the VAD problem and the need for a realistic VAD approach for the evaluation. They also talked about the boundaries that will be provided for the evaluation.\n5. The team is working on different VAD methods, including an energy-based VAD and a MLP-based VAD. They are also experimenting with different over-estimation factors and noise levels in the spectral subtraction process.\n6. The team is trying to smooth the results of the subtraction process to get rid of the musical noise and using a filter to achieve this. They are also trying to estimate the noise initially and then update it quickly using a small time constant.\n7. The group discussed the bugs in the program and the need to clear them up before running more experiments. They also talked about the time constants for smoothing the noise spectrum and the clean speech.\n ---------------------- Your Score (1~5) ----------------------\n", "chatglm2-6b-8k_pred #0": "During a meeting between Professor B and Morgan, they discussed the TORRENT chip, which has a 2-month schedule. They also mentioned that the TORRENT chip has a thing called the \"mean subtraction approach,\" which is a way to remove low-energy sections from the speech signal. They discussed the SRI system, which has a 3.9% error rate, and how it is different from the Aurora system. The Aurora system has a 2.7% error rate and is more complex. The group also discussed the idea of normalizing the phase of the FFT spectrum to normalize the speech signal.\n ---------------------- Your Score (1~5) ----------------------\n", "chatglm2-6b-8k_pred #1": "Grad F proposed a new feature for the Aurora system, which would allow for more detailed information about the conversation to be included in the training data.\n ---------------------- Your Score (1~5) ----------------------\n", "chatglm2-6b-8k_pred #2": "Grad F believed that the intermediate categories were not necessary for the task and did not help with the overall performance.\n ---------------------- Your Score (1~5) ----------------------\n", "chatglm2-6b-8k_pred #3": "The discussion is about the use of mean subtraction in speech recognition (SR) systems. Professor B explains that the technique was first proposed by Chuck and was later adopted by the SRI team. He notes that the main idea behind mean subtraction is to remove some of the low-energy components in the speech signal that may not be representative of the desired output. The professor also mentions that some studies have shown that this approach can improve the performance of SR systems.\n ---------------------- Your Score (1~5) ----------------------\n", "chatglm2-6b-8k_pred #4": "The professor seemed to have a positive attitude towards echoes and reverberation. He acknowledged that the Aurora system has some advantages over the SRI system, but he also mentioned that the Aurora system has some complex acoustic models that may not be as effective for certain tasks. He suggested that the Aurora system may be better suited for certain types of data, but it may not be the best choice for all applications.\n ---------------------- Your Score (1~5) ----------------------\n", "chatglm2-6b-8k_pred #5": "The signal-to-noise ratio (SNR) is an important metric for evaluating the quality of a sound signal. It measures the ratio of the maximum possible signal amplitude to the average signal amplitude. A higher SNR value indicates a higher quality sound signal.\n ---------------------- Your Score (1~5) ----------------------\n", "chatglm2-6b-8k_pred #6": "During a meeting, a curious human and an artificial intelligence assistant discuss their work on a project that involves training a speech recognition system. The assistant explains that they have been working on a system for two years, but they have not yet seen any results. The human then asks if they should continue with the project, and the assistant responds that they should wait for their colleague to return. The human then mentions that they had been discussing the possibility of using a mean subtraction approach to improve the system's performance, as they had heard it could work for the Aurora system. The assistant explains that mean subtraction involves subtracting the mean of a sample from its original value to remove non-zero components in the data. The human then asks if the system would work better if they used this approach, and the assistant responds that it might. The human also asks about the normalization of the data, and the assistant explains that it is a process of removing non-zero components from the data to improve its quality.\n ---------------------- Your Score (1~5) ----------------------\n"}